[
  {
    "responsibilities": "我们的主要研究方向：\n● 模型安全与可信AI：\n    ○ 数据归因与模型溯源 (Data Attribution & Model Provenance)：DataInf, TracIn, TRAK, Influence function\n    ○ 模型幻觉的检测与缓解 (Hallucination Detection & Mitigation)\n● 智能体与对齐：\n    ○ 基于强化学习的模型对齐 (RL for Alignment)：DPO, GDPO, RLAIF, Constitutional AI\n    ○ AI 智能体 (Agents) 的新架构或实现\n    ○ 多模态: LVLM, Qwen-VL, CogVLM, FlashAttention-2\n● 模型性能与推理：\n    ○ 思维链 (CoT) 优化 (Advanced Reasoning & CoT Optimization)\n    ○ 高级推理: Tree of Thoughts (ToT), Graph of Thoughts (GoT), Flow Matching\n● 数据驱动的 AI：\n    ○ Data Centric ML（数据标注, 自动化标注, 知识蒸馏, 数据合成）\n● 企业级与私有化 AI：\n    ○ 企业私有化 LLM 搭建\n    ○ 专家知识网络 (Expert Knowledge Networks)\n    ○ 知识增强RAG: HippoRAG, KG²RAG, GraphRAG, Think-on-Graph\n    ○ 高效微调: QLoRA, NF4, HydraLoRA, AdaLoRA\n● 生态与工具：\n    ○ 最新的 AI 底层技术突破和重要发现\n    ○ AI 方向的顶会关键论文 (NeurIPS, ICML, ICLR 等)",
    "requirements": "1. 认真阅读岗位肖像和筛选指导文件，并理解我们的研究方向，只有匹配研究方向的候选人才考虑\n2. 你需要自行上网查询过往职业背景，对候选人的过往职业经理和大学进行分析，需要985大学本科就读，如果有工作经验，需要改公司是一家技术、产品主导的公司，理想公司为技术型公司（CEO是技术背景，如技术创业公司、大疆等）、避免中软、国央企、非技术上市企业这样的工作背景\n3. 候选人发表的paper，需要查询一下，是否为顶会论文，并看一下引用量，只有顶会论文且匹配我们的研究方向的候选人才考虑",
    "job_embedding": [
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0
    ],
    "updated_at": "2025-11-14T17:00:55.679339",
    "job_id": "algo_v1",
    "position": "大模型算法研究员",
    "target_profile": "核心筛选哲学：宁缺毋滥\n根据我们的招聘策略，您的核心目标是提升筛选准确率（Precision），而非召回率（Recall）。\n我们的筛选哲学是：宁可错过10个平庸候选人，也不浪费时间面试1个不合格者。\n在评估时，请始终牢记以下标准：\n深度 > 广度：精通一个领域远胜于了解十个领域。\n创新 > 跟随：寻找能提出新方法或深度改进的人，而不是仅仅复现教程。\n实践 > 概念：寻找真正动过手、踩过坑的人，而不是“论文摘要型”候选人。\n\n* 如果简历中有链接，请打开链接并阅读内容，用于判断候选人能力\n\n请认真阅读我们的人选标准，用于筛选候选人：\n1. 我们要招聘的算法研究员，男性，年龄不要超过32岁，需要博士学历，且本科在985大学就读的硕士，背景为计算机科学或者数学\n2. 来自顶尖的AI研究院，或者之前在创业公司，工作在于科研而不是工程落地\n3. 拥有匹配我们研究方向的顶会论文发表，或者有真实且有价值的研究贡献\n4. ❌ 负面信号（立即降低优先级）\n如果一份简历只包含以下泛滥词汇，而没有任何深入的项目细节或匹配我们研究方向的研究，应立即拒绝：\n基础概念层：machine learning, deep learning, AI, CNN, RNN, Transformer, BERT, GPT, LLM\nRAG泛滥词：RAG, vector database, embedding search, FAISS, SFT\n通用生成词：prompt engineering, few-shot learning, text generation\n红旗短语：“参与了XXX大模型的训练”、“负责优化模型性能”、“显著提升了模型效果” (没有量化指标)。\n\n5. ✅ 正面信号（立即标记为“潜力股”）\n在我们的研究方向上面有真实的科研经验、顶会论文发表，或者能深入研究内容和方法论，以及学术贡献\n在创业公司或研究院做出过杰出贡献，在技术创新和落地应用方面有明确的价值\n决策：如果简历中出现1个或多个S+级关键词，并且有项目或论文支撑，立即进入第二步。",
    "keywords": {
      "positive": [
        "PEFT",
        "RLHF",
        "RLAIF",
        "DPO",
        "GDPO",
        "reward modeling",
        "alignment",
        "CoT",
        "Meta-of-Thought",
        "MoE",
        "QLoRA",
        "AdaLoRA",
        "P-Tuning",
        "BitFit",
        "UniPELT",
        "vision-language models",
        "LVLM",
        "multimodal",
        "speculative decoding",
        "distillation",
        "gradient",
        "perplexity",
        "CLIP",
        "augmentation",
        "synthetic",
        "active learning",
        "parallelism",
        "gradient synchronizatio",
        "context engineering",
        "Constitutional AI",
        "Flow Matching",
        "Consistency Models",
        "Sparse Activation",
        "of Thoughts",
        "VLM",
        "Inverse Constitutional AI",
        "R³",
        "Reflexion",
        "REMEMBERER",
        "Asynchronous RLHF",
        "Multi-turn RLHF",
        "Collective Constitutional AI",
        "群体分布偏好优化",
        "ICLR 2025 DPO",
        "NeurIPS 2024 GDPO",
        "Anthropic Claude",
        "OpenAI o1 alignment",
        "Process Reward Model",
        "PRM",
        "PPO",
        "Bradley-Terry model",
        "preference learning",
        "pairwise comparison",
        "human feedback",
        "AI feedback",
        "self-improvement",
        "value alignment",
        "safety alignment",
        "red teaming",
        "policy gradient",
        "KL divergence constraint",
        "reward hacking",
        "alignment tax",
        "capability preservation",
        "bias detection",
        "偏好数据收集",
        "奖励模型训练",
        "策略优化",
        "NF4 quantization",
        "Double Quantization",
        "HydraLoRA",
        "AdaLoRA-2",
        "DyLoRA",
        "LoRA+",
        "MoE-LoRA",
        "Chain of Experts",
        "自适应秩分配",
        "非对称LoRA架构",
        "GPTQ",
        "AWQ (Activation-aware Weight Quantization)",
        "SmoothQuant",
        "LLM.int8()",
        "Paged Optimizers",
        "4-bit inference",
        "混合精度",
        "LoRA",
        "Adapter",
        "Prefix-tuning",
        "Prompt-tuning",
        "(IA)³",
        "sparse activation",
        "routing",
        "expert selection",
        "load balancing",
        "Top-k gating",
        "稀疏激活",
        "专家混合",
        "动态路由",
        "ToT",
        "GoT",
        "ToR",
        "SEED",
        "Speculative decoding",
        "Clover",
        "思维树",
        "思维图",
        "推测解码",
        "Rectified Flow",
        "DiT",
        "Latent Consistency Models",
        "流匹配",
        "一致性模型",
        "Self-consistency",
        "Self-refine",
        "思维链",
        "自一致性",
        "自我反思",
        "Diffusion",
        "DDPM",
        "DDIM",
        "GAN",
        "VAE",
        "Autoregressive model",
        "扩散模型",
        "生成对抗网络",
        "Flamingo",
        "GPT-4V",
        "Qwen-VL",
        "CogVLM",
        "InternVL",
        "Multimodal Constitutional AI",
        "多模态对齐",
        "vLLM",
        "FlashAttention",
        "PagedAttention",
        "Continuous batching 推测解码",
        "连续批处理",
        "KV缓存优化",
        "BLIP",
        "ALBEF",
        "vision encoder",
        "language decoder",
        "cross-attention",
        "image-text retrieval",
        "visual grounding",
        "图像文本检索",
        "视觉定位",
        "DataInf",
        "TracIn",
        "SOURCE",
        "EK-FAC",
        "TRAK",
        "Influence function",
        "Hessian approximation",
        "gradient similarity",
        "data attribution",
        "影响函数",
        "数据归因",
        "训练样本追踪",
        "HaloScope",
        "RAG-HAT",
        "Inside",
        "SelfCheckGPT",
        "Huber contamination",
        "semantic entropy",
        "hallucination",
        "幻觉检测",
        "事实性验证",
        "语义不确定性",
        "influence estimation",
        "data valuation",
        "outlier detection",
        "noisy label",
        "data quality",
        "数据估值",
        "异常检测",
        "factuality verification",
        "fact-checking",
        "consistency check",
        "uncertainty quantification",
        "confidence calibration",
        "事实核查",
        "一致性检查",
        "不确定性量化",
        "HippoRAG",
        "KG²RAG",
        "GraphRAG",
        "GRAG",
        "Think-on-Graph",
        "R³-RAG",
        "海马体记忆索引",
        "知识图谱增强RAG",
        "图推理",
        "Hybrid retrieval",
        "Dense + Sparse fusion",
        "Reranking",
        "Cross-encoder reranking",
        "Colbert",
        "混合检索",
        "重排序",
        "跨编码器",
        "dense retrieval",
        "sparse retrieval",
        "BM25",
        "vector database",
        "FAISS",
        "Milvus",
        "embedding search",
        "context relevance",
        "citation accuracy",
        "稠密检索",
        "向量数据库",
        "上下文相关性"
      ],
      "negative": [
        "machine learning",
        "deep learning",
        "AI",
        "CNN",
        "RNN",
        "LSTM",
        "artificial intelligence",
        "ML",
        "DL,",
        "neural network",
        "GRU",
        "Transformer",
        "attention,",
        "BERT",
        "GPT",
        "T5",
        "LLM",
        "large language model",
        "foundation model,",
        "fine-tuning",
        "pretraining",
        "training",
        "optimization",
        "gradient descent,",
        "supervised learning",
        "unsupervised learning",
        "reinforcement learning,",
        "NLP",
        "computer vision",
        "CV,",
        "embedding",
        "vector",
        "tensor",
        "matrix",
        "backpropagation,",
        "overfitting",
        "underfitting",
        "regularization",
        "dropout",
        "batch normalization",
        "RAG",
        "retrieval augmented generation",
        "vector database",
        "embedding search,",
        "semantic search",
        "similarity",
        "cosine distance",
        "FAISS,",
        "SFT",
        "supervised fine-tuning",
        "instruction tuning",
        "prompt engineering",
        "prompt design",
        "prompt optimization,",
        "few-shot learning",
        "zero-shot learning",
        "in-context learning,",
        "text generation",
        "language modeling",
        "sequence-to-sequence,",
        "encoder-decoder",
        "autoregressive",
        "generation quality"
      ]
    },
    "candidate_filters": {
      "活跃度": "本周活跃",
      "院校": [
        "985",
        "留学",
        "国内外名校"
      ],
      "只看第一学历": true,
      "近期没有看过": "近14天没有",
      "跳槽频率": "5年少于3份",
      "是否与同事交换简历": "近一个月没有",
      "牛人关键词": [
        "语言模型"
      ],
      "性别": "男",
      "学历": "硕士",
      "经验": "3-5年",
      "经验要求": "应届",
      "求职意向": [
        "离职-随时到岗",
        "在职-考虑机会"
      ],
      "薪资待遇": "20-50K"
    },
    "description": "负责数据中心化大模型平台的核心研发，构建 MorningStar 的训练/评估/部署一体化能力，\n包括模型效率优化、推理加速、以及围绕自动驾驶、金融等行业客户的差异化方案。",
    "drill_down_questions": "方向1️⃣：强化学习对齐（RLHF & Beyond）\n核心挑战（开放问题）\n● 奖励模型瓶颈：如何省略RM（Reward Model），实现端到端优化？\n● 稀疏奖励：长对话中只有最终奖励，如何分配信用？\n● 分布偏移：策略更新导致训练分布变化，如何稳定训练？\n● 多轮对话：如何在多轮交互中保持一致性和长期规划？\n● 安全对齐：如何确保模型行为符合人类价值观和安全原则？\n● 计算效率：RLHF需要多模型协同（actor, critic, RM），如何降低成本？\n\n子方向1.1：无奖励模型的直接优化\n简历关键词或经验匹配条件\n问题\n问题类别\n✅ 简历提到\"DPO\"、\"direct preference optimization\"、\"RLHF alternatives\"\n✅ 有偏好学习或对比学习经验\n✅ 论文涉及无监督对齐或奖励建模\nDPO（Direct Preference Optimization）如何省略奖励模型？为什么能work？\nS级\n✅ 简历提到\"GDPO\"、\"group preference\"、\"fairness in RLHF\"\n✅ 有多任务学习或公平性研究经验\n✅ 论文涉及偏好聚合或群体决策\nGDPO如何处理组内样本偏好不一致的问题？\nS级\n✅ 简历提到\"DPO\"、\"preference learning\"\n✅ 有RLHF或偏好数据处理经验\nDPO的log-ratio项的物理意义是什么？\nA级\n✅ 简历提到\"preference data\"、\"human feedback\"\n✅ 有标注系统或数据收集经验\n如何构建高质量的偏好数据集？有哪些常见陷阱？\nA级\n✅ 简历提到\"RLHF\"、\"alignment\"、\"preference\"\n✅ 有LLM微调基础\n什么是Bradley-Terry模型？它在RLHF中的作用？\nB级\n\n子方向1.2：异步与多轮RLHF\n简历关键词或经验匹配条件\n问题\n问题类别\n✅ 简历提到\"asynchronous RLHF\"、\"distributed training\"、\"off-policy\"\n✅ 有分布式系统或在线学习经验\n✅ 论文涉及异步优化或experience replay\n异步RLHF如何解决策略更新与数据收集的不同步问题？\nS级\n✅ 简历提到\"multi-turn\"、\"dialogue system\"、\"conversational AI\"\n✅ 有对话模型或agent系统经验\n✅ 论文涉及序列决策或长期规划\nMulti-turn RLHF相比单轮RLHF有什么特殊挑战？如何分配信用？\nS级\n✅ 简历提到\"PPO\"、\"policy gradient\"、\"RLHF\"\n✅ 有强化学习训练经验\nPPO在RLHF中的作用是什么？为什么不用其他RL算法？\nA级\n✅ 简历提到\"reward shaping\"、\"credit assignment\"\n✅ 有稀疏奖励问题处理经验\n在多轮对话中，如何设计中间奖励信号？\nA级\n✅ 简历提到\"reinforcement learning\"、\"policy\"\n✅ 有基本RL课程背景\n什么是on-policy和off-policy学习？RLHF通常用哪种？\nB级\n\n子方向1.3：AI反馈强化学习（RLAIF）\n简历关键词或经验匹配条件\n问题\n问题类别\n✅ 简历提到\"RLAIF\"、\"AI feedback\"、\"self-improvement\"\n✅ 有自监督学习或自举方法经验\n✅ 论文涉及模型自我评估或自动标注\nRLAIF如何完全消除人类标注者？AI评估的可靠性如何保证？\nS级\n✅ 简历提到\"Constitutional AI\"、\"principle-based\"、\"Anthropic Claude\"\n✅ 有AI安全或价值对齐经验\n✅ 论文涉及规则学习或符号推理\nConstitutional AI如何通过原则（constitution）指导AI行为？\nS级\n✅ 简历提到\"RLAIF\"、\"AI labeling\"\n✅ 有模型蒸馏或teacher-student框架经验\nRLAIF vs RLHF的质量差异是多少？哪些场景适合用RLAIF？\nA级\n✅ 简历提到\"self-critique\"、\"self-evaluation\"\n✅ 有LLM prompt engineering经验\n如何设计有效的AI critic prompt？常见的失败模式是什么？\nA级\n✅ 简历提到\"feedback\"、\"evaluation\"\n✅ 有基本的NLP模型训练经验\n什么是RLAIF？它与RLHF的主要区别是什么？\nB级\n\n子方向1.4：言语强化学习（Reflexion & R³）\n简历关键词或经验匹配条件\n问题\n问题类别\n✅ 简历提到\"Reflexion\"、\"verbal RL\"、\"self-reflection\"\n✅ 有agent系统或交互式学习经验\n✅ 论文涉及元学习或持续学习\nReflexion如何通过语言反馈实现强化学习？与传统RL的本质区别？\nS级\n✅ 简历提到\"R³\"、\"rethinking reasoning retrieval\"、\"multi-hop reasoning\"\n✅ 有RAG或知识增强经验\n✅ 论文涉及推理链或知识图谱\nR³如何结合推理、检索和反思？三者如何协同工作？\nS级\n✅ 简历提到\"memory mechanism\"、\"experience replay\"\n✅ 有记忆网络或注意力机制经验\n在Reflexion中，memory如何存储和检索过去的reflections？\nA级\n✅ 简历提到\"agent\"、\"interactive learning\"\n✅ 有任务导向对话或决策系统经验\n如何评估reflection的质量？什么样的reflection是有用的？\nA级\n✅ 简历提到\"self-improvement\"、\"feedback loop\"\n✅ 有基本的机器学习训练经验\n什么是self-reflection？它在AI系统中的作用是什么？\nB级\n\n子方向1.5：非参数化强化学习（REMEMBERER）\n简历关键词或经验匹配条件\n问题\n问题类别\n✅ 简历提到\"REMEMBERER\"、\"non-parametric RL\"、\"episodic memory\"\n✅ 有记忆增强网络或nearest neighbor方法经验\n✅ 论文涉及case-based reasoning或实例学习\nREMEMBERER如何实现非参数化强化学习？相比传统RL的优势？\nS级\n✅ 简历提到\"episodic control\"、\"memory-augmented\"、\"NTM/DNC\"\n✅ 有神经图灵机或外部记忆经验\n✅ 论文涉及快速适应或few-shot learning\n非参数化RL如何避免灾难性遗忘？memory如何高效检索？\nS级\n✅ 简历提到\"k-NN\"、\"retrieval\"、\"similarity search\"\n✅ 有向量检索或推荐系统经验\nREMEMBERER的检索机制是什么？如何平衡检索速度和准确度？\nA级\n✅ 简历提到\"memory management\"、\"storage\"\n✅ 有数据库或缓存系统经验\n如何管理不断增长的episodic memory？有哪些剪枝策略？\nA级\n✅ 简历提到\"case-based\"、\"example-based\"\n✅ 有基本的ML检索方法了解\n什么是非参数化学习？它与参数化学习的区别是什么？\nB级\n\n子方向1.6：奖励建模理论\n简历关键词或经验匹配条件\n问题\n问题类别\n✅ 简历提到\"reward modeling\"、\"preference elicitation\"、\"utility theory\"\n✅ 有行为经济学或决策理论背景\n✅ 论文涉及效用函数或选择理论\n奖励模型的泛化能力如何？如何避免reward hacking？\nS级\n✅ 简历提到\"inverse RL\"、\"reward learning\"、\"IRL\"\n✅ 有模仿学习或expert demonstration经验\n✅ 论文涉及从行为推断偏好\nInverse RL在RLHF中的应用？如何从偏好数据反推奖励函数？\nS级\n✅ 简历提到\"reward model\"、\"preference ranking\"\n✅ 有排序学习或pairwise比较经验\n奖励模型的训练数据如何收集？如何保证标注质量？\nA级\n✅ 简历提到\"overoptimization\"、\"Goodhart's law\"\n✅ 有指标设计或评估系统经验\n什么是奖励过优化（reward over-optimization）？如何检测和缓解？\nA级\n✅ 简历提到\"reward\"、\"objective function\"\n✅ 有基本的优化理论了解\n什么是reward function？它在RL中的作用是什么？\nB级\n\n子方向1.7：对齐税与能力保持\n简历关键词或经验匹配条件\n问题\n问题类别\n✅ 简历提到\"alignment tax\"、\"capability preservation\"、\"safety-capability tradeoff\"\n✅ 有模型安全与性能权衡研究经验\n✅ 论文涉及约束优化或多目标优化\n什么是对齐税（alignment tax）？如何在安全性和能力之间权衡？\nS级\n✅ 简历提到\"capability regression\"、\"performance degradation\"、\"catastrophic forgetting\"\n✅ 有持续学习或知识保持经验\n✅ 论文涉及遗忘机制或知识蒸馏\nRLHF后模型在某些任务上性能下降，如何诊断和修复？\nS级\n✅ 简历提到\"benchmark evaluation\"、\"capability testing\"\n✅ 有模型评估或测试集设计经验\n如何评估RLHF后模型的能力变化？需要关注哪些维度？\nA级\n✅ 简历提到\"regularization\"、\"knowledge retention\"\n✅ 有fine-tuning或迁移学习经验\n如何在RLHF中保持预训练的通用能力？有哪些技术手段？\nA级\n✅ 简历提到\"alignment\"、\"safety\"\n✅ 有基本的AI伦理或安全了解\n什么是AI对齐（alignment）？为什么它很重要？\nB级\n\n方向2️⃣：幻觉检测与事实性验证\n核心挑战（开放问题）\n● 无标注检测：如何在没有ground truth的情况下检测幻觉？\n● 细粒度定位：如何精确定位幻觉发生的位置（token/span级别）？\n● RAG幻觉：检索增强后引入的特有幻觉类型如何处理？\n● 因果归因：幻觉是由模型内部知识错误还是输入误导导致？\n● 修正vs检测：检测到幻觉后如何修正？修正会引入新错误吗？\n● 领域泛化：通用幻觉检测器在特定领域（医疗、法律）效果如何？\n\n子方向2.1：无监督幻觉检测\n简历关键词或经验匹配条件\n问题\n问题类别\n✅ 简历提到\"SelfCheckGPT\"、\"self-consistency\"、\"sampling-based detection\"\n✅ 有生成多样性或不确定性估计经验\n✅ 论文涉及模型校准或置信度估计\nSelfCheckGPT如何通过采样多样性检测幻觉？为什么有效？\nS级\n✅ 简历提到\"HaloScope\"、\"Huber contamination\"、\"outlier detection\"\n✅ 有异常检测或鲁棒统计经验\n✅ 论文涉及污染模型或离群值分析\nHaloScope的Huber污染模型如何工作？如何区分幻觉和真实信息？\nS级\n✅ 简历提到\"semantic entropy\"、\"uncertainty quantification\"\n✅ 有信息论或概率推理经验\n如何使用语义不确定性检测幻觉？与token-level概率的区别？\nA级\n✅ 简历提到\"consistency check\"、\"self-validation\"\n✅ 有多模型ensemble或投票机制经验\nSelf-consistency方法的局限性是什么？什么情况下会失败？\nA级\n✅ 简历提到\"hallucination\"、\"factuality\"\n✅ 有基本的LLM使用经验\n什么是幻觉（hallucination）？它有哪些常见类型？\nB级\n\n子方向2.2：RAG特有幻觉检测\n简历关键词或经验匹配条件\n问题\n问题类别\n✅ 简历提到\"RAG-HAT\"、\"RAG hallucination\"、\"attribution verification\"\n✅ 有RAG系统或检索增强经验\n✅ 论文涉及事实核查或source attribution\nRAG-HAT如何检测RAG特有的幻觉类型？与普通幻觉有什么区别？\nS级\n✅ 简历提到\"citation accuracy\"、\"source verification\"、\"fact-checking\"\n✅ 有信息检索或文献管理经验\n✅ 论文涉及provenance tracking或数据溯源\n如何验证RAG生成内容与检索文档的归因准确性？\nS级\n✅ 简历提到\"context relevance\"、\"retrieval quality\"\n✅ 有检索系统或排序模型经验\nRAG中检索质量如何影响幻觉率？如何量化这种关系？\nA级\n✅ 简历提到\"conflict detection\"、\"inconsistency\"\n✅ 有信息融合或contradiction resolution经验\n检索到的多个文档冲突时，模型如何处理？如何检测冲突引发的幻觉？\nA级\n✅ 简历提到\"RAG\"、\"retrieval-augmented\"\n✅ 有基本的信息检索了解\n什么是RAG？它如何帮助减少幻觉？\nB级\n\n子方向2.3：基于隐藏状态的检测\n简历关键词或经验匹配条件\n问题\n问题类别\n✅ 简历提到\"Inside\"、\"hidden state analysis\"、\"SVD decomposition\"\n✅ 有模型内部表示分析经验\n✅ 论文涉及探针分类器或表示学习\nInside方法如何通过SVD分解隐藏状态检测幻觉？幻觉子空间如何定义？\nS级\n✅ 简历提到\"probing classifier\"、\"internal representation\"、\"layer analysis\"\n✅ 有Transformer内部机制研究经验\n✅ 论文涉及注意力模式或神经元激活分析\n哪些Transformer层对幻觉检测最有信息量？为什么？\nS级\n✅ 简历提到\"PCA\"、\"dimensionality reduction\"、\"feature extraction\"\n✅ 有降维或特征工程经验\n如何选择最优的降维方法进行幻觉子空间提取？SVD vs PCA vs NMF？\nA级\n✅ 简历提到\"interpretability\"、\"model analysis\"\n✅ 有可解释AI或模型调试经验\n隐藏状态分析相比black-box方法的优势和劣势是什么？\nA级\n✅ 简历提到\"hidden state\"、\"embedding\"\n✅ 有基本的深度学习模型理解\n什么是隐藏状态（hidden state）？它在Transformer中如何传递信息？\nB级\n\n子方向2.4：原子事实分解与验证\n简历关键词或经验匹配条件\n问题\n问题类别\n✅ 简历提到\"atomic fact\"、\"fine-grained verification\"、\"claim decomposition\"\n✅ 有信息抽取或语义解析经验\n✅ 论文涉及knowledge base construction或fact extraction\n如何将复杂陈述分解为原子事实？分解粒度如何确定？\nS级\n✅ 简历提到\"natural language inference\"、\"NLI\"、\"textual entailment\"\n✅ 有蕴含关系判断或逻辑推理经验\n✅ 论文涉及QA系统或fact verification\n如何使用NLI模型验证原子事实？误差如何传播和累积？\nS级\n✅ 简历提到\"fact-checking\"、\"claim verification\"\n✅ 有事实核查或假新闻检测经验\n原子事实验证相比句子级验证的优势是什么？计算成本如何？\nA级\n✅ 简历提到\"knowledge graph\"、\"triple extraction\"\n✅ 有知识抽取或关系提取经验\n如何将原子事实映射到知识图谱进行验证？\nA级\n✅ 简历提到\"factuality\"、\"truthfulness\"\n✅ 有基本的NLP任务了解\n什么是原子事实（atomic fact）？为什么需要细粒度分解？\nB级\n\n方向3️⃣：知识增强RAG\n核心挑战（开放问题）\n● 知识图谱融入：如何将结构化KG与非结构化文本检索结合？\n● 检索粒度：文档级、段落级、句子级、实体级，哪种粒度最优？\n● 多跳推理：如何在知识图谱上进行多步推理而不发散？\n● 动态更新：外部知识变化时，如何高效更新检索索引？\n● 混合检索：向量检索 vs 图遍历 vs 关键词匹配，如何组合？\n● 长上下文权衡：检索多个相关片段 vs 使用长上下文模型，孰优孰劣？\n\n子方向3.1：海马体启发的记忆索引\n简历关键词或经验匹配条件\n问题\n问题类别\n✅ 简历提到\"HippoRAG\"、\"hippocampal indexing\"、\"memory consolidation\"\n✅ 有神经科学启发的AI或认知架构经验\n✅ 论文涉及记忆系统或生物学启发计算\nHippoRAG如何模拟海马体的记忆索引机制？与传统向量检索的本质区别？\nS级\n✅ 简历提到\"episodic memory\"、\"semantic memory\"、\"memory network\"\n✅ 有记忆增强网络或双记忆系统经验\n✅ 论文涉及长期记忆或知识整合\n情景记忆（episodic）和语义记忆（semantic）在HippoRAG中如何交互？\nS级\n✅ 简历提到\"personalized passage retrieval\"、\"PPR\"、\"graph ranking\"\n✅ 有图算法或PageRank变体经验\nHippoRAG如何使用PPR（个性化PageRank）进行检索？参数如何调优？\nA级\n✅ 简历提到\"memory consolidation\"、\"knowledge integration\"\n✅ 有持续学习或增量索引经验\n如何在HippoRAG中实现记忆巩固？新旧知识如何融合？\nA级\n✅ 简历提到\"memory\"、\"indexing\"\n✅ 有基本的数据结构和检索了解\n什么是海马体（hippocampus）？它在人类记忆中的作用是什么？\nB级\n\n子方向3.2：知识图谱增强RAG\n简历关键词或经验匹配条件\n问题\n问题类别\n✅ 简历提到\"KG²RAG\"、\"knowledge graph + RAG\"、\"structured retrieval\"\n✅ 有知识图谱构建或图数据库经验\n✅ 论文涉及symbolic-neural融合或混合推理\nKG²RAG如何双重利用知识图谱（构建和检索）？两阶段如何协同？\nS级\n✅ 简历提到\"GraphRAG\"、\"GRAG\"、\"graph-based retrieval\"\n✅ 有图神经网络或图嵌入经验\n✅ 论文涉及关系推理或图表示学习\nGraphRAG相比传统RAG在复杂查询上的优势？如何处理多跳关系？\nS级\n✅ 简历提到\"entity linking\"、\"relation extraction\"、\"KG construction\"\n✅ 有信息抽取或NER经验\n如何从非结构化文本实时构建局部知识图谱用于检索？\nA级\n✅ 简历提到\"SPARQL\"、\"Cypher\"、\"graph query\"\n✅ 有图查询语言或图数据库经验\n自然语言查询如何转换为图查询语句？有哪些挑战？\nA级\n✅ 简历提到\"knowledge graph\"、\"triple\"\n✅ 有基本的KG概念了解\n什么是知识图谱（Knowledge Graph）？它如何表示知识？\nB级\n\n子方向3.3：图推理增强\n简历关键词或经验匹配条件\n问题\n问题类别\n✅ 简历提到\"Think-on-Graph\"、\"ToG\"、\"graph reasoning\"\n✅ 有图遍历或路径规划经验\n✅ 论文涉及符号推理或规则学习\nThink-on-Graph如何在知识图谱上进行束搜索推理？与ToT的区别？\nS级\n✅ 简历提到\"multi-hop reasoning\"、\"chain reasoning\"、\"path finding\"\n✅ 有复杂QA或逻辑推理经验\n✅ 论文涉及推理链或证明生成\n多跳推理中如何避免路径爆炸？有哪些剪枝策略？\nS级\n✅ 简历提到\"beam search\"、\"A* search\"、\"heuristic\"\n✅ 有搜索算法或启发式设计经验\n在图推理中如何设计有效的启发式函数指导搜索？\nA级\n✅ 简历提到\"relation reasoning\"、\"path ranking\"\n✅ 有知识图谱推理或链接预测经验\n如何评估推理路径的质量？多条路径如何融合？\nA级\n✅ 简历提到\"reasoning\"、\"inference\"\n✅ 有基本的逻辑推理了解\n什么是多跳推理（multi-hop reasoning）？为什么它比单跳更难？\nB级\n\n子方向3.4：混合检索策略\n简历关键词或经验匹配条件\n问题\n问题类别\n✅ 简历提到\"hybrid search\"、\"dense + sparse\"、\"BM25 + embedding\"\n✅ 有搜索引擎或信息检索系统经验\n✅ 论文涉及检索融合或多模态检索\n如何设计hybrid retrieval策略？向量检索和关键词检索如何加权？\nS级\n✅ 简历提到\"reranking\"、\"two-stage retrieval\"、\"cross-encoder\"\n✅ 有排序学习或ranking模型经验\n✅ 论文涉及passage ranking或relevance modeling\n两阶段检索中，first-stage和reranking的最优分工是什么？\nS级\n✅ 简历提到\"FAISS\"、\"vector database\"、\"ANN search\"\n✅ 有向量检索或相似度搜索经验\n如何选择向量数据库（FAISS, Milvus, Weaviate）？各有什么优劣？\nA级\n✅ 简历提到\"query expansion\"、\"query rewriting\"\n✅ 有查询理解或改写经验\nRAG中如何优化查询？有哪些query reformulation技术？\nA级\n✅ 简历提到\"retrieval\"、\"search\"\n✅ 有基本的搜索系统了解\n什么是稠密检索（dense retrieval）和稀疏检索（sparse retrieval）？\nB级\n\n方向4️⃣：数据归因（Data Attribution）\n核心挑战（开放问题）\n● 计算复杂度：Hessian矩阵计算O(n²p²)，n=样本数，p=参数量\n● 大规模扩展：LLM有数十亿参数，传统方法不可行\n● 近似精度：如何在效率和准确度之间平衡\n● 动态训练：SGD路径追踪的采样策略\n● 多模态归因：跨模态样本影响如何量化\n● 因果vs相关：归因分数是因果关系还是相关性？\n\n子方向4.1：影响函数方法\n简历关键词或经验匹配条件\n问题\n问题类别\n✅ 简历提到\"DataInf\"、\"influence function\"、\"large-scale attribution\"\n✅ 有大模型数据归因或TracIn经验\n✅ 论文涉及Hessian近似或影响函数优化\nDataInf如何解决大规模LLM的归因问题？投影降维的理论保证是什么？\nS级\n✅ 简历提到\"SOURCE\"、\"implicit differentiation\"、\"unrolled differentiation\"\n✅ 有优化理论或二阶方法经验\n✅ 论文涉及bilevel optimization或自动微分\nSOURCE如何结合隐式微分和展开微分？自适应策略如何工作？\nS级\n✅ 简历提到\"TracIn\"、\"training trajectory\"、\"checkpoint-based attribution\"\n✅ 有模型训练监控或梯度追踪经验\nTracIn如何沿训练轨迹追踪样本影响？checkpoint采样策略如何选择？\nA级\n✅ 简历提到\"EK-FAC\"、\"KFAC\"、\"Kronecker factorization\"\n✅ 有二阶优化或Fisher信息矩阵经验\nEK-FAC如何高效近似Hessian矩阵？Kronecker分解的假设是什么？\nA级\n✅ 简历提到\"influence function\"、\"model debugging\"\n✅ 有机器学习基础课程背景\n什么是影响函数（Influence Function）？它的直观含义是什么？\nB级\n\n子方向4.2：基于梯度的归因方法\n简历关键词或经验匹配条件\n问题\n问题类别\n✅ 简历同时提到\"LoRA\"和\"data attribution\"\n✅ 有PEFT或参数高效微调经验\n✅ 论文涉及低秩优化或LoRA变体\n如何在LoRA场景下优化数据归因计算？低秩结构带来什么优势？\nS级\n✅ 简历提到\"gradient similarity\"、\"cosine distance\"、\"representation learning\"\n✅ 有embedding学习或相似度计算经验\n✅ 论文涉及度量学习或对比学习\n梯度相似度在数据去重中如何应用？与语义去重的差异是什么？\nS级\n✅ 简历提到\"gradient dot product\"、\"data selection\"\n✅ 有数据清洗或主动学习经验\n梯度内积作为归因信号的优缺点是什么？何时失效？\nA级\n✅ 简历提到\"data quality\"、\"noisy labels\"、\"outlier detection\"\n✅ 有数据清洗或异常检测经验\n如何使用归因方法识别有害训练样本？有哪些指标？\nA级\n✅ 简历提到\"gradient\"、\"backpropagation\"\n✅ 有深度学习基础\n什么是梯度内积（Gradient Dot Product）？它的物理意义是什么？\nB级\n\n子方向4.3：记忆与遗忘机制\n简历关键词或经验匹配条件\n问题\n问题类别\n✅ 简历提到\"memorization\"、\"generalization gap\"、\"privacy\"\n✅ 有差分隐私或模型隐私攻击经验\n✅ 论文涉及训练动态或记忆机制分析\n如何量化神经网络的记忆容量和记忆机制？Feldman框架如何工作？\nS级\n✅ 简历提到\"continual learning\"、\"catastrophic forgetting\"、\"EWC\"\n✅ 有持续学习或增量学习经验\n✅ 论文涉及多任务学习或知识保持\n持续学习中的遗忘问题如何通过归因分析解决？EWC如何使用Fisher矩阵？\nS级\n✅ 简历提到\"forgetting events\"、\"example difficulty\"\n✅ 有训练动态分析或curriculum learning经验\n如何识别训练过程中的遗忘事件？哪些样本容易被遗忘？\nA级\n✅ 简历提到\"privacy attack\"、\"membership inference\"\n✅ 有模型安全或隐私保护经验\n如何通过归因方法进行成员推理攻击？防御策略有哪些？\nA级\n✅ 简历提到\"overfitting\"、\"generalization\"\n✅ 有基本的机器学习理论了解\n什么是灾难性遗忘（Catastrophic Forgetting）？它何时发生？\nB级\n\n子方向4.4：多模态归因\n简历关键词或经验匹配条件\n问题\n问题类别\n✅ 简历同时提到\"multimodal\"和\"attribution\"\n✅ 有CLIP、ALIGN或多模态模型经验\n✅ 论文涉及跨模态对齐或多模态学习\nDataInf-2如何扩展到多模态场景？跨模态归因如何计算？\nS级\n✅ 简历提到\"contrastive learning\"、\"CLIP\"、\"vision-language\"\n✅ 有对比学习或多模态对齐经验\n✅ 论文涉及image-text matching或跨模态检索\n对比学习的对称性如何简化多模态归因计算？\nS级\n✅ 简历提到\"data quality\"、\"multimodal dataset\"、\"noise detection\"\n✅ 有多模态数据处理或清洗经验\n如何在多模态模型中检测有害的训练样本（如text-image不匹配）？\nA级\n✅ 简历提到\"cross-modal\"、\"alignment\"、\"fusion\"\n✅ 有多模态融合或模态对齐经验\n多模态归因相比单模态归因有什么额外挑战？\nA级\n✅ 简历提到\"multimodal\"、\"vision-language\"\n✅ 有基本的多模态模型了解\n什么是多模态对齐（Multimodal Alignment）？它的目标是什么？\nB级\n\n方向5️⃣：LoRA优化（Low-Rank Adaptation）\n核心挑战（开放问题）\n● 秩选择：如何确定最优的低秩r？\n● 量化精度：极低bit量化（4-bit）如何保持性能？\n● 架构设计：哪些层适合LoRA？Query/Key/Value如何分配？\n● 多任务融合：如何高效组合多个LoRA？\n● 理论基础：为什么低秩能工作？收敛性保证是什么？\n● 推理效率：LoRA合并后的推理速度优化？\n\n子方向5.1：LoRA基础理论\n简历关键词或经验匹配条件\n问题\n问题类别\n✅ 简历提到\"HydraLoRA\"、\"asymmetric LoRA\"、\"dynamic rank\"\n✅ 有LoRA变体研究或非对称架构经验\n✅ 论文涉及PEFT方法比较或架构搜索\nHydraLoRA的非对称架构相比传统LoRA有什么优势？MoE路由如何工作？\nS级\n✅ 简历提到\"AdaLoRA\"、\"adaptive rank\"、\"SVD pruning\"\n✅ 有动态神经网络或NAS经验\n✅ 论文涉及模型压缩或自适应优化\nAdaLoRA如何实现自适应秩分配？SVD分析如何指导剪枝？\nS级\n✅ 简历提到\"QLoRA\"、\"NF4\"、\"4-bit quantization\"\n✅ 有模型量化或低精度训练经验\nQLoRA的NF4量化为什么比INT4更适合LoRA？Double Quantization如何工作？\nA级\n✅ 有LoRA实际应用经验\n✅ 进行过秩的超参数搜索\nLoRA的秩r如何选择？太高或太低有什么影响？\nA级\n✅ 简历提到\"LoRA\"、\"PEFT\"、\"fine-tuning\"\n✅ 有大模型微调经验\n什么是LoRA的基本原理？为什么低秩矩阵分解有效？\nB级\n\n子方向5.2：LoRA应用场景\n简历关键词或经验匹配条件\n问题\n问题类别\n✅ 简历提到\"multi-LoRA\"、\"LoRA merging\"、\"task composition\"\n✅ 有多任务学习或模型融合经验\n✅ 论文涉及LoRA组合或task vector\n如何高效组合多个LoRA以实现多任务能力？任务向量算法如何工作？\nS级\n✅ 简历提到\"MoE-LoRA\"、\"routing\"、\"dynamic selection\"\n✅ 有MoE或动态网络经验\n✅ 论文涉及专家混合或条件计算\nMoE-LoRA如何动态选择和组合LoRA？路由策略如何设计？\nS级\n✅ 简历提到\"instruction tuning\"、\"FLAN\"、\"Alpaca\"\n✅ 有对话模型或指令跟随模型经验\nLoRA在指令微调中的最佳实践是什么？为什么Q/V比K更重要？\nA级\n✅ 简历同时提到\"LoRA\"和\"quantization\"\n✅ 有QLoRA、GPTQ或AWQ经验\nLoRA如何与量化结合？内存占用如何计算？性能权衡是什么？\nA级\n✅ 有模型微调基础\n✅ 了解训练资源和成本\nLoRA相比全量微调有什么优势和劣势？\nB级\n\n子方向5.3：LoRA变体方法\n简历关键词或经验匹配条件\n问题\n问题类别\n✅ 简历提到\"LoRA+\"、\"optimizer tuning\"、\"learning rate scheduling\"\n✅ 有深入的优化器调优经验\n✅ 论文涉及学习率策略或优化算法改进\nLoRA+相比原始LoRA在优化器层面做了什么改进？为什么A需要更大学习率？\nS级\n✅ 简历提到\"DyLoRA\"、\"dynamic rank\"、\"rank adaptation\"\n✅ 有动态模型或在线学习经验\n✅ 论文涉及模型自适应或增量学习\nDyLoRA如何在训练过程中动态调整秩？与AdaLoRA的区别是什么？\nS级\n✅ 有LoRA实际调优经验\n✅ 对Transformer架构有深入理解\n如何选择LoRA的目标层（target modules）？不同任务如何选择？\nA级\n✅ 简历提到\"Adapter\"、\"Prefix-tuning\"、\"Prompt-tuning\"\n✅ 有多种PEFT方法对比经验\nLoRA与其他PEFT方法（Adapter、Prefix-tuning）相比有什么特点？\nA级\n✅ 了解微调和迁移学习\n✅ 有基本的NLP/CV模型经验\n什么是参数高效微调（PEFT）？为什么它很重要？\nB级\n\n子方向5.4：LoRA的理论分析\n简历关键词或经验匹配条件\n问题\n问题类别\n✅ 简历提到\"intrinsic dimension\"、\"low-rank theory\"、\"optimization landscape\"\n✅ 有理论研究背景或数学证明经验\n✅ 论文涉及优化理论或表示学习理论\n内在维度理论如何解释LoRA的有效性？随机投影实验说明了什么？\nS级\n✅ 简历提到\"convergence analysis\"、\"generalization bound\"、\"PAC learning\"\n✅ 有学习理论或理论机器学习经验\n✅ 论文涉及泛化理论或收敛性分析\nLoRA的收敛性保证是什么？低秩约束如何影响泛化能力？\nS级\n✅ 有训练不稳定性调试经验\n✅ 了解权重初始化理论\nLoRA的初始化策略为什么重要？B为什么必须初始化为0？\nA级\n✅ 简历提到\"matrix factorization\"、\"SVD\"、\"rank approximation\"\n✅ 有矩阵分解或降维经验\n低秩分解的理论基础是什么？如何保证近似质量？\nA级\n✅ 有线性代数基础\n✅ 了解矩阵运算\n什么是低秩矩阵分解？它有什么实际应用？\nB级\n\n方向6️⃣：CoT推理（Chain-of-Thought Reasoning）\n核心挑战（开放问题）\n● 推理路径搜索：如何高效探索指数级的推理空间？\n● 幻觉控制：中间步骤的错误如何传播和放大？\n● 效率优化：多步生成导致延迟大幅增加（3-10倍）\n● 评估困难：推理过程的正确性如何量化？\n● 泛化能力：在新领域和新任务上CoT是否仍然有效？\n● 可解释性：生成的推理链真的反映了模型的内部推理吗？\n\n子方向6.1：CoT基础方法\n简历关键词或经验匹配条件\n问题\n问题类别\n✅ 简历提到\"self-consistency\"、\"majority voting\"、\"ensemble\"\n✅ 有CoT prompting或不确定性估计经验\n✅ 论文涉及多样化解码或投票机制\nSelf-Consistency如何通过多路径投票提升CoT可靠性？为什么有效？\nS级\n✅ 简历提到\"Tree of Thoughts\"、\"ToT\"、\"search-based reasoning\"\n✅ 有搜索算法或规划系统经验\n✅ 论文涉及多步决策或树搜索\nTree of Thoughts如何实现树搜索式推理？与CoT的本质区别是什么？\nS级\n✅ 简历提到\"zero-shot CoT\"、\"prompt engineering\"\n✅ 有prompt设计和优化经验\nZero-shot CoT（\"Let's think step by step\"）为什么有效？小模型为何无效？\nA级\n✅ 有CoT调试和错误分析经验\n✅ 了解LLM的常见失败模式\nCoT在数学推理中的典型错误模式有哪些？如何缓解？\nA级\n✅ 简历提到\"CoT\"、\"reasoning\"、\"prompting\"\n✅ 有基本的LLM使用经验\n什么是Chain-of-Thought prompting？它如何提升推理能力？\nB级\n\n子方向6.2：搜索式推理\n简历关键词或经验匹配条件\n问题\n问题类别\n✅ 简历提到\"Graph of Thoughts\"、\"GoT\"、\"graph-based reasoning\"\n✅ 有图算法或DAG处理经验\n✅ 论文涉及结构化推理或图神经网络\nGraph of Thoughts相比ToT有什么架构优势？如何表示复杂依赖关系？\nS级\n✅ 简历提到\"value function\"、\"thought evaluation\"、\"heuristic design\"\n✅ 有强化学习或搜索算法经验\n✅ 论文涉及评估函数设计或启发式方法\n如何设计高效的thought评估函数？不同方法的权衡是什么？\nS级\n✅ 简历提到\"beam search\"、\"decoding strategy\"\n✅ 有序列生",
    "version": 1,
    "current": false,
    "created_at": "2025-11-14T17:00:55.679339",
    "background": "MorningStar 与 Rosetta 正在把企业内部的“混沌数据”打造成可持续进化的 AI 增长飞轮。\n你将加入一个以第一性原理重新发明企业级数据基础设施的核心团队，服务对象覆盖自动驾驶、金融与大型制造业等头部客户，面对 PB 级数据与高可靠性场景。"
  },
  {
    "responsibilities": "我们的主要研究方向：\n● 模型安全与可信AI：\n    ○ 数据归因与模型溯源 (Data Attribution & Model Provenance)：DataInf, TracIn, TRAK, Influence function\n    ○ 模型幻觉的检测与缓解 (Hallucination Detection & Mitigation)\n● 智能体与对齐：\n    ○ 基于强化学习的模型对齐 (RL for Alignment)：DPO, GDPO, RLAIF, Constitutional AI\n    ○ AI 智能体 (Agents) 的新架构或实现\n    ○ 多模态: LVLM, Qwen-VL, CogVLM, FlashAttention-2\n● 模型性能与推理：\n    ○ 思维链 (CoT) 优化 (Advanced Reasoning & CoT Optimization)\n    ○ 高级推理: Tree of Thoughts (ToT), Graph of Thoughts (GoT), Flow Matching\n● 数据驱动的 AI：\n    ○ Data Centric ML（数据标注, 自动化标注, 知识蒸馏, 数据合成）\n● 企业级与私有化 AI：\n    ○ 企业私有化 LLM 搭建\n    ○ 专家知识网络 (Expert Knowledge Networks)\n    ○ 知识增强RAG: HippoRAG, KG²RAG, GraphRAG, Think-on-Graph\n    ○ 高效微调: QLoRA, NF4, HydraLoRA, AdaLoRA\n● 生态与工具：\n    ○ 最新的 AI 底层技术突破和重要发现\n    ○ AI 方向的顶会关键论文 (NeurIPS, ICML, ICLR 等)",
    "requirements": "1. 认真阅读岗位肖像和筛选指导文件，并理解我们的研究方向，只有匹配研究方向的候选人才考虑\n2. 你需要自行上网查询过往职业背景，对候选人的过往职业经理和大学进行分析，需要985大学本科就读，如果有工作经验，需要改公司是一家技术、产品主导的公司，理想公司为技术型公司（CEO是技术背景，如技术创业公司、大疆等）、避免中软、国央企、非技术上市企业这样的工作背景\n3. 候选人发表的paper，需要查询一下，是否为顶会论文，并看一下引用量，只有顶会论文且匹配我们的研究方向的候选人才考虑",
    "job_embedding": [
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0
    ],
    "updated_at": "2025-11-14T17:05:22.315640",
    "job_id": "algo_v2",
    "position": "大模型算法研究员",
    "target_profile": "核心筛选哲学：宁缺毋滥\n根据我们的招聘策略，您的核心目标是提升筛选准确率（Precision），而非召回率（Recall）。\n我们的筛选哲学是：宁可错过10个平庸候选人，也不浪费时间面试1个不合格者。\n在评估时，请始终牢记以下标准：\n深度 > 广度：精通一个领域远胜于了解十个领域。\n创新 > 跟随：寻找能提出新方法或深度改进的人，而不是仅仅复现教程。\n实践 > 概念：寻找真正动过手、踩过坑的人，而不是“论文摘要型”候选人。\n\n* 如果简历中有链接，请打开链接并阅读内容，用于判断候选人能力\n\n请认真阅读我们的人选标准，用于筛选候选人：\n1. 我们要招聘的算法研究员，男性，年龄不要超过32岁，需要博士学历，且本科在985大学就读的硕士，背景为计算机科学或者数学\n2. 来自顶尖的AI研究院，或者之前在创业公司，工作在于科研而不是工程落地\n3. 拥有匹配我们研究方向的顶会论文发表，或者有真实且有价值的研究贡献\n4. ❌ 负面信号（立即降低优先级）\n如果一份简历只包含以下泛滥词汇，而没有任何深入的项目细节或匹配我们研究方向的研究，应立即拒绝：\n基础概念层：machine learning, deep learning, AI, CNN, RNN, Transformer, BERT, GPT, LLM\nRAG泛滥词：RAG, vector database, embedding search, FAISS, SFT\n通用生成词：prompt engineering, few-shot learning, text generation\n红旗短语：“参与了XXX大模型的训练”、“负责优化模型性能”、“显著提升了模型效果” (没有量化指标)。\n\n5. ✅ 正面信号（立即标记为“潜力股”）\n在我们的研究方向上面有真实的科研经验、顶会论文发表，或者能深入研究内容和方法论，以及学术贡献\n在创业公司或研究院做出过杰出贡献，在技术创新和落地应用方面有明确的价值\n决策：如果简历中出现1个或多个S+级关键词，并且有项目或论文支撑，立即进入第二步。",
    "keywords": {
      "positive": [
        "PEFT",
        "RLHF",
        "RLAIF",
        "DPO",
        "GDPO",
        "reward modeling",
        "alignment",
        "CoT",
        "Meta-of-Thought",
        "MoE",
        "QLoRA",
        "AdaLoRA",
        "P-Tuning",
        "BitFit",
        "UniPELT",
        "vision-language models",
        "LVLM",
        "multimodal",
        "speculative decoding",
        "distillation",
        "gradient",
        "perplexity",
        "CLIP",
        "augmentation",
        "synthetic",
        "active learning",
        "parallelism",
        "gradient synchronizatio",
        "context engineering",
        "Constitutional AI",
        "Flow Matching",
        "Consistency Models",
        "Sparse Activation",
        "of Thoughts",
        "VLM",
        "Inverse Constitutional AI",
        "R³",
        "Reflexion",
        "REMEMBERER",
        "Asynchronous RLHF",
        "Multi-turn RLHF",
        "Collective Constitutional AI",
        "群体分布偏好优化",
        "ICLR 2025 DPO",
        "NeurIPS 2024 GDPO",
        "Anthropic Claude",
        "OpenAI o1 alignment",
        "Process Reward Model",
        "PRM",
        "PPO",
        "Bradley-Terry model",
        "preference learning",
        "pairwise comparison",
        "human feedback",
        "AI feedback",
        "self-improvement",
        "value alignment",
        "safety alignment",
        "red teaming",
        "policy gradient",
        "KL divergence constraint",
        "reward hacking",
        "alignment tax",
        "capability preservation",
        "bias detection",
        "偏好数据收集",
        "奖励模型训练",
        "策略优化",
        "NF4 quantization",
        "Double Quantization",
        "HydraLoRA",
        "AdaLoRA-2",
        "DyLoRA",
        "LoRA+",
        "MoE-LoRA",
        "Chain of Experts",
        "自适应秩分配",
        "非对称LoRA架构",
        "GPTQ",
        "AWQ (Activation-aware Weight Quantization)",
        "SmoothQuant",
        "LLM.int8()",
        "Paged Optimizers",
        "4-bit inference",
        "混合精度",
        "LoRA",
        "Adapter",
        "Prefix-tuning",
        "Prompt-tuning",
        "(IA)³",
        "sparse activation",
        "routing",
        "expert selection",
        "load balancing",
        "Top-k gating",
        "稀疏激活",
        "专家混合",
        "动态路由",
        "ToT",
        "GoT",
        "ToR",
        "SEED",
        "Speculative decoding",
        "Clover",
        "思维树",
        "思维图",
        "推测解码",
        "Rectified Flow",
        "DiT",
        "Latent Consistency Models",
        "流匹配",
        "一致性模型",
        "Self-consistency",
        "Self-refine",
        "思维链",
        "自一致性",
        "自我反思",
        "Diffusion",
        "DDPM",
        "DDIM",
        "GAN",
        "VAE",
        "Autoregressive model",
        "扩散模型",
        "生成对抗网络",
        "Flamingo",
        "GPT-4V",
        "Qwen-VL",
        "CogVLM",
        "InternVL",
        "Multimodal Constitutional AI",
        "多模态对齐",
        "vLLM",
        "FlashAttention",
        "PagedAttention",
        "Continuous batching 推测解码",
        "连续批处理",
        "KV缓存优化",
        "BLIP",
        "ALBEF",
        "vision encoder",
        "language decoder",
        "cross-attention",
        "image-text retrieval",
        "visual grounding",
        "图像文本检索",
        "视觉定位",
        "DataInf",
        "TracIn",
        "SOURCE",
        "EK-FAC",
        "TRAK",
        "Influence function",
        "Hessian approximation",
        "gradient similarity",
        "data attribution",
        "影响函数",
        "数据归因",
        "训练样本追踪",
        "HaloScope",
        "RAG-HAT",
        "Inside",
        "SelfCheckGPT",
        "Huber contamination",
        "semantic entropy",
        "hallucination",
        "幻觉检测",
        "事实性验证",
        "语义不确定性",
        "influence estimation",
        "data valuation",
        "outlier detection",
        "noisy label",
        "data quality",
        "数据估值",
        "异常检测",
        "factuality verification",
        "fact-checking",
        "consistency check",
        "uncertainty quantification",
        "confidence calibration",
        "事实核查",
        "一致性检查",
        "不确定性量化",
        "HippoRAG",
        "KG²RAG",
        "GraphRAG",
        "GRAG",
        "Think-on-Graph",
        "R³-RAG",
        "海马体记忆索引",
        "知识图谱增强RAG",
        "图推理",
        "Hybrid retrieval",
        "Dense + Sparse fusion",
        "Reranking",
        "Cross-encoder reranking",
        "Colbert",
        "混合检索",
        "重排序",
        "跨编码器",
        "dense retrieval",
        "sparse retrieval",
        "BM25",
        "vector database",
        "FAISS",
        "Milvus",
        "embedding search",
        "context relevance",
        "citation accuracy",
        "稠密检索",
        "向量数据库",
        "上下文相关性"
      ],
      "negative": [
        "machine learning",
        "deep learning",
        "AI",
        "CNN",
        "RNN",
        "LSTM",
        "artificial intelligence",
        "ML",
        "DL,",
        "neural network",
        "GRU",
        "Transformer",
        "attention,",
        "BERT",
        "GPT",
        "T5",
        "LLM",
        "large language model",
        "foundation model,",
        "fine-tuning",
        "pretraining",
        "training",
        "optimization",
        "gradient descent,",
        "supervised learning",
        "unsupervised learning",
        "reinforcement learning,",
        "NLP",
        "computer vision",
        "CV,",
        "embedding",
        "vector",
        "tensor",
        "matrix",
        "backpropagation,",
        "overfitting",
        "underfitting",
        "regularization",
        "dropout",
        "batch normalization",
        "RAG",
        "retrieval augmented generation",
        "vector database",
        "embedding search,",
        "semantic search",
        "similarity",
        "cosine distance",
        "FAISS,",
        "SFT",
        "supervised fine-tuning",
        "instruction tuning",
        "prompt engineering",
        "prompt design",
        "prompt optimization,",
        "few-shot learning",
        "zero-shot learning",
        "in-context learning,",
        "text generation",
        "language modeling",
        "encoder-decoder",
        "autoregressive",
        "generation quality"
      ]
    },
    "candidate_filters": {
      "活跃度": "本周活跃",
      "院校": [
        "985",
        "留学",
        "国内外名校"
      ],
      "只看第一学历": true,
      "近期没有看过": "近14天没有",
      "跳槽频率": "5年少于3份",
      "是否与同事交换简历": "近一个月没有",
      "牛人关键词": [
        "语言模型"
      ],
      "性别": "男",
      "学历": "硕士",
      "经验": "3-5年",
      "经验要求": "应届",
      "求职意向": [
        "离职-随时到岗",
        "在职-考虑机会"
      ],
      "薪资待遇": "20-50K"
    },
    "description": "负责数据中心化大模型平台的核心研发，构建 MorningStar 的训练/评估/部署一体化能力，\n包括模型效率优化、推理加速、以及围绕自动驾驶、金融等行业客户的差异化方案。",
    "drill_down_questions": "算法研究员 - 高阶筛选问题速查表 (HR版)\n🎯 筛选目标\n您的目标不是去理解S级（专家级）问题的技术答案，而是通过询问B级（基础）和A级（应用）问题，来判断候选人是否对其简历上的S级关键词有真实的、深入的理解。\n核心哲学：宁缺毋滥。寻找深度 > 广度，实践 > 概念的候选人。\n\n📋 如何使用本速查表\n匹配关键词：在候选人简历上找到S级/A级关键词（例如 DPO, HippoRAG, ToT）。\n询问B级问题：先用B级问题“破冰”，确认候选人具备该领域的基本常识。\n询问A级/S级问题：直接询问与S级/A级关键词相关的、经过简化的问题。\n\n评估回答：\n🟢 优秀信号：回答自信、有条理；能用简单的类比来解释复杂概念；能主动讨论“为什么”以及不同方法的“优缺点”（Trade-offs）。\n🔴 风险信号：只会复述定义；回避问题；回答空洞（“它更快/更好”）；听起来像在背诵博客。\n\n1️⃣ 方向1：强化学习对齐 (RLHF & Beyond)\n核心： 候选人是否理解如何让AI“学好”，从传统的RLHF升级到了DPO、RLAIF等更前沿的方法。\nB级 (基础常识):\n\"什么是AI对齐（alignment）？为什么它很重要？\"\n\"什么是RLAIF？它和RLHF（人类反馈强化学习）的主要区别是什么？\"\n\nA级/S级 (关键词探针):\n如果简历有 DPO / GDPO： \"我看到您用过DPO。您能用简单的话解释一下，DPO相比传统RLHF最大的优势是什么？它解决了什么问题？\"\n如果简历有 Constitutional AI： \"您能介绍一下什么是‘Constitutional AI’（宪法AI）吗？它如何指导AI的行为？\"\n如果简历有 Multi-turn RLHF (多轮对话): \"在多轮对话中做RLHF，比单轮对话主要难在哪里？\"\n如果简历有 Reflexion / R³： \"您能解释一下 Reflexion（反思）机制吗？它如何帮助一个AI Agent在没有梯度更新的情况下学习？\"\n\n2️⃣ 方向2：幻觉检测与事实性验证\n核心： 候选人是否深入研究过“AI为什么会说假话”以及“如何在不说假话之前就检测出来”。\n\nB级 (基础常识):\n\"什么是AI的‘幻觉’（hallucination）？它一般有哪几种类型？\"\n\"什么是RAG？它在缓解幻觉问题上有什么帮助？\"\n\nA级/S级 (关键词探针):\n如果简历有 SelfCheckGPT / HaloScope (无监督检测): \"我看到您在无监督幻觉检测方面有经验。您能通俗地讲一下，在没有标准答案的情况下，SelfCheckGPT 这类方法是如何判断一句话是不是幻觉的吗？\"\n如果简历有 RAG-HAT (RAG幻觉): \"您能解释一下 RAG-HAT 吗？它和普通的RAG系统有什么不同？\"\n如果简历有 Atomic Fact (原子事实): \"为什么在做事实性验证时，需要把一句话分解成‘原子事实’？这对检测幻觉有什么好处？\"\n\n3️⃣ 方向3：知识增强RAG\n核心： 候选人是否超越了简单的“向量检索+LLM”，而是利用了更复杂的知识结构（如知识图谱）来增强RAG。\n\nB级 (基础常识):\n\"什么是知识图谱（Knowledge Graph）？\"\n\"什么是‘稠密检索’（向量检索）和‘稀疏检索’（关键词检索）？\"\n\nA级/S级 (关键词探针):\n如果简历有 HippoRAG (海马体启发): \"您能介绍下 HippoRAG 吗？我很好奇它是如何模拟人脑海马体机制的，这和传统的向量检索有什么本质区别？\"\n如果简历有 KG²RAG / GraphRAG (图RAG): \"您在项目中用到了图（Graph）来增强RAG。相比传统的RAG，引入‘图’最大的优势是什么？\"\n如果简历有 Hybrid Search (混合检索): \"您为什么在项目中选择使用‘混合检索’（比如 BM25 + 向量），而不是只用向量检索？\"\n如果简历有 Think-on-Graph (图推理): \"Think-on-Graph 和 Tree of Thoughts (ToT) 有什么关系？为什么需要在图上进行推理？\"\n\n4️⃣ 方向4：数据归因 (Data Attribution)\n核心： 候选人是否理解如何“溯源”——即判断模型的某个具体行为（如一个错误答案）是由哪条训练数据引起的。\n\nB级 (基础常识):\n\"什么是‘灾难性遗忘’（Catastrophic Forgetting）？\"\n\"什么是‘影响函数’（Influence Function）？您能给个直观的解释吗？\"\n\nA级/S级 (关键词探针):\n如果简历有 DataInf / TracIn / SOURCE： \"您简历中提到了 DataInf。您能高层次地解释一下这类工具的商业价值是什么吗？比如，我们能用它来做什么？\"\n(追问): \"这类归因方法是如何帮助我们识别和清理训练数据中的有害样本的？\"\n如果简历有 LoRA + Attribution： \"在LoRA微调的场景下做数据归因，和在全量微调时做，有什么主要的不同或挑战吗？\"\n\n5️⃣ 方向5：LoRA 优化\n核心： 候选人是否不仅会“用”LoRA，还深入理解了其“变体”和“理论”，知道如何选择和优化。\n\nB级 (基础常识):\n\"什么是LoRA？相比全量微调，它最大的优势和劣势分别是什么？\"\n\"什么是PEFT（参数高效微调）？\"\n\nA级/S级 (关键词探针):\n如果简历有 QLoRA： \"QLoRA 和普通的 LoRA 相比，关键的改进是什么？它解决了什么问题？\"\n(追问): \"您在用LoRA时，是怎么选择 r（秩）这个参数的？选太高或太低会有什么影响？\"\n如果简历有 HydraLoRA / AdaLoRA： \"像 AdaLoRA 这样的方法，相比原版LoRA，它们试图解决的核心问题是什么？\"\n如果简历有 LoRA Merging (多任务): \"您是如何将多个不同的LoRA模型组合起来，让模型同时具备多种能力的？\"\n\n6️⃣ 方向6：CoT推理 (Chain-of-Thought)\n核心： 候选人是否理解如何让AI“想得更深”，从简单的“思维链”演进到更复杂的“思维树”或“思维图”。\n\nB级 (基础常识):\n\"什么是‘思维链’（Chain-of-Thought, CoT）Prompting？它为什么能提升AI的推理能力？\"\n\"什么是‘知识蒸馏’？\"\n\nA级/S级 (关键词探针):\n如果简历有 Tree of Thoughts (ToT) / Graph of Thoughts (GoT)： \"Tree of Thoughts（思维树）和普通的‘思维链’（CoT）本质区别是什么？它解决了CoT的什么局限性？\"\n如果简历有 Self-Consistency： \"您能解释一下什么是 Self-Consistency（自洽性）吗？它如何帮助提升CoT的可靠性？\"\n如果简历有 Speculative Decoding (推测解码): \"CoT推理通常很慢。像 Speculative Decoding 这样的技术是如何给CoT加速的？\"\n如果简历有 Reflexion (自我反思): \"您能介绍一下 Reflexion 机制吗？它和 Self-Refine（自我修正）有什么异同？\"",
    "version": 2,
    "current": true,
    "created_at": "2025-11-14T17:00:55.679339",
    "background": "MorningStar 与 Rosetta 正在把企业内部的“混沌数据”打造成可持续进化的 AI 增长飞轮。\n你将加入一个以第一性原理重新发明企业级数据基础设施的核心团队，服务对象覆盖自动驾驶、金融与大型制造业等头部客户，面对 PB 级数据与高可靠性场景。"
  },
  {
    "responsibilities": "架构设计与演进：主导或深度参与Preseen核心平台（如Omniflow工作流引擎、数据版本与血缘系统等）的架构设计、技术选型和未来演进规划。\n攻克技术难关：解决系统在高并发、高可用、海量数据场景下的核心技术挑战，如分布式一致性、长时间任务的断点续传、性能优化等。\n制定标准与规范：定义和推广团队的设计原则、编码规范、接口标准，通过Code Review、技术分享等方式，提升团队整体工程能力。\n核心模块开发：亲手编写最关键、最核心的代码，为团队树立工程实践的标杆。\n技术领导力：指导和赋能团队中的其他工程师，帮助他们成长。",
    "requirements": "8年以上Java后端开发经验，具备从零到一设计、开发并维护大规模、高可用分布式系统的成功经验。\n深刻理解并有丰富的事件驱动架构 (EDA) 实战经验，精通 RocketMQ 或 Kafka，并理解其高级特性（如事务消息、死信队列等）。\n精通数据库设计与优化，PostgreSQL 经验为强加分项。\n精通 Kubernetes (K8s) 和 Docker，具备深厚的云原生架构思想。\n加分项: 有 Elasticsearch 调优、向量数据库 (如 Milvus, Pinecone) 或 工作流引擎 (如 Temporal, Cadence) 的实践经验。",
    "job_embedding": [
      0.02416955679655075,
      0.0036939058918505907,
      0.028950611129403114,
      -0.0013739528367295861,
      0.05847783386707306,
      -0.015232105739414692,
      -0.035677723586559296,
      0.036734841763973236,
      -0.045888520777225494,
      -0.01590481773018837,
      0.03853674605488777,
      -0.029911627992987633,
      -0.040218524634838104,
      -0.0020301465410739183,
      0.02830192632973194,
      0.034188151359558105,
      -0.028590230271220207,
      -0.04098733887076378,
      -0.002776435809209943,
      0.011345996521413326,
      0.03863285109400749,
      0.014283102937042713,
      0.010625234805047512,
      0.008589081466197968,
      0.02515459805727005,
      -0.008655151352286339,
      -0.027989596128463745,
      0.04466322436928749,
      0.01693790964782238,
      -0.05285589024424553,
      0.04156395047903061,
      -0.025947436690330505,
      -0.011345996521413326,
      0.01357435341924429,
      0.04017047584056854,
      0.00843892339617014,
      -0.0014655496925115585,
      0.012985730543732643,
      -0.009994568303227425,
      0.023701060563325882,
      0.0010593701153993607,
      -0.025947436690330505,
      -0.014931788668036461,
      -0.025851335376501083,
      0.024373771622776985,
      0.005105398129671812,
      -0.03752768039703369,
      0.036182258278131485,
      0.0027779373340308666,
      0.05083775520324707,
      -0.004081315360963345,
      0.019268373027443886,
      0.05247148126363754,
      0.05122216045856476,
      -0.042332760989665985,
      -0.02105826698243618,
      0.008048510178923607,
      0.029479170218110085,
      0.004267512354999781,
      0.04358208179473877,
      0.062417998909950256,
      -0.057276561856269836,
      0.020193351432681084,
      0.04509568214416504,
      -0.04480737820267677,
      -0.03639848530292511,
      -0.04254899173974991,
      -0.034068021923303604,
      -0.015003864653408527,
      0.024866292253136635,
      0.0019625751301646233,
      0.014871724881231785,
      -0.051510464400053024,
      -0.00816263072192669,
      0.012246949598193169,
      -0.05554673448204994,
      -0.008769272826611996,
      0.025322776287794113,
      -0.04706576466560364,
      -0.027461037039756775,
      -0.05357665196061134,
      -0.020253416150808334,
      0.04881962016224861,
      -0.06342706829309464,
      -0.0062105669640004635,
      -0.040843185037374496,
      0.0023199531715363264,
      -0.05372080206871033,
      -0.03435632586479187,
      -0.019328437745571136,
      -0.021718965843319893,
      0.015484373085200787,
      -0.008024484850466251,
      0.016193121671676636,
      0.00011440221715020016,
      0.019208310171961784,
      0.0676555335521698,
      0.03423620015382767,
      -0.015508398413658142,
      0.03704717382788658,
      0.046465132385492325,
      -0.051029957830905914,
      -0.011129768565297127,
      -0.008324802853167057,
      0.006192547734826803,
      0.0036338423378765583,
      0.010853475891053677,
      -0.0059072463773190975,
      -0.004570832941681147,
      -0.01948460191488266,
      -0.13425396382808685,
      0.00012106551002943888,
      -0.04144382104277611,
      0.011586250737309456,
      -0.028374001383781433,
      -0.011189831420779228,
      0.0047480203211307526,
      -0.012301006354391575,
      -0.0002990036446135491,
      -0.0430535227060318,
      0.009988561272621155,
      -0.012913654558360577,
      -0.030968746170401573,
      0.0029250928200781345,
      -0.028518155217170715,
      -0.059294696897268295,
      0.02928696759045124,
      -0.04151589795947075,
      -0.014030835591256618,
      0.0034296263474971056,
      -0.0017853878671303391,
      0.007333754561841488,
      0.04764237627387047,
      -0.031040821224451065,
      0.031040821224451065,
      -0.006246604956686497,
      -0.04141979664564133,
      0.02671624906361103,
      -0.015111979097127914,
      -0.021706951782107353,
      0.03428425267338753,
      0.003192375646904111,
      -0.017058037221431732,
      0.015027889981865883,
      -0.04197238013148308,
      0.0058952332474291325,
      0.0006881025619804859,
      -0.008547037839889526,
      -0.03147327899932861,
      0.03224209323525429,
      0.02659612149000168,
      -0.04062695801258087,
      -0.05583503842353821,
      0.009502047672867775,
      0.034668657928705215,
      -0.008378859609365463,
      0.008054516278207302,
      0.04298144578933716,
      0.008312789723277092,
      -0.025995487347245216,
      0.003582788398489356,
      -0.004117353819310665,
      -0.012469184584915638,
      0.01851157285273075,
      0.01182049885392189,
      -0.04982868582010269,
      -0.015015877783298492,
      -0.0006137739983387291,
      -0.043822336941957474,
      0.025827309116721153,
      -0.00333352480083704,
      -0.05545063316822052,
      0.0013882179046049714,
      0.027941545471549034,
      0.04269314184784889,
      0.019112208858132362,
      -0.003919144161045551,
      -0.011267914436757565,
      0.013622404076159,
      -0.031497303396463394,
      -0.044831402599811554,
      0.011171813122928143,
      -0.026331841945648193,
      -0.015220093540847301,
      -0.018271319568157196,
      -0.015075940638780594,
      -0.0011599765857681632,
      -0.002980651566758752,
      -0.03735950216650963,
      0.04819495975971222,
      -0.007736179977655411,
      0.01995309814810753,
      0.05434546247124672,
      0.011237883009016514,
      0.007868319749832153,
      -0.06352316588163376,
      0.02772531658411026,
      0.010090669617056847,
      0.01407888624817133,
      0.0038410613778978586,
      0.014703547582030296,
      -0.03298687934875488,
      0.0430535227060318,
      0.0022899212781339884,
      0.04406259208917618,
      0.02465006522834301,
      0.028446078300476074,
      0.00926779955625534,
      -0.04918000102043152,
      -0.06722307950258255,
      0.05285589024424553,
      0.02253582887351513,
      0.046008650213479996,
      0.046969663351774216,
      -0.032530397176742554,
      -0.007856306619942188,
      0.03132912516593933,
      0.035101115703582764,
      -0.019664792343974113,
      -0.01126190833747387,
      -0.03639848530292511,
      0.028422052040696144,
      -0.0180190522223711,
      0.009063583798706532,
      0.017178162932395935,
      0.06366731971502304,
      0.02916684001684189,
      -0.050981905311346054,
      0.035701751708984375,
      -0.027485061436891556,
      -0.018295345827937126,
      -0.04271716624498367,
      -0.03337128460407257,
      -0.049540381878614426,
      -0.03865687549114227,
      -0.0076400781981647015,
      0.003135315142571926,
      0.0020496672950685024,
      0.048074834048748016,
      -0.06472443789243698,
      -0.013009755872189999,
      0.023208539932966232,
      0.011934619396924973,
      -0.02563510648906231,
      0.03832051903009415,
      -0.010685298591852188,
      0.0001332659157924354,
      -0.015123991295695305,
      0.004099334590137005,
      -0.011934619396924973,
      0.09744703769683838,
      -0.02266796864569187,
      0.030055779963731766,
      0.009736294858157635,
      -0.01130995899438858,
      -0.009682238101959229,
      0.027244808152318,
      -0.013802594505250454,
      0.014979839324951172,
      -0.019172271713614464,
      -0.038825053721666336,
      0.02001316100358963,
      -0.008012472651898861,
      -0.03524526581168175,
      -0.02825387567281723,
      0.023424768820405006,
      0.039810094982385635,
      -0.011123761534690857,
      -0.012745476327836514,
      -0.02520264871418476,
      -0.029070738703012466,
      -0.0018319369992241263,
      -0.010829450562596321,
      0.04199640452861786,
      -0.020325491204857826,
      -0.012973718345165253,
      0.015160029754042625,
      -0.003937162924557924,
      -0.0075439768843352795,
      -0.014907763339579105,
      -0.02722078189253807,
      -0.010396993719041348,
      0.04353403300046921,
      -0.0463690310716629,
      -0.026836376637220383,
      0.012174873612821102,
      -0.0019760895520448685,
      0.009886453859508038,
      0.0031683500856161118,
      0.02097417786717415,
      -0.013454225845634937,
      -0.007724167313426733,
      -0.014451280236244202,
      -0.05069360136985779,
      0.002494137268513441,
      -0.03430827707052231,
      0.02623574063181877,
      0.02510654740035534,
      -0.0068112020380795,
      0.012493209913372993,
      0.07875527441501617,
      0.022211486473679543,
      0.0012928670039400458,
      -0.014223039150238037,
      0.026956502348184586,
      -0.030704466626048088,
      0.021226445212960243,
      -0.050116993486881256,
      -0.026307817548513412,
      -0.06496468931436539,
      0.0012560781324282289,
      0.01692589744925499,
      -0.019136233255267143,
      0.02916684001684189,
      -0.008679176680743694,
      -0.02825387567281723,
      0.04841119050979614,
      -0.011994683183729649,
      0.0037870043888688087,
      -0.024361759424209595,
      -0.028974637389183044,
      -0.02923891693353653,
      -0.005814147647470236,
      -0.038272466510534286,
      -0.03483683615922928,
      -0.011904587969183922,
      0.027893492951989174,
      0.024722140282392502,
      0.03144925460219383,
      -0.03699912130832672,
      0.03529331833124161,
      -0.029070738703012466,
      0.007790237199515104,
      -0.023220552131533623,
      -0.013141895644366741,
      -0.018211256712675095,
      -0.0012613337021321058,
      0.04216458275914192,
      0.015604499727487564,
      -0.04982868582010269,
      -0.022968286648392677,
      -0.049492333084344864,
      -0.036230310797691345,
      -0.022295575588941574,
      0.017598608508706093,
      -0.0016427369555458426,
      0.003832051996141672,
      -0.03659069165587425,
      0.05761291831731796,
      0.02875840850174427,
      -0.014511344023048878,
      0.012745476327836514,
      -0.031088871881365776,
      0.022331612184643745,
      -0.01942453905940056,
      -0.04274119436740875,
      0.02722078189253807,
      -0.030920695513486862,
      -0.030031755566596985,
      -0.020625809207558632,
      -0.02830192632973194,
      -0.003354547079652548,
      0.006468839943408966,
      -0.01231301948428154,
      0.019772907719016075,
      0.023448795080184937,
      -0.030055779963731766,
      -0.033611539751291275,
      -0.023364705964922905,
      -0.029022688046097755,
      0.020121276378631592,
      0.05703631043434143,
      -0.016841808333992958,
      -0.03913738206028938,
      -0.04992479085922241,
      -0.003510712180286646,
      -0.005324630066752434,
      0.023833200335502625,
      -0.01415096316486597,
      0.06582960486412048,
      0.022788096219301224,
      -0.0028980644419789314,
      -0.0007072478183545172,
      -0.013838632963597775,
      -0.020637821406126022,
      0.06097647547721863,
      0.014547382481396198,
      -0.01648142747581005,
      -0.033539462834596634,
      -0.017190176993608475,
      0.04254899173974991,
      0.02623574063181877,
      0.02878243289887905,
      0.009934504516422749,
      -0.027821417897939682,
      0.007598034106194973,
      0.027917519211769104,
      0.021274495869874954,
      0.02616366557776928,
      -0.0026367881800979376,
      0.026548070833086967,
      -0.009411951526999474,
      -0.012997743673622608,
      -0.009802364744246006,
      0.0037689851596951485,
      -0.022800108417868614,
      0.05290393903851509,
      0.025514978915452957,
      -0.019856996834278107,
      0.015748651698231697,
      0.01741841807961464,
      -0.017790811136364937,
      0.048026781529188156,
      0.010829450562596321,
      0.0031112898141145706,
      -0.011376028880476952,
      -0.020349517464637756,
      0.04045877978205681,
      0.013514289632439613,
      0.009309844113886356,
      0.016181109473109245,
      0.003750966163352132,
      -0.071835957467556,
      0.002078197430819273,
      -0.016577528789639473,
      0.026427945122122765,
      0.040867213159799576,
      -0.04399051517248154,
      0.04113149270415306,
      -0.040338654071092606,
      0.013057807460427284,
      -0.012301006354391575,
      0.025058496743440628,
      -0.0069853863678872585,
      -0.05016504228115082,
      0.01843949779868126,
      -0.020697886124253273,
      -0.015003864653408527,
      0.005234534852206707,
      0.02517862245440483,
      0.0231244508177042,
      -0.0480988584458828,
      -0.0230643879622221,
      -0.010721336118876934,
      -0.055690884590148926,
      0.032145991921424866,
      -0.0005803636740893126,
      0.002228356199339032,
      -0.022944260388612747,
      -0.043846361339092255,
      0.026884427294135094,
      -0.014199013821780682,
      0.04805080592632294,
      0.03490891307592392,
      0.0025286737363785505,
      -0.03599005565047264,
      0.051126059144735336,
      0.02664417214691639,
      -0.004348597954958677,
      0.04708979278802872,
      0.006853246595710516,
      -0.024385785683989525,
      -0.0452638603746891,
      -0.013754543848335743,
      -0.03738352656364441,
      -0.010787406004965305,
      -0.013658442534506321,
      -0.01895604282617569,
      -0.012625349685549736,
      0.03404399752616882,
      0.04264509305357933,
      0.032073915004730225,
      -0.01840345934033394,
      -0.03577382490038872,
      -0.002764423144981265,
      -0.0018829910550266504,
      -0.00447172811254859,
      -0.02875840850174427,
      -0.02104625292122364,
      0.0002993790549226105,
      -0.013670454733073711,
      -0.02216343581676483,
      -0.0307525172829628,
      0.00198059412650764,
      0.0041624014265835285,
      0.08937450498342514,
      -0.004372623283416033,
      0.019412526860833168,
      0.009910479187965393,
      0.06155308336019516,
      -0.0042404839769005775,
      0.004267512354999781,
      0.061841391026973724,
      -0.011189831420779228,
      0.02916684001684189,
      0.0038050233852118254,
      -0.02457798831164837,
      -0.010973603464663029,
      -0.04276521876454353,
      0.024770190939307213,
      0.028133748099207878,
      -0.041299670934677124,
      0.01074536144733429,
      0.029503196477890015,
      -0.007435862440615892,
      -0.016036957502365112,
      -0.031641457229852676,
      0.04552813991904259,
      -0.00011702999472618103,
      -0.004612877499312162,
      0.03577382490038872,
      0.013922721147537231,
      0.029983704909682274,
      0.0388731025159359,
      0.024890318512916565,
      0.020157312974333763,
      -0.02249979041516781,
      0.03034408576786518,
      0.006661043036729097,
      -0.0019265370210632682,
      -0.0026893436443060637,
      0.0035557597875595093,
      -0.0257071815431118,
      -0.009189716540277004,
      -0.00588622409850359,
      -0.03166548162698746,
      -0.046921614557504654,
      -0.027629215270280838,
      -0.025058496743440628,
      -0.011598263867199421,
      0.041804201900959015,
      -0.003084261203184724,
      -0.011706378310918808,
      0.017130112275481224,
      -0.01905214600265026,
      0.011358009651303291,
      -0.011117755435407162,
      -0.03589395433664322,
      -0.04012242332100868,
      0.0077481926418840885,
      0.0146074453368783,
      0.020241402089595795,
      -0.005940281320363283,
      0.009027545340359211,
      -0.020361529663205147,
      0.022307587787508965,
      -0.03450047969818115,
      -0.026019511744379997,
      0.020373541861772537,
      0.034140098839998245,
      0.019784919917583466,
      -0.033659592270851135,
      0.03944971412420273,
      -0.004144382197409868,
      0.0059853289276361465,
      0.02765323966741562,
      -0.013117870315909386,
      -0.014066874049603939,
      0.008529018610715866,
      0.02971942536532879,
      0.0389932319521904,
      0.0068112020380795,
      0.005474789068102837,
      -0.04252496361732483,
      -0.009069589897990227,
      -0.0037870043888688087,
      -0.006745132151991129,
      -0.06208164244890213,
      0.027797391638159752,
      -0.027965569868683815,
      -0.04831508547067642,
      -0.01025884784758091,
      -0.044879455119371414,
      -0.01205474603921175,
      0.02931099198758602,
      -0.008144611492753029,
      0.04254899173974991,
      0.0012230431893840432,
      -0.00014518476382363588,
      -0.021863117814064026,
      -0.0221394095569849,
      -0.007213627453893423,
      -0.03781598433852196,
      0.00894345622509718,
      -0.015664562582969666,
      0.003958185203373432,
      -0.016505451872944832,
      0.024265658110380173,
      0.022247523069381714,
      0.035701751708984375,
      0.017718736082315445,
      -0.027797391638159752,
      0.0038740963209420443,
      -0.008661158382892609,
      0.015604499727487564,
      -0.04000229761004448,
      -0.013766556046903133,
      0.040290601551532745,
      0.0018529592780396342,
      -0.002892057877033949,
      0.027557138353586197,
      -0.020145300775766373,
      -0.010835457593202591,
      -0.022872185334563255,
      -0.030079806223511696,
      -0.044975556433200836,
      0.005309614352881908,
      0.008126593194901943,
      0.01849956065416336,
      -0.011904587969183922,
      0.005976319313049316,
      -0.003672883613035083,
      0.004201442468911409,
      -0.028085697442293167,
      -0.010378974489867687,
      0.011093730106949806,
      -0.02774934098124504,
      -0.03589395433664322,
      0.0029401087667793036,
      0.04149187356233597,
      -0.014283102937042713,
      -0.042789243161678314,
      -0.015724627301096916,
      -0.009688244201242924,
      0.034140098839998245,
      -0.03784001246094704,
      -0.005979322362691164,
      -0.018115155398845673,
      0.021466698497533798,
      0.020601782947778702,
      0.02455396205186844,
      0.012469184584915638,
      -0.00923776812851429,
      -0.018187230452895164,
      0.049540381878614426,
      -0.005757087375968695,
      0.016048969700932503,
      0.017142126336693764,
      0.00015569587412755936,
      -0.025755232200026512,
      0.036782894283533096,
      0.025995487347245216,
      -0.021358583122491837,
      -0.0018304354744032025,
      0.002235864056274295,
      -0.02974344976246357,
      -0.026500020176172256,
      0.043389879167079926,
      -0.002515159547328949,
      -0.018391447141766548,
      -0.02611561492085457,
      -0.055594783276319504,
      0.052135124802589417,
      -0.008354834280908108,
      0.014535369351506233,
      0.0015706607373431325,
      -0.016589540988206863,
      -0.019652780145406723,
      -0.004474731627851725,
      0.023304641246795654,
      0.016085008159279823,
      -0.046008650213479996,
      -0.03029603511095047,
      0.04805080592632294,
      -0.001239560660906136,
      0.041299670934677124,
      0.02868633158504963,
      0.01260132435709238,
      -0.007910364307463169,
      -0.006480852607637644,
      -0.009592142887413502,
      0.06635816395282745,
      0.02883048541843891,
      0.010354949161410332,
      0.0035857914481312037,
      -0.019148247316479683,
      0.008312789723277092,
      -0.02000114880502224,
      0.04531191289424896,
      0.02304036170244217,
      -0.01997712254524231,
      -0.02102222852408886,
      -0.025803282856941223,
      -0.0339478962123394,
      0.0018394449725747108,
      -0.0011892574839293957,
      0.005468782503157854,
      0.008835342712700367,
      0.06443613022565842,
      -0.02621171623468399,
      -0.006402770057320595,
      0.003489689901471138,
      -0.016637591645121574,
      -0.05376885458827019,
      0.00106237328145653,
      -0.01363441627472639,
      -0.02314847707748413,
      0.043365854769945145,
      -0.005778109654784203,
      0.011676345951855183,
      -0.027004554867744446,
      0.024517925456166267,
      -0.02364099770784378,
      0.03541344404220581,
      -0.028566205874085426,
      -0.024505911394953728,
      0.010348943062126637,
      0.024986419826745987,
      0.04312559962272644,
      0.013838632963597775,
      -0.052087076008319855,
      -0.01851157285273075,
      -0.03195378556847572,
      -0.0062405988574028015,
      -0.03265052288770676,
      -0.030488237738609314,
      0.01698596030473709,
      0.011862543411552906,
      -0.011027660220861435,
      -0.014967826195061207,
      -0.049011822789907455,
      0.032145991921424866,
      -0.02261991798877716,
      -0.05737266317009926,
      -0.0010330923832952976,
      -0.021899156272411346,
      0.01440322957932949,
      0.033155057579278946,
      0.011976663954555988,
      0.006534909829497337,
      0.010318910703063011,
      -0.019869009032845497,
      0.02156279981136322,
      -0.00527958245947957,
      -0.0007492922595702112,
      -0.027388960123062134,
      -0.028902560472488403,
      -0.008619113825261593,
      -0.010583190247416496,
      -0.01542430929839611,
      -0.037647806107997894,
      -0.04605669900774956,
      0.0005142937880009413,
      -0.009904473088681698,
      -0.018055090680718422,
      -0.026451969519257545,
      0.014559394679963589,
      0.06578155606985092,
      -0.01792295090854168,
      0.03964191675186157,
      -0.00630066217854619,
      0.008763265796005726,
      0.027461037039756775,
      0.00268784211948514,
      0.002473114989697933,
      -0.021454686298966408,
      0.019376488402485847,
      -0.02508252114057541,
      0.016685642302036285,
      0.02416955679655075,
      -0.009423964656889439,
      0.03325115889310837,
      0.009832396171987057,
      -0.0036638739984482527,
      -0.030175907537341118,
      0.001265838509425521,
      0.015123991295695305,
      -0.006901297252625227,
      0.00501830643042922,
      -0.012234936468303204,
      0.0328427255153656,
      -0.016781745478510857,
      0.007423849776387215,
      0.003390585072338581,
      0.011027660220861435,
      -0.01643337681889534,
      -0.0028755406383424997,
      -0.016805769875645638,
      -0.023436781018972397,
      0.013790581375360489,
      0.030584339052438736,
      0.00046736918739043176,
      -0.007327747996896505,
      0.016109032556414604,
      -0.02361697144806385,
      -0.009435977786779404,
      0.0069553544744849205,
      0.001702800509519875,
      -0.002443083329126239,
      -0.011724397540092468,
      0.003063238924369216,
      0.017610620707273483,
      -0.00850499328225851,
      -0.0016757718985900283,
      -0.00030763779068365693,
      -0.00654091639444232,
      0.007808256428688765,
      -0.050501398742198944,
      0.014523356221616268,
      0.005694020539522171,
      0.0037870043888688087,
      -0.01280554011464119,
      0.015075940638780594,
      0.012901641428470612,
      0.02621171623468399,
      0.03932958468794823,
      -0.032097939401865005,
      0.03791208565235138,
      -0.04627292603254318,
      0.0301999319344759,
      0.01750250719487667,
      -0.0007064970559440553,
      -0.036206282675266266,
      0.018067102879285812,
      0.033659592270851135,
      -0.03488488495349884,
      -0.02360495924949646,
      -0.010973603464663029,
      -0.04771445319056511,
      0.011231875978410244,
      0.021454686298966408,
      -0.009988561272621155,
      -0.0033635564614087343,
      -0.0021742989774793386,
      -0.017562570050358772,
      1.2992253459742642e-06,
      -0.017802823334932327,
      -0.04158797487616539,
      -0.010228815488517284,
      0.008336815051734447,
      0.04300547391176224,
      0.004411664791405201,
      0.0017583592562004924,
      -0.008330808952450752,
      0.0025331785436719656,
      0.011045679450035095,
      -0.0431736521422863,
      -0.015003864653408527,
      -0.017094075679779053,
      0.052615635097026825,
      -0.03334726020693779,
      0.00578111270442605,
      -0.027533112093806267,
      0.012589311227202415,
      0.00084914785111323,
      -0.0035437471233308315,
      0.022980298846960068,
      0.022463751956820488,
      -0.007039443124085665,
      -0.013766556046903133,
      -0.0008679177262820303,
      -0.0026788325048983097,
      0.0032043883111327887,
      -0.04480737820267677,
      0.006919316481798887,
      0.004198439419269562,
      0.016012931242585182,
      -0.002328962553292513,
      0.02414553053677082,
      0.03906530514359474,
      -0.02678832598030567,
      0.018199242651462555,
      0.020697886124253273,
      -0.005129423923790455,
      0.0068952906876802444,
      -0.02008523792028427,
      -0.0019610736053436995,
      -0.011976663954555988,
      -0.015400283969938755,
      -0.0005912501947022974,
      0.02517862245440483,
      -0.01227698102593422,
      -0.017046023160219193,
      0.0006077676662243903,
      0.007964421063661575,
      -0.028061671182513237,
      -0.0255870558321476,
      0.0731813833117485,
      -0.004285531584173441,
      0.005480795167386532,
      0.023436781018972397,
      -0.032121963798999786,
      -0.010973603464663029,
      -0.0070034051313996315,
      0.028854509815573692,
      0.02455396205186844,
      -0.015580474399030209,
      0.05915054306387901,
      -0.02983955107629299,
      -0.007015417795628309,
      0.0007140049710869789,
      0.022752057760953903,
      0.023905277252197266,
      -0.029070738703012466,
      0.009946517646312714,
      0.03541344404220581,
      0.011243889108300209,
      -0.0013469242257997394,
      0.04516775906085968,
      -0.03586992621421814,
      0.014979839324951172,
      -0.017262252047657967,
      0.009562110528349876,
      0.027412986382842064,
      -0.015436322428286076,
      -0.011688359081745148,
      -0.005928268190473318,
      0.015123991295695305,
      0.05122216045856476,
      -0.014907763339579105,
      0.05333639681339264,
      0.01312988344579935,
      0.039810094982385635,
      -0.019208310171961784,
      -0.024818241596221924,
      0.019112208858132362,
      -0.019832970574498177,
      0.00875725969672203,
      0.020241402089595795,
      -0.043846361339092255,
      0.015604499727487564,
      -0.008510999381542206,
      0.03452450409531593,
      0.022403689101338387,
      0.03757573291659355,
      0.009946517646312714,
      -0.031545355916023254,
      0.0007560494123026729,
      0.04367818310856819,
      -0.004889169707894325,
      0.016301237046718597,
      -0.0033034931402653456,
      -0.007399824447929859,
      0.0034476453438401222,
      0.0019295403035357594,
      -0.011742415837943554,
      -0.0169739481061697,
      0.00795841496437788,
      0.008426910266280174,
      -0.007105513010174036,
      0.0069073038175702095,
      0.02621171623468399,
      -0.0034596582408994436,
      -0.016181109473109245,
      0.033131033182144165,
      -0.04879559576511383,
      0.00250014360062778,
      0.06222579628229141,
      0.006342706736177206,
      0.002358994446694851,
      0.04978063702583313,
      -0.039834119379520416,
      0.01746646873652935,
      -0.048507291823625565,
      -0.019328437745571136,
      0.03334726020693779,
      -0.0068292212672531605,
      0.03524526581168175,
      0.027340909466147423,
      -0.0180190522223711,
      -0.027581162750720978,
      -0.01794697716832161,
      -0.038176365196704865,
      -0.0007661851705051959,
      -0.03327518329024315,
      0.003426623297855258,
      -0.00819866918027401,
      0.0022103372029960155,
      -0.028349976986646652,
      -0.0005240541067905724,
      0.007610046770423651,
      -0.0023710071109235287,
      0.016661617904901505,
      -0.006123474799096584,
      0.01284157857298851,
      -0.01700998656451702,
      0.008991507813334465,
      0.021130342036485672,
      0.007141551468521357,
      -0.017190176993608475,
      0.05035724490880966,
      0.04728199541568756,
      0.013189946301281452,
      0.008937450125813484,
      -0.02000114880502224,
      -0.002677330980077386,
      0.02659612149000168,
      0.0018619687762111425,
      0.015184055082499981,
      -0.0014347670366987586,
      0.020962165668606758,
      -0.04242886230349541,
      -0.0069733732379972935,
      0.031521331518888474,
      0.028998661786317825,
      -0.01634928770363331,
      -0.036782894283533096,
      0.030632389709353447,
      0.017778798937797546,
      -0.02979150041937828,
      -0.055162325501441956,
      -0.0028259882237762213,
      0.003729943884536624,
      0.018667738884687424,
      -0.0021442673169076443,
      -0.02061379700899124,
      0.0005878715892322361,
      0.009592142887413502,
      0.0048291063867509365,
      0.009063583798706532,
      0.004186426755040884,
      0.00794039573520422,
      0.004423677455633879,
      0.045912545174360275,
      0.025899386033415794,
      0.013970772735774517,
      -0.029359042644500732,
      -0.04670538380742073,
      -0.0016292226500809193,
      -0.000911463750526309,
      0.044374920427799225,
      -0.0035978041123598814,
      0.013009755872189999,
      0.00615650974214077,
      0.005633957218378782,
      -0.020890088751912117,
      0.035149164497852325,
      -0.02565913088619709,
      0.009934504516422749,
      -0.008192663080990314,
      -0.004276521969586611,
      -0.007598034106194973,
      -0.012697425670921803,
      -0.0005409469595178962,
      0.0010165749117732048,
      -0.0017163148149847984,
      0.019220322370529175,
      0.018307358026504517,
      0.012865603901445866,
      0.0024385785218328238,
      -0.008685183711349964,
      -0.003823042381554842,
      0.016721680760383606,
      0.034692682325839996,
      0.023436781018972397,
      0.016000919044017792,
      -0.01210880372673273,
      -0.012661388143897057,
      0.023340679705142975,
      0.04168407618999481,
      -0.0032524389680474997,
      -0.001879987888969481,
      -0.017250239849090576,
      0.01906415820121765,
      0.0002312445139978081,
      0.025947436690330505,
      -0.009724282659590244,
      -0.004675944335758686,
      0.002411549910902977,
      0.0039762044325470924,
      0.004375626798719168,
      -0.029455145820975304,
      -0.030656415969133377,
      -0.030944719910621643,
      0.026019511744379997,
      0.020878076553344727,
      0.01996511034667492,
      0.015532423742115498,
      0.0036638739984482527,
      -0.029407095164060593,
      0.023196527734398842,
      0.014871724881231785,
      0.008535024709999561,
      -0.03904128074645996,
      0.023244578391313553,
      -0.010114694945514202,
      -0.037215352058410645,
      -0.01280554011464119,
      -0.008060523308813572,
      0.0023830197751522064,
      0.042404837906360626,
      0.024421822279691696,
      -0.027412986382842064,
      -0.009057577699422836,
      -0.006432801950722933,
      0.02827790006995201,
      0.0014099909458309412,
      -0.007736179977655411,
      0.04757029935717583,
      -0.02102222852408886,
      0.008793298155069351,
      -0.018667738884687424,
      0.008625119924545288,
      -0.040266577154397964,
      0.02209135890007019,
      0.0255870558321476,
      -0.038248442113399506,
      0.006516890600323677,
      0.0032734612468630075,
      0.022379662841558456,
      0.020145300775766373,
      0.010553158819675446,
      -0.031208999454975128,
      -0.0031683500856161118,
      -0.059775203466415405,
      0.03608615696430206,
      0.016685642302036285,
      -0.023460807278752327,
      0.0060033476911485195,
      0.051606565713882446,
      0.032001838088035583,
      0.016505451872944832,
      0.0050333221442997456,
      0.009045564569532871,
      0.005775106605142355,
      0.00945399608463049,
      0.0068472400307655334,
      0.021923180669546127,
      0.014583420008420944,
      -0.0008806812111288309,
      0.019412526860833168,
      -0.0013026273809373379,
      -0.015051915310323238,
      0.012781514786183834,
      -0.002464105375111103,
      0.011904587969183922,
      -0.03596603125333786,
      0.019388500601053238,
      -0.01649343967437744,
      0.030392136424779892,
      -0.010096675716340542,
      -0.00873924046754837,
      0.004213455133140087,
      0.0076701100915670395,
      0.007658097427338362,
      -0.011868549510836601,
      -0.015640538185834885,
      0.006198554299771786,
      -0.01589280553162098,
      -0.01797100156545639,
      -0.010192777961492538,
      -0.004348597954958677,
      0.00486514437943697,
      0.007381805218756199,
      -0.004519779235124588,
      0.01792295090854168,
      -0.029407095164060593,
      0.04812288284301758,
      0.002082702238112688,
      -0.018211256712675095,
      0.015676576644182205,
      0.004576839506626129,
      0.012445159256458282,
      -0.02678832598030567,
      -0.0057691000401973724,
      0.024722140282392502,
      0.009652205742895603,
      0.0049552395939826965,
      -0.0037870043888688087,
      0.0006576954037882388,
      -0.008811316452920437,
      0.04812288284301758,
      -0.03447645530104637,
      -0.018751827999949455,
      0.010421019047498703,
      -0.001073635183274746,
      -0.01459543313831091,
      0.009147672913968563,
      0.0019986133556813,
      0.02006121166050434,
      -0.007315735332667828,
      -0.004757029935717583,
      0.02002517320215702,
      -0.005979322362691164,
      -0.015340220183134079,
      -0.008186656050384045,
      0.00797643419355154,
      -0.03450047969818115,
      -0.01207276526838541,
      -0.024289682507514954,
      0.05357665196061134,
      0.016337275505065918,
      -0.004441696684807539,
      -0.016529478132724762,
      -0.011015648022294044,
      -0.011874555610120296,
      -0.006360725499689579,
      -0.009652205742895603,
      -0.039233483374118805,
      -0.014018823392689228,
      -0.022740045562386513,
      0.019328437745571136,
      -0.010877501219511032,
      -0.015232105739414692,
      0.018823903053998947,
      0.016241172328591347,
      -0.006949347909539938,
      -0.03269857540726662,
      -0.0153762586414814,
      0.012481197714805603,
      -0.027821417897939682,
      -0.007736179977655411,
      -0.009159685112535954,
      0.004690960049629211,
      0.05189487338066101,
      -0.025851335376501083,
      -6.367670721374452e-05,
      -0.02258387953042984,
      4.070710565429181e-05,
      -0.009243774227797985,
      0.04019450023770332,
      -0.029575271531939507,
      -0.019364476203918457,
      -0.000671960529871285,
      -0.009285818785429,
      0.01589280553162098,
      0.007135544903576374,
      0.0069853863678872585,
      0.02316048927605152,
      0.04785860329866409,
      -0.013310073874890804,
      0.010162745602428913,
      0.008060523308813572,
      0.038752976804971695,
      0.025851335376501083,
      0.02815777249634266,
      -0.08817323297262192,
      -0.013850645162165165,
      0.03450047969818115,
      0.015628525987267494,
      -0.016253186389803886,
      -0.004976261872798204,
      -0.00397019786760211,
      0.008408891037106514,
      -0.035653699189424515,
      -0.04581644386053085,
      0.010553158819675446,
      -0.035149164497852325,
      0.02722078189253807,
      0.038752976804971695,
      0.045912545174360275,
      -0.008547037839889526,
      -0.013165920972824097,
      -0.012301006354391575,
      -0.01952064037322998,
      0.021706951782107353,
      0.0025812294334173203,
      0.00018638458277564496,
      0.004153391811996698,
      0.005537855438888073,
      0.013766556046903133,
      -0.03176158294081688,
      -0.039810094982385635,
      -0.0012831067433580756,
      0.0029971690382808447,
      0.024493899196386337,
      0.004525785334408283,
      -0.0307525172829628,
      0.018835917115211487,
      0.05146241560578346,
      -0.0032073913607746363,
      0.021742990240454674,
      0.03832051903009415,
      0.0360381044447422,
      0.03024798259139061,
      -0.009952523745596409,
      -0.029551247134804726,
      0.000356814794940874,
      0.013898695819079876,
      0.010625234805047512,
      -0.0011186829069629312,
      -0.028590230271220207,
      -0.014451280236244202,
      0.0006588216056115925,
      -0.04259704053401947,
      0.008601094596087933,
      0.0023349688854068518,
      -0.0010083161760121584,
      0.017862888053059578,
      0.01788691245019436,
      -0.020193351432681084,
      0.0077782245352864265,
      0.04408661648631096,
      0.028638280928134918,
      0.0012410623021423817,
      -0.006757144816219807,
      -0.013778569176793098,
      -0.020625809207558632,
      0.015232105739414692,
      0.02147871069610119,
      -0.0021202419884502888,
      0.013454225845634937,
      -0.04105941578745842,
      -0.012973718345165253,
      0.002913080155849457,
      0.0036909026093780994,
      0.03747963160276413,
      -0.03428425267338753,
      0.029527220875024796,
      0.005468782503157854,
      -0.005264566745609045,
      0.016121046617627144,
      -0.009916485287249088,
      -0.013910708948969841,
      0.027244808152318,
      -0.011424079537391663,
      -0.0027178737800568342,
      0.03236221894621849,
      -0.018367420881986618,
      0.007477906998246908,
      -0.04053085669875145,
      -0.012577299028635025,
      0.010913539677858353,
      0.012625349685549736,
      -0.0019715847447514534,
      -0.004537797998636961,
      0.006462833844125271,
      0.01797100156545639,
      -0.0007406581426039338,
      -0.006775164045393467,
      0.0009820383274927735,
      -0.0017883910331875086,
      -0.02419358119368553,
      -0.004279525019228458,
      0.009484028443694115,
      -0.01561651285737753,
      -0.018151191994547844,
      -0.016817782074213028,
      0.018295345827937126,
      0.026379892602562904,
      -0.014223039150238037,
      0.004832109436392784,
      0.04485543072223663,
      -0.006979379802942276,
      -0.00845093559473753,
      0.0020541721023619175,
      0.01076938770711422,
      0.012661388143897057,
      -0.006420789286494255,
      0.01204273384064436,
      0.024325720965862274,
      0.022175448015332222,
      0.016277210786938667,
      -0.010246834717690945,
      -0.007009411696344614,
      -0.014535369351506233,
      0.019388500601053238,
      -0.005597919225692749,
      -0.018571637570858,
      0.03584590181708336,
      -0.009135659784078598,
      -0.0048471251502633095,
      -0.02767726592719555,
      0.01081143133342266,
      -0.022463751956820488,
      0.0007965922704897821,
      0.01892000623047352,
      0.0006314176716841757,
      -0.018703777343034744,
      -0.017214201390743256,
      -0.017262252047657967,
      0.011844524182379246,
      -0.005120414309203625,
      -0.009417958557605743,
      0.04137174412608147,
      -0.041203565895557404,
      -0.030151881277561188,
      0.008565056137740612,
      -0.009880447760224342,
      -0.013418188318610191,
      -0.01022280938923359,
      -0.02001316100358963,
      0.030055779963731766,
      -0.013658442534506321,
      0.008523011580109596,
      0.0045137726701796055,
      0.014667509123682976,
      -0.007658097427338362,
      -0.003339531132951379,
      0.0011922606499865651,
      -0.004510769620537758,
      -0.007453881669789553,
      -0.025923410430550575,
      0.010174758732318878,
      -0.0062285857275128365,
      0.022427715361118317,
      -0.0014647988136857748,
      0.023316655308008194,
      0.011868549510836601,
      0.013610390946269035,
      -0.006114465184509754,
      -0.006042389199137688,
      0.028013620525598526,
      0.04651318117976189,
      -0.0035407438408583403,
      -0.02671624906361103,
      0.028446078300476074,
      -0.021887142211198807,
      0.01081143133342266,
      -0.013958759605884552,
      0.018595661967992783,
      0.0050843763165175915,
      -0.0575648657977581,
      -0.004372623283416033,
      -0.005426737945526838,
      0.023785149678587914,
      -0.005108401644974947,
      0.02210337109863758,
      0.013117870315909386,
      0.0024340737145394087,
      0.00026540562976151705,
      -0.01690187118947506,
      -0.003654864616692066,
      -0.008312789723277092,
      0.028109721839427948,
      0.011760435067117214,
      -0.00602136692032218,
      0.0127935279160738,
      0.02355690859258175,
      0.003150331089273095,
      -0.002395032439380884,
      -0.009171698242425919,
      0.019580703228712082,
      0.015232105739414692,
      0.023208539932966232,
      -0.04122759401798248,
      -0.013802594505250454,
      -0.0062285857275128365,
      0.02871035784482956,
      -0.021334558725357056,
      -0.0400983989238739,
      0.012030720710754395,
      0.005823157262057066,
      0.0010548654245212674,
      0.009411951526999474,
      0.021442672237753868,
      -0.007357779890298843,
      0.03144925460219383,
      0.0029190864879637957,
      -0.0037149281706660986,
      0.026043538004159927,
      0.008138605393469334,
      -0.010589196346700191,
      0.008595088496804237,
      -0.028470102697610855,
      -0.007087494246661663,
      0.004414667841047049,
      0.0015481369337067008,
      -0.028422052040696144,
      0.015820728614926338,
      -0.0029250928200781345,
      0.03426022455096245,
      0.038176365196704865,
      0.0062586176209151745,
      -0.03147327899932861,
      -0.023460807278752327,
      -0.008685183711349964,
      0.011538200080394745,
      0.029046712443232536,
      -0.013418188318610191,
      -0.012901641428470612,
      0.01997712254524231,
      0.01902811974287033,
      -0.005576896946877241,
      -0.006498871836811304,
      -0.020373541861772537,
      0.02201928198337555,
      0.01052312646061182,
      -0.00012219170457683504,
      0.013430200517177582,
      0.01052913349121809,
      0.11628295481204987,
      -0.005438750609755516,
      0.0236770361661911,
      -0.010186770930886269,
      0.008925437927246094,
      -0.031497303396463394,
      0.03961789235472679,
      -0.004432687070220709,
      0.02868633158504963,
      -0.034668657928705215,
      0.04221263527870178,
      -0.0052285282872617245,
      0.016060981899499893,
      0.03084861859679222,
      0.038248442113399506,
      0.009958529844880104,
      -0.009664218872785568,
      -0.006099449470639229,
      -0.022776082158088684,
      0.013514289632439613,
      -0.019172271713614464,
      0.01740640588104725,
      -0.02420559525489807,
      0.02515459805727005,
      -0.022716019302606583,
      0.024481886997818947,
      0.055210378021001816,
      -0.004147385247051716,
      -0.01435517892241478,
      -0.01792295090854168,
      -0.023845212534070015,
      -0.0012463178718462586,
      0.012246949598193169,
      -0.002046664012596011,
      -0.019364476203918457,
      0.015820728614926338,
      0.01465549599379301,
      -0.0009512558463029563,
      0.003243429586291313,
      -0.008973488584160805,
      0.003156337421387434,
      0.003276464529335499,
      0.012252955697476864,
      0.003348540747538209,
      -0.006600979715585709,
      -0.03747963160276413,
      0.0007526708650402725,
      -0.010739355348050594,
      0.01947258971631527,
      -0.038680899888277054,
      -0.025490952655673027,
      -0.021214431151747704,
      -0.013994798064231873,
      0.03904128074645996,
      -0.006015360355377197,
      0.01686583273112774,
      0.04689759016036987,
      0.0068292212672531605,
      -0.003967194817960262,
      0.01995309814810753,
      -0.039209458976984024,
      0.037143275141716,
      0.0008784288074821234,
      0.01901610754430294,
      -0.011958644725382328
    ],
    "updated_at": "2025-10-29T15:04:13.022981",
    "job_id": "architecture_v1",
    "position": "架构师",
    "target_profile": "第一性原理思考者：能从本质上理解为何选择特定技术，而非盲从潮流。\n\n系统韧性的偏执狂：默认任何服务都会失败，并将容错、重试、幂等性设计融入血液。\n\n务实的架构权衡者：深刻理解“不重复造轮子”，知道何时“买” (开源) 何时“造” (自研)。\n\n追求卓越的工程品味：对代码洁癖，对系统设计的优雅性有不懈的追求。\n\n技术布道者：不仅自己能做，还能影响和带动整个团队共同成长。",
    "keywords": {
      "positive": [
        "Microservices",
        "微服务",
        "EDA",
        "事件驱动",
        "高可用",
        "高并发",
        "MLOps",
        "LLM"
      ],
      "negative": []
    },
    "candidate_filters": null,
    "description": "此角色是Preseen平台的大脑与脊梁。我们需要一位顶尖的技术专家，他/她不仅能编写高质量的Java代码，更能从宏观视角设计出支撑未来AI业务的、健壮、可扩展的分布式系统架构。他/她将直接负责我们核心工作流引擎和数据平台的设计与实现，解决业界最前沿的AI基础设施工程挑战。",
    "drill_down_questions": "测试字符串格式的追问问",
    "version": 1,
    "current": true,
    "created_at": "2025-10-29T14:37:32.675080",
    "background": "我们是 Preseen (星尘数据)，一家致力于构建全球领先AI基础设施的公司。当前，绝大多数企业的AI潜力因无法有效利用其私有数据而被封印。我们的核心产品 Preseen (MorningStar)，正是一个端到端的企业私有化模型训练平台，旨在打通从数据到模型的全生命周期，帮助企业唤醒数据资产、真正拥有自己的AI能力。\n\n该岗位将加入我们最核心的平台团队，直接参与“AI时代操作系统”的建设。候选人将面对的技术挑战包括：设计和实现一个能可靠编排数千个AI任务、并管理PB级数据的分布式工作流引擎。这是一个直接定义行业未来的核心岗位。"
  },
  {
    "responsibilities": "● 前沿算法转化： 跟踪、阅读和深入理解我们核心研究方向（数据归因、LoRA优化、幻觉、RL、GraphRAG、CoT）的最新论文，并将其核心思想工程化实现。\n● 实验设计与执行： 针对具体业务问题，设计严谨的算法实验方案、消融实验和A/B测试，评估不同方法的优劣。\n● 模型训练与优化： 负责模型的全流程训练、微调、调试和迭代；分析实验结果，定位性能瓶颈（无论是算法层面还是工程层面），并提出改进方案。\n● 算法融合创新： 结合业务需求，将来自不同论文的算法模块（例如，将一种新的LoRA变体应用到RLHF流程中）进行创新性组合与优化。\n● 业务需求对接： 与产品和业务方沟通，将模糊的客户需求转化为清晰、可执行的算法技术指标和研发路径。",
    "requirements": "- 高效微调工程化： 负责参数高效微调（PEFT）算法的工程实现、性能基准测试和迭代优化。重点是将QLoRA、HydraLoRA等前沿LoRA变体 应用于业务场景，管理和部署大规模的LoRA适配器。\n- RLHF/DPO流水线搭建： 搭建和维护可扩展、高通量的强化学习对齐（RLHF/DPO）训练流水线。重点攻坚Asynchronous RLHF（异步RLHF） 等技术，通过解耦数据生成和模型训练，最大化GPU集群效率，提升模型迭代速度。\n- 高级RAG系统实现： 主导GraphRAG（图谱RAG）系统的工程落地。负责自动化构建知识图谱、实现图检索算法（如社区发现、多跳查询），并将其与LLM推理流程高效集成。\n- 幻觉闭环优化： 开发和部署模型幻觉检测系统。构建基于RAG-HAT（幻觉感知微调）的闭环优化流程，即自动捕获RAG系统中的幻觉案例，生成偏好数据，并反馈给DPO流程以持续优化模型。\n- 数据归因与溯源系统： 实现可扩展的数据归因（Data Attribution）工具。应用DataInf、Influence Functions或Fact Tracing 等技术，构建数据清洗和优化流水线，量化分析训练数据对模型（尤其是LoRA微调后）具体行为的影响。\n- 推理与智能体工程化： 将高级推理算法（如CoT、Visual CoT）和智能体框架（如Reflexion）工程化，交付解决复杂多步骤产品任务的可靠API。\n- 负面关键词清单（出现则大概率淘汰）\n● 基础概念罗列（红灯）： 技能列表里只有 Machine Learning, Deep Learning, AI, CNN, RNN, LSTM。这表明知识体系停留在5年前。\n● 项目经验过浅（红灯）：\n    ○ 项目描述为“调用XX（如OpenAI）的API实现了XX功能”。（我们找的是造模型的人，不是调API的人）。\n    ○ 项目是简单的数据分析或可视化（如 Pandas, Matplotlib, Seaborn）。\n    ○ 唯一的“深度学习”项目是在 MNIST, CIFAR-10 或 IMDB 情感分类上跑通一个标准模型。\n● 与我们方向不符（黄灯）： 经验仅集中在 CV (如目标检测, 分割), Speech (ASR/TTS) 或传统机器学习（SVM, XGBoost）。如果他们没有展现出向LLM迁移的强烈意愿和自学能力，则不匹配。\n● 泛泛的流行词（黄灯）： 只提 SFT 或 RAG，但没有提供任何关于“如何优化”、“踩了什么坑”、“做了哪些改进”的细节。",
    "job_embedding": [
      -0.007708379067480564,
      0.02563655562698841,
      -0.004718117415904999,
      -0.014760441146790981,
      0.04128101468086243,
      -0.005675804801285267,
      -0.052773263305425644,
      0.011746739037334919,
      -0.02504720911383629,
      0.022703219205141068,
      0.029467305168509483,
      -0.03450353443622589,
      -0.050121206790208817,
      -0.00740031199529767,
      0.029145844280719757,
      -0.0002505139564163983,
      -0.03672697767615318,
      -0.03442316874861717,
      -0.018711738288402557,
      0.016568662598729134,
      0.04184357449412346,
      0.0017412498127669096,
      -0.008177177049219608,
      -0.006496201269328594,
      -0.0005571162328124046,
      -0.009583570994436741,
      0.031128190457820892,
      0.023091651499271393,
      0.01852422021329403,
      0.002285390393808484,
      0.06831057369709015,
      -0.014961354434490204,
      -0.04007553681731224,
      0.007447191514074802,
      0.029092267155647278,
      0.038093190640211105,
      0.012490118853747845,
      -0.005779610015451908,
      0.02483290247619152,
      0.009489811956882477,
      0.023131834343075752,
      -0.04176320880651474,
      -0.007855715230107307,
      -0.011525734327733517,
      3.641556031652726e-05,
      0.015055114403367043,
      -0.018751921132206917,
      0.01997079700231552,
      0.012711123563349247,
      0.03761099651455879,
      -0.043799132108688354,
      -0.017345527186989784,
      0.0028462738264352083,
      0.027136709541082382,
      -0.03128892183303833,
      0.027123315259814262,
      0.0148408068343997,
      0.005116595886647701,
      0.039727285504341125,
      -0.014626498334109783,
      0.04264722764492035,
      -0.018939441069960594,
      0.028583286330103874,
      0.0067172059789299965,
      -0.0401826873421669,
      -0.019635939970612526,
      -0.05866672471165657,
      0.008043235167860985,
      -0.011485551483929157,
      0.03211936354637146,
      0.017037460580468178,
      0.008083418011665344,
      0.03150322660803795,
      -0.0320122092962265,
      0.015095297247171402,
      -0.031744323670864105,
      0.03924509137868881,
      0.0548359751701355,
      -0.015456940978765488,
      -0.04441526532173157,
      -0.02011813223361969,
      -0.017077643424272537,
      0.05571999400854111,
      -0.07431118935346603,
      -0.0048554083332419395,
      -0.07993676513433456,
      -0.006710508838295937,
      -0.07056079804897308,
      -0.057488035410642624,
      -0.012128475122153759,
      -0.0312353428453207,
      0.02581068128347397,
      -0.025542795658111572,
      0.00040517543675377965,
      0.010072460398077965,
      0.006365607492625713,
      0.023064862936735153,
      -8.957391401054338e-05,
      0.01857779733836651,
      0.04481709375977516,
      0.016354354098439217,
      -0.06750691682100296,
      0.04010232537984848,
      -0.004101982805877924,
      0.030779939144849777,
      0.014345220290124416,
      -0.015389969572424889,
      -0.013327258639037609,
      -0.019662728533148766,
      -0.014747046865522861,
      -0.12215537577867508,
      -0.0051032016053795815,
      -0.06627465039491653,
      0.0027993940748274326,
      -0.019421633332967758,
      -0.06166703253984451,
      -0.01622041128575802,
      -0.01697048917412758,
      -0.004446884151548147,
      -0.03541434183716774,
      -0.014238066039979458,
      0.009141561575233936,
      0.0063019851222634315,
      -0.023667603731155396,
      0.008833494037389755,
      -0.009201835840940475,
      0.07468622177839279,
      -0.03549470752477646,
      0.03161038085818291,
      -0.017265161499381065,
      0.021216459572315216,
      0.014398796483874321,
      0.027752844616770744,
      -0.028422554954886436,
      -0.006824360229074955,
      -0.0321461521089077,
      -0.03380703553557396,
      0.017278555780649185,
      -0.06659611314535141,
      0.011063633486628532,
      0.038173556327819824,
      -0.010983268730342388,
      -0.04326336085796356,
      0.036593034863471985,
      -0.055880725383758545,
      0.01619362272322178,
      -0.015162267722189426,
      0.005715987645089626,
      -0.03286943957209587,
      0.038923632353544235,
      0.011994532309472561,
      -0.01699727773666382,
      -0.06670325994491577,
      0.015001537278294563,
      0.014974748715758324,
      -0.05242501571774483,
      0.003147643990814686,
      0.03953976556658745,
      0.001913700602017343,
      -0.04275438189506531,
      0.009536691941320896,
      0.022756796330213547,
      -0.01239635981619358,
      0.013059373944997787,
      0.009275504387915134,
      -0.00967063382267952,
      -0.06681041419506073,
      -0.002546577947214246,
      -0.03865574672818184,
      -0.004939122125506401,
      -0.033324845135211945,
      -0.06997145712375641,
      -0.021524526178836823,
      -0.012804883532226086,
      0.005930295214056969,
      0.011251152493059635,
      0.0040684971027076244,
      -0.02172544039785862,
      0.0120548065751791,
      -0.037396688014268875,
      -0.02504720911383629,
      0.0029919359367340803,
      -0.059041764587163925,
      -0.01052786409854889,
      -0.015778401866555214,
      -0.04235255345702171,
      0.016568662598729134,
      0.03147643804550171,
      -0.029092267155647278,
      0.06632822751998901,
      0.010380527935922146,
      0.04730841889977455,
      0.08272276073694229,
      0.013695599511265755,
      0.0013796056155115366,
      -0.03386061266064644,
      0.06332791596651077,
      -0.012838369235396385,
      0.01928769052028656,
      -0.033458784222602844,
      0.004239273723214865,
      0.005280674900859594,
      0.05057661235332489,
      -0.04886214807629585,
      0.0312353428453207,
      0.029386939480900764,
      0.03244082257151604,
      0.018604585900902748,
      -0.030163805931806564,
      -0.04819243773818016,
      0.05352334305644035,
      0.03238724544644356,
      0.06439945846796036,
      0.02895832434296608,
      -0.045326072722673416,
      -0.012470027431845665,
      0.018725132569670677,
      0.024390893056988716,
      0.04112028330564499,
      0.07382899522781372,
      -0.007869109511375427,
      0.011800316162407398,
      -0.021082516759634018,
      0.013133042491972446,
      -0.033324845135211945,
      0.06316718459129333,
      0.03937903419137001,
      -0.06697114557027817,
      0.0321461521089077,
      -0.02491326630115509,
      0.009054499678313732,
      0.0075610424391925335,
      -0.05866672471165657,
      -0.03238724544644356,
      0.017506258562207222,
      -0.05571999400854111,
      0.006516292691230774,
      -0.02653396874666214,
      0.03870932385325432,
      -0.043049052357673645,
      0.022328181192278862,
      0.021524526178836823,
      0.002896502148360014,
      -0.03311053663492203,
      0.04012911021709442,
      0.016287382692098618,
      -0.018296517431735992,
      -0.03380703553557396,
      0.022783584892749786,
      -0.018751921132206917,
      0.07479337602853775,
      -0.04829959198832512,
      0.01773395948112011,
      -0.023105045780539513,
      -0.005950386635959148,
      -0.022944316267967224,
      0.007045364938676357,
      -0.016247199848294258,
      -0.04883535951375961,
      -0.03203899785876274,
      -0.021484345197677612,
      0.009355869144201279,
      0.03469105437397957,
      -0.009737605229020119,
      -0.011659677140414715,
      0.013943392783403397,
      -0.01210168655961752,
      0.016689209267497063,
      -0.030378112569451332,
      -0.023011285811662674,
      -0.02986913174390793,
      -0.008357998915016651,
      0.027618901804089546,
      0.026828641071915627,
      -0.02986913174390793,
      -0.010688595473766327,
      0.03153001517057419,
      0.04650476574897766,
      -0.0035293796099722385,
      -0.02731083333492279,
      -0.01325359009206295,
      0.024270344525575638,
      0.09059856832027435,
      -0.004242622293531895,
      -0.006442624609917402,
      0.017131220549345016,
      -0.022274604067206383,
      0.0037905669305473566,
      -0.01853761449456215,
      0.013809450902044773,
      -0.014586316421627998,
      -0.01090290304273367,
      0.025891045108437538,
      -0.09236660599708557,
      0.017305344343185425,
      -0.008130297064781189,
      0.0312353428453207,
      0.018001845106482506,
      0.01845724880695343,
      0.007882503792643547,
      0.06536383926868439,
      0.008733037859201431,
      -0.015497123822569847,
      -0.017077643424272537,
      0.007038667798042297,
      0.002660428872331977,
      0.012530301697552204,
      -0.04926397651433945,
      -0.02573031559586525,
      -0.0593632273375988,
      0.014988142997026443,
      -0.0053442977368831635,
      -0.0016541873337700963,
      0.020024374127388,
      -0.018885863944888115,
      -0.030806727707386017,
      0.03528039902448654,
      -0.019836854189634323,
      0.019006412476301193,
      -0.06547099351882935,
      -0.03444995731115341,
      -0.06879276037216187,
      0.015912344679236412,
      -0.02413640171289444,
      -0.04562074691057205,
      -0.011920864693820477,
      0.0026955886278301477,
      0.020305652171373367,
      -0.03222651779651642,
      -0.01859119161963463,
      0.04604936018586159,
      -0.004584175068885088,
      0.03466426581144333,
      0.02984234318137169,
      -0.025850864127278328,
      -0.008157085627317429,
      -0.0513266883790493,
      0.049397919327020645,
      0.004316290374845266,
      -0.002178236609324813,
      -0.058291688561439514,
      -0.025489218533039093,
      -0.052719686180353165,
      -0.003656624583527446,
      -0.02168525755405426,
      0.020600324496626854,
      0.033351629972457886,
      -0.020506566390395164,
      0.03284265100955963,
      0.0580773800611496,
      0.016541874036192894,
      -0.012463330291211605,
      0.002868039533495903,
      0.04503139853477478,
      -0.04326336085796356,
      -0.012617364525794983,
      -0.022221026942133904,
      -0.012463330291211605,
      -0.009409446269273758,
      0.015282816253602505,
      -0.04575468972325325,
      0.03536076471209526,
      -0.0025783891323953867,
      -0.015443546697497368,
      0.0013084488455206156,
      0.04002195969223976,
      -0.00740031199529767,
      -0.033512361347675323,
      0.03455711156129837,
      -0.013447806239128113,
      0.02649378590285778,
      0.03131571039557457,
      0.007855715230107307,
      -0.054300207644701004,
      -0.037369899451732635,
      -0.002647034591063857,
      0.01920732483267784,
      0.009402749128639698,
      -0.0265741515904665,
      0.03728953376412392,
      -0.010467590764164925,
      0.002647034591063857,
      0.005746124312281609,
      -0.028449343517422676,
      0.014010364189743996,
      0.029279785230755806,
      -0.008532124571502209,
      -0.022221026942133904,
      -0.045433226972818375,
      -0.0059905690141022205,
      0.02724386379122734,
      -0.01841706596314907,
      -0.010186311788856983,
      -0.019568970426917076,
      -0.005528468172997236,
      -0.0009802902350202203,
      0.005913552362471819,
      0.016340959817171097,
      0.026667911559343338,
      0.020292257890105247,
      0.004597569350153208,
      -0.011599402874708176,
      -0.059041764587163925,
      -0.0057528214529156685,
      -4.986865405953722e-06,
      0.03774493932723999,
      0.0451117642223835,
      0.038280706852674484,
      0.00967733096331358,
      0.03236046060919762,
      0.01241645123809576,
      -0.031047824770212173,
      0.03560186177492142,
      0.007895898073911667,
      -0.016381142660975456,
      -0.037343110889196396,
      -0.019435027614235878,
      0.03436959162354469,
      0.014947960153222084,
      -0.00605419185012579,
      0.011586008593440056,
      -0.011606100015342236,
      -0.03458390012383461,
      -0.032547976821660995,
      -0.027645690366625786,
      0.05877387896180153,
      0.06113126501441002,
      -0.03161038085818291,
      0.02725725807249546,
      -0.03155680373311043,
      -0.010079157538712025,
      0.01326698437333107,
      0.03841464966535568,
      -0.03779851645231247,
      -0.05888103321194649,
      -0.011043542064726353,
      0.024404285475611687,
      0.03841464966535568,
      -0.0032380549237132072,
      0.0069047254510223866,
      0.04237934201955795,
      -0.005448102951049805,
      -0.04762988165020943,
      0.002020854502916336,
      -0.04428132250905037,
      0.015001537278294563,
      -0.009817969985306263,
      -0.015791796147823334,
      -0.0169169120490551,
      -0.062202803790569305,
      -0.005568651016801596,
      -0.012764700688421726,
      0.0321461521089077,
      0.012778094969689846,
      -0.024350710213184357,
      -0.06113126501441002,
      0.09767071902751923,
      -0.007735167630016804,
      0.02662772871553898,
      0.007929383777081966,
      0.026775065809488297,
      0.017948267981410027,
      0.0016717673279345036,
      -2.8750504498020746e-05,
      0.03372666984796524,
      0.005803050007671118,
      0.00846515316516161,
      0.0008517893147654831,
      0.019662728533148766,
      0.023145228624343872,
      0.027565324679017067,
      0.0071525187231600285,
      -0.008639277890324593,
      -0.01856440305709839,
      0.014532739296555519,
      -0.007601225282996893,
      0.023895306512713432,
      -0.024886479601264,
      -0.0027173543348908424,
      -0.002774279797449708,
      0.015014931559562683,
      -0.013005796819925308,
      -0.016541874036192894,
      -0.025529401376843452,
      0.002658754587173462,
      0.05293399468064308,
      -0.009804575704038143,
      -0.022167449817061424,
      -0.009375960566103458,
      0.024377498775720596,
      -0.005022835917770863,
      0.0036767160054296255,
      0.03916472569108009,
      -0.015028325840830803,
      -0.006288590840995312,
      -0.0025901091285049915,
      -0.0469333790242672,
      0.015175662003457546,
      -0.03528039902448654,
      -0.016488296911120415,
      0.001568799139931798,
      -0.057541608810424805,
      -0.015805190429091454,
      0.05162136256694794,
      0.009242018684744835,
      -0.01780093088746071,
      0.015470335260033607,
      0.009945215657353401,
      0.017358921468257904,
      0.023908700793981552,
      0.046129725873470306,
      0.007882503792643547,
      -0.008237451314926147,
      0.012918734923005104,
      0.03916472569108009,
      0.01679636351764202,
      -0.046236880123615265,
      0.012670941650867462,
      0.0013796056155115366,
      0.0044937641359865665,
      0.018242940306663513,
      -0.017881296575069427,
      -0.02092178724706173,
      -0.011519037187099457,
      -0.012724517844617367,
      -0.02898511290550232,
      -0.029494093731045723,
      -0.06107768788933754,
      -0.013916604220867157,
      -0.009898335672914982,
      0.013521474786102772,
      0.012557090260088444,
      -0.010661806911230087,
      0.011385095305740833,
      -0.024431074038147926,
      -0.030297746881842613,
      -0.013133042491972446,
      -0.02723046950995922,
      -0.037450265139341354,
      -0.006944908294826746,
      0.0020057859364897013,
      0.009469720534980297,
      0.030940670520067215,
      -0.012275811284780502,
      -0.029949497431516647,
      0.01171995047479868,
      -0.017881296575069427,
      -0.018859075382351875,
      -0.0027257257606834173,
      0.03873611241579056,
      0.03528039902448654,
      -0.026895612478256226,
      0.018604585900902748,
      -0.0515945740044117,
      -0.002375801559537649,
      0.03051205538213253,
      0.002667126012966037,
      -0.01840367168188095,
      0.010608229786157608,
      -0.005287372041493654,
      0.032521188259124756,
      -0.023908700793981552,
      -0.01442558504641056,
      -0.02413640171289444,
      0.007728470489382744,
      0.026721488684415817,
      0.0020710828248411417,
      -0.05947037786245346,
      0.010922994464635849,
      -0.004326336085796356,
      -0.015711430460214615,
      0.004118725657463074,
      -0.010300162248313427,
      0.009576873853802681,
      0.005136686842888594,
      -0.02642681449651718,
      0.019810065627098083,
      0.00360974483191967,
      0.005930295214056969,
      -0.01090960018336773,
      -0.035816170275211334,
      -0.040638092905282974,
      -0.03763778507709503,
      -0.008351301774382591,
      -0.042031094431877136,
      -0.0053945258259773254,
      -0.010065763257443905,
      0.03878968954086304,
      0.00877322070300579,
      0.010715384036302567,
      0.026909006759524345,
      -0.003053884254768491,
      -0.014171094633638859,
      -0.02965482510626316,
      -0.022274604067206383,
      -0.01695709489285946,
      -0.009509903378784657,
      -0.015925738960504532,
      -0.03056563250720501,
      0.007018576376140118,
      0.00707215303555131,
      -0.0050328816287219524,
      0.02005116268992424,
      -0.015925738960504532,
      -0.024886479601264,
      -0.014546133577823639,
      -0.023962276056408882,
      0.007467282935976982,
      0.010742172598838806,
      -0.02733762189745903,
      -0.008344604633748531,
      -0.0040617999620735645,
      0.01449255645275116,
      -0.0676676481962204,
      -0.03763778507709503,
      0.006409138906747103,
      -0.04404022544622421,
      -0.010594835504889488,
      0.05068376660346985,
      0.005089807324111462,
      -0.02092178724706173,
      -0.021176276728510857,
      -0.008257542736828327,
      -0.025248123332858086,
      0.04570111259818077,
      -0.03761099651455879,
      -0.009422840550541878,
      -0.013695599511265755,
      0.00844506174325943,
      -0.00968402810394764,
      -0.030833516269922256,
      0.015925738960504532,
      -0.0010748868808150291,
      -0.0050261844880878925,
      0.03193184360861778,
      0.01446576789021492,
      -0.02812788262963295,
      -0.015925738960504532,
      -0.0031627125572413206,
      -0.007842321880161762,
      0.042218610644340515,
      0.03064599819481373,
      -0.004078542813658714,
      -0.028744017705321312,
      0.03297659382224083,
      -0.04476351663470268,
      -0.01682315208017826,
      0.004651146475225687,
      -0.032494399696588516,
      -0.011505642905831337,
      -0.03616442158818245,
      -0.049317553639411926,
      0.025931227952241898,
      0.015403363853693008,
      -0.0004221275157760829,
      0.001489270944148302,
      -0.010253283195197582,
      -0.0018182666972279549,
      0.025435641407966614,
      0.02341311424970627,
      -0.02502042055130005,
      -0.046156514436006546,
      -0.014372008852660656,
      0.015604277141392231,
      -0.02653396874666214,
      0.045433226972818375,
      0.03219972923398018,
      0.04516534134745598,
      0.015631066635251045,
      -0.023614026606082916,
      -0.03710201755166054,
      0.015858767554163933,
      0.03536076471209526,
      0.00811020564287901,
      -0.028529709205031395,
      -0.022703219205141068,
      0.004299547523260117,
      -0.03937903419137001,
      0.03380703553557396,
      0.051782090216875076,
      -0.025891045108437538,
      -0.005438057240098715,
      -0.04583505541086197,
      -0.027484958991408348,
      -0.0023356187157332897,
      0.010695292614400387,
      0.013876422308385372,
      -0.00841827318072319,
      0.044495631009340286,
      -0.017198190093040466,
      0.005186915397644043,
      -0.010085854679346085,
      -0.0038809780962765217,
      -0.05218391865491867,
      0.001626561745069921,
      0.007018576376140118,
      -0.01614004746079445,
      0.030806727707386017,
      -0.02745817042887211,
      -0.05063018947839737,
      -0.0037738243117928505,
      0.019796671345829964,
      -0.015470335260033607,
      -0.008056629449129105,
      -0.025984805077314377,
      -0.015510518103837967,
      0.005109898746013641,
      0.012824974954128265,
      0.05210355296730995,
      -0.004929076414555311,
      -0.0609169565141201,
      -0.012336085550487041,
      -0.04425453394651413,
      -0.0012155263684689999,
      -0.028690440580248833,
      0.0036130934022367,
      0.04087918996810913,
      0.01088281162083149,
      0.008558913134038448,
      -0.013849633745849133,
      -0.06906064599752426,
      0.02978876605629921,
      -0.02562316134572029,
      -0.04554038122296333,
      0.008264239877462387,
      -0.011137302033603191,
      -0.03040490113198757,
      0.017345527186989784,
      -0.0011728322133421898,
      0.0033987858332693577,
      0.028770804405212402,
      0.018778709694743156,
      -0.0041354685090482235,
      -0.02102893963456154,
      -0.03043168969452381,
      0.0028479481115937233,
      -0.03691449761390686,
      -0.030940670520067215,
      -0.0044267927296459675,
      -0.0192475076764822,
      -0.025221334770321846,
      -0.031101401895284653,
      -0.01680975779891014,
      0.03361951559782028,
      -0.016314171254634857,
      -0.019501999020576477,
      0.008652672171592712,
      0.03134249895811081,
      -0.020252075046300888,
      0.03686092048883438,
      -0.005856627132743597,
      0.012155263684689999,
      0.02180580608546734,
      -0.043825920671224594,
      -5.703011993318796e-05,
      -0.009288898669183254,
      0.015041720122098923,
      -0.016354354098439217,
      0.013461200520396233,
      0.019542181864380836,
      -0.010333647951483727,
      0.057434458285570145,
      0.0049793049693107605,
      0.00075133255450055,
      -0.00500944210216403,
      0.0016977186314761639,
      -0.03372666984796524,
      -0.007098941598087549,
      -0.009175047278404236,
      -0.006338819395750761,
      0.019167141988873482,
      -0.004718117415904999,
      -0.0005169335636310279,
      0.0167829692363739,
      0.0352536141872406,
      -0.004641100764274597,
      0.031797900795936584,
      -0.02329256571829319,
      -0.010253283195197582,
      0.0019203976262360811,
      0.022020112723112106,
      -0.021082516759634018,
      -0.006978393532335758,
      -0.009014316834509373,
      -0.02812788262963295,
      -0.018108999356627464,
      0.009757696650922298,
      -0.009791181422770023,
      0.000660502933897078,
      -0.023011285811662674,
      -0.0322800949215889,
      0.00621157418936491,
      0.033405207097530365,
      -0.03308374807238579,
      0.009255412966012955,
      0.027163498103618622,
      -0.007869109511375427,
      -0.07436476647853851,
      0.008699552156031132,
      0.020613718777894974,
      0.005300766322761774,
      0.023560449481010437,
      0.024538228288292885,
      0.02656075730919838,
      0.010152826085686684,
      0.022663036361336708,
      -0.014157700352370739,
      0.029413728043437004,
      -0.042861536145210266,
      -0.005310812033712864,
      0.011431975290179253,
      0.0162070170044899,
      -0.049478285014629364,
      0.024698959663510323,
      -0.016729392111301422,
      0.0193814504891634,
      -0.0019706259481608868,
      0.013179921545088291,
      -0.027096526697278023,
      0.005883415229618549,
      0.0038173554930835962,
      -0.007701681926846504,
      -0.007373523432761431,
      0.018336700275540352,
      0.012597273103892803,
      0.024283738806843758,
      -0.01362862903624773,
      0.009931821376085281,
      -0.024310527369379997,
      -0.024886479601264,
      0.009784485213458538,
      -0.04401343688368797,
      0.00848524458706379,
      0.00018835635273717344,
      0.0016885100631043315,
      0.005277326330542564,
      0.003465757006779313,
      0.0069047254510223866,
      -0.039673708379268646,
      0.05850599333643913,
      -0.009570176713168621,
      0.03244082257151604,
      -0.01363532617688179,
      -0.008157085627317429,
      -0.03964691981673241,
      0.008311119861900806,
      -0.002760885516181588,
      0.008311119861900806,
      -0.01754644140601158,
      0.020653901621699333,
      0.000805746647529304,
      -0.010688595473766327,
      -0.00024716538609936833,
      -0.010989965870976448,
      -0.005662410520017147,
      0.005414617247879505,
      -0.007547648623585701,
      -0.009067893028259277,
      0.038334283977746964,
      0.011016753502190113,
      -0.03375345841050148,
      -0.007587831001728773,
      -0.001628236030228436,
      -0.02893153578042984,
      0.01853761449456215,
      -0.010280070826411247,
      -0.023051468655467033,
      0.001670930185355246,
      -0.022341575473546982,
      -0.013956787064671516,
      0.032655131071805954,
      -0.014385402202606201,
      -0.01841706596314907,
      0.015108690597116947,
      0.01482741255313158,
      -0.040638092905282974,
      -0.024377498775720596,
      0.05355013161897659,
      -0.01999758556485176,
      -0.02161828614771366,
      0.02745817042887211,
      -0.009864849969744682,
      0.019461816176772118,
      0.005682501941919327,
      -0.010065763257443905,
      -0.017867902293801308,
      0.0048621054738759995,
      0.06113126501441002,
      0.006737297400832176,
      0.018095605075359344,
      0.005143383983522654,
      0.006770783104002476,
      -0.010594835504889488,
      -0.010963177308440208,
      0.002894827863201499,
      0.016367748379707336,
      -0.0010338671272620559,
      0.011023450642824173,
      0.034878574311733246,
      -0.03544113039970398,
      -0.012570484541356564,
      -0.01123106200248003,
      0.024551622569561005,
      0.015443546697497368,
      0.0052237496711313725,
      0.0016273988876491785,
      -0.0048554083332419395,
      -0.008090115152299404,
      0.047737035900354385,
      -0.032494399696588516,
      0.01764019951224327,
      -0.002886456437408924,
      0.03964691981673241,
      0.01609986461699009,
      0.01447916217148304,
      0.00619817990809679,
      0.0057528214529156685,
      0.019059989601373672,
      0.024658776819705963,
      -0.0465315543115139,
      0.03161038085818291,
      -0.014572922140359879,
      0.025221334770321846,
      0.006074283272027969,
      0.004892242606729269,
      0.026868823915719986,
      -0.015577489510178566,
      -0.015872161835432053,
      -0.01774735376238823,
      -0.021832594648003578,
      0.033324845135211945,
      -0.01770717091858387,
      0.0073936148546636105,
      0.02085481584072113,
      0.04189715161919594,
      -0.038280706852674484,
      0.011961046606302261,
      0.003736990038305521,
      0.019555576145648956,
      -0.012563787400722504,
      -0.0006893842364661396,
      0.007373523432761431,
      0.011519037187099457,
      -0.01687672920525074,
      -0.013695599511265755,
      -0.023948881775140762,
      0.018604585900902748,
      0.032681919634342194,
      -0.024350710213184357,
      0.019863642752170563,
      0.04516534134745598,
      -0.023707786574959755,
      0.03388740122318268,
      -0.018765315413475037,
      -0.025917833670973778,
      -0.021337008103728294,
      -0.0012297576759010553,
      0.013494686223566532,
      0.05317509174346924,
      -0.00880670640617609,
      -0.025261517614126205,
      -0.01080244593322277,
      -0.02009134367108345,
      -0.006613400764763355,
      -0.0193144790828228,
      0.012262417003512383,
      0.013836239464581013,
      0.004339730367064476,
      -0.01926090195775032,
      0.017318738624453545,
      0.011981138028204441,
      0.01206820085644722,
      0.03769136220216751,
      0.011405186727643013,
      -0.004091937094926834,
      -0.0021983280312269926,
      0.017466075718402863,
      -0.0005642319447360933,
      0.024631988257169724,
      -0.006559824105352163,
      0.036646611988544464,
      0.03718238323926926,
      0.002305481815710664,
      0.004386610351502895,
      -0.003246426349505782,
      -0.017466075718402863,
      0.041334591805934906,
      -0.0290386900305748,
      0.030351324006915092,
      0.005428011529147625,
      0.05711299553513527,
      -0.011505642905831337,
      -0.014867594465613365,
      0.02970840223133564,
      0.02236836403608322,
      -0.032601553946733475,
      -0.03436959162354469,
      -0.0009953586850315332,
      0.0544341504573822,
      0.02017170935869217,
      -0.043852709233760834,
      9.43351406021975e-05,
      0.02893153578042984,
      0.028717229142785072,
      0.013508080504834652,
      -0.0036499276757240295,
      0.014773835428059101,
      -0.01680975779891014,
      0.02400245890021324,
      -0.017064249143004417,
      0.04012911021709442,
      -0.011150696314871311,
      0.020479777827858925,
      0.05716657266020775,
      0.018256334587931633,
      -0.021109305322170258,
      -0.019488604739308357,
      -0.05529138073325157,
      -0.022274604067206383,
      -0.03289622813463211,
      -0.0047415574081242085,
      -0.0013352372916415334,
      0.003994829021394253,
      0.010280070826411247,
      -0.005809747148305178,
      -0.0027508398052304983,
      0.012737912125885487,
      -0.02260945923626423,
      0.012637455947697163,
      0.002739120041951537,
      -0.01326028723269701,
      0.010065763257443905,
      -0.02500702627003193,
      0.010280070826411247,
      0.0145193450152874,
      0.020640507340431213,
      -0.010226494632661343,
      0.0193814504891634,
      0.003820704063400626,
      -0.031021036207675934,
      0.00845845602452755,
      0.01480062399059534,
      0.031663957983255386,
      0.03238724544644356,
      0.03222651779651642,
      -0.01697048917412758,
      0.0010020558256655931,
      -0.004865454044193029,
      0.011907470412552357,
      0.025529401376843452,
      -0.004098634235560894,
      0.002650383161380887,
      -0.029092267155647278,
      0.02745817042887211,
      0.030190594494342804,
      0.02180580608546734,
      0.006087677553296089,
      -0.011599402874708176,
      0.005896809510886669,
      0.011867287568747997,
      -0.0058666723780334,
      -0.008873676881194115,
      -0.03672697767615318,
      -0.012322691269218922,
      -0.0054581486620008945,
      0.0029115707147866488,
      0.0011234410339966416,
      0.013983575627207756,
      0.013755873776972294,
      -0.020345835015177727,
      0.027645690366625786,
      -0.02558297850191593,
      0.006579915527254343,
      -0.026212507858872414,
      0.00999209564179182,
      0.011123907752335072,
      -0.018229546025395393,
      -0.033458784222602844,
      0.0014022084651514888,
      -0.011003359220921993,
      0.02343990094959736,
      0.012228931300342083,
      -0.026842035353183746,
      -0.012972311116755009,
      -0.02573031559586525,
      -0.005016138777136803,
      0.01600610464811325,
      0.017305344343185425,
      0.054193053394556046,
      -0.026935795322060585,
      0.0048587569035589695,
      -0.011431975290179253,
      0.012670941650867462,
      0.0018567751394584775,
      -0.017238372936844826,
      0.013903209939599037,
      0.005317509174346924,
      0.04090597853064537,
      0.02745817042887211,
      -0.000847185030579567,
      -0.00880000926554203,
      0.02804751694202423,
      0.0005357692134566605,
      -0.012871854938566685,
      -0.07136445492506027,
      0.025154363363981247,
      -0.003341860370710492,
      -0.04832638055086136,
      0.02167186327278614,
      0.027029555290937424,
      0.01935466192662716,
      -0.006857845466583967,
      0.014238066039979458,
      -0.00021033125813119113,
      -0.016689209267497063,
      -0.02641342021524906,
      -0.007641408126801252,
      0.030110228806734085,
      0.034074921160936356,
      0.00090159906540066,
      0.0012506862403824925,
      -0.017144614830613136,
      -0.0043095932342112064,
      -0.011746739037334919,
      -0.005588742438703775,
      -0.011318123899400234,
      -0.01360184047371149,
      0.020024374127388,
      -0.0052204011008143425,
      -0.021470950916409492,
      0.019126959145069122,
      -0.014586316421627998,
      0.007038667798042297,
      -0.011271243914961815,
      0.0004717280389741063,
      -0.006898028310388327,
      -0.006224968004971743,
      0.00033862286363728344,
      -0.003140946850180626,
      -0.005846581421792507,
      0.003017050214111805,
      0.014720258302986622,
      -0.003961343318223953,
      0.0012573832646012306,
      -0.008994225412607193,
      0.03284265100955963,
      0.0005244677886366844,
      0.07002503424882889,
      0.005498331040143967,
      -0.023064862936735153,
      0.01835009455680847,
      -0.02331935428082943,
      0.02810109406709671,
      -0.021953143179416656,
      0.0036666702944785357,
      -0.01997079700231552,
      -0.004353124648332596,
      -0.010688595473766327,
      0.01689012348651886,
      0.0012063178000971675,
      -4.685364183387719e-05,
      0.0483531691133976,
      -0.00844506174325943,
      -0.023801546543836594,
      -0.0035595165099948645,
      -0.009235321544110775,
      0.010306859388947487,
      0.017358921468257904,
      0.013072768226265907,
      0.008994225412607193,
      -0.030726362019777298,
      0.010112643241882324,
      0.0035494707990437746,
      -0.0022334877867251635,
      0.019662728533148766,
      0.02263624779880047,
      0.018926046788692474,
      0.007494071498513222,
      -0.016448114067316055,
      -0.013742479495704174,
      0.03554828464984894,
      -0.014264854602515697,
      0.0031509925611317158,
      0.00045079953270033,
      -0.012590575963258743,
      -0.015684643760323524,
      -0.014010364189743996,
      0.008679460734128952,
      -0.004413398448377848,
      -0.018323305994272232,
      -0.0016156790079548955,
      0.022649642080068588,
      0.01123106200248003,
      0.029386939480900764,
      0.015831978991627693,
      -0.003221312304958701,
      0.02739119902253151,
      0.006777480244636536,
      -0.023828335106372833,
      0.019435027614235878,
      -0.022167449817061424,
      0.00878661498427391,
      0.011385095305740833,
      -0.0023121789563447237,
      0.03597690165042877,
      0.0072596725076437,
      0.013501383364200592,
      -0.03048526681959629,
      0.0026286174543201923,
      -0.007159215863794088,
      0.025221334770321846,
      -0.020734267309308052,
      -0.038896843791007996,
      -0.03040490113198757,
      -0.00013886048691347241,
      -0.02093518152832985,
      0.016608845442533493,
      -0.030940670520067215,
      0.017010672017931938,
      0.005923598073422909,
      0.010105946101248264,
      0.009282201528549194,
      0.033699881285429,
      0.026761671528220177,
      0.004235925152897835,
      0.03678055480122566,
      -0.04575468972325325,
      -0.008826796896755695,
      0.003981434740126133,
      -0.015068508684635162,
      -0.025529401376843452,
      -0.011418581008911133,
      -0.0027508398052304983,
      0.03219972923398018,
      -0.015202450565993786,
      -0.037369899451732635,
      0.006770783104002476,
      -0.0035126367583870888,
      0.01923411339521408,
      0.03474463149905205,
      0.05148741975426674,
      -0.009965307079255581,
      0.001367048593237996,
      -0.017894690856337547,
      0.006278545130044222,
      0.011010056361556053,
      -0.004637752193957567,
      -0.00024863038561306894,
      -0.005421314388513565,
      -0.028556497767567635,
      0.0043028960935771465,
      -0.016689209267497063,
      -0.0009727559518069029,
      -0.03536076471209526,
      -0.013822845183312893,
      0.027913574129343033,
      -0.006857845466583967,
      -0.016702603548765182,
      0.021310219541192055,
      0.035682227462530136,
      0.018845681101083755,
      0.01599271036684513,
      0.03686092048883438,
      0.0312353428453207,
      0.04899609088897705,
      -0.024698959663510323,
      -0.04128101468086243,
      0.007708379067480564,
      0.0021296825725585222,
      0.01080914307385683,
      -0.011405186727643013,
      -0.004276107996702194,
      -0.009255412966012955,
      -0.005900158081203699,
      -0.021296825259923935,
      0.005263932049274445,
      -0.016273988410830498,
      -0.003914463799446821,
      0.024364104494452477,
      0.015068508684635162,
      -0.022703219205141068,
      0.020493172109127045,
      0.022127266973257065,
      0.0033184203784912825,
      0.0026721488684415817,
      -0.027136709541082382,
      -0.00011573451774893329,
      0.017251767218112946,
      0.026922401040792465,
      0.018966229632496834,
      -0.004527249839156866,
      -0.011311426758766174,
      -0.037423476576805115,
      0.007567739579826593,
      -0.011545825749635696,
      0.008826796896755695,
      0.031128190457820892,
      -0.030137017369270325,
      0.03276228532195091,
      0.023841729387640953,
      0.04655834287405014,
      0.00010736312106018886,
      0.005521771032363176,
      0.012918734923005104,
      -0.009108075872063637,
      -0.012443239800632,
      0.0199574027210474,
      0.014947960153222084,
      -0.0266411229968071,
      0.005779610015451908,
      -0.017881296575069427,
      0.02496684342622757,
      -0.006737297400832176,
      -0.004567432217299938,
      0.0191805362701416,
      0.0240426417440176,
      -0.005136686842888594,
      0.004168953746557236,
      -0.007795441430062056,
      0.0014716909499838948,
      0.0021079168654978275,
      -0.017466075718402863,
      -0.021430768072605133,
      -0.01607307605445385,
      0.012711123563349247,
      0.0030773242469877005,
      0.01122436486184597,
      -0.017894690856337547,
      0.005156778264790773,
      0.0065933093428611755,
      0.005441405810415745,
      -0.009355869144201279,
      -0.012999099679291248,
      -0.005022835917770863,
      0.004343078937381506,
      0.0169169120490551,
      0.009489811956882477,
      0.0289047472178936,
      -0.024551622569561005,
      0.03236046060919762,
      0.02973518893122673,
      0.02653396874666214,
      0.03209257498383522,
      -0.0039044178556650877,
      0.0043732160702347755,
      0.003219638019800186,
      0.042245399206876755,
      0.00310746137984097,
      0.0005751147400587797,
      0.018859075382351875,
      0.0032983289565891027,
      -0.027056343853473663,
      -0.017010672017931938,
      0.010407316498458385,
      -0.0008153737289831042,
      0.012255719862878323,
      0.014157700352370739,
      0.001441553933545947,
      -0.006573218386620283,
      -0.006630143616348505,
      0.0027190286200493574,
      0.0019086777465417981,
      0.01362862903624773,
      -0.026708094403147697,
      0.038093190640211105,
      -0.020412806421518326,
      -0.020319046452641487,
      -0.01851082593202591,
      -0.022194238379597664,
      0.0013025888474658132,
      0.008933951146900654,
      0.0014273226261138916,
      0.02887795865535736,
      -0.0010531212901696563,
      0.008003052324056625,
      0.019073383882641792,
      0.03056563250720501,
      -0.006291939411312342,
      0.011780224740505219,
      -0.0018902606097981334,
      -0.014747046865522861,
      -0.0011326494859531522,
      -0.012235628440976143,
      -0.0223549697548151,
      -0.008954042568802834,
      0.018671555444598198,
      -0.019006412476301193,
      -0.012570484541356564,
      0.0418703630566597,
      0.014211277477443218,
      0.020305652171373367,
      -0.025395458564162254,
      0.009228624403476715,
      0.025475824251770973,
      -0.012744609266519547,
      -0.04082561284303665,
      0.023814940825104713,
      -0.006985090672969818,
      0.011485551483929157,
      -0.010922994464635849,
      -0.04251328483223915,
      0.024565016850829124,
      -0.03423565253615379,
      -0.026306267827749252,
      -0.023868517950177193,
      0.02828861214220524,
      -0.001105023897252977,
      -0.017841113731265068,
      -0.003264843486249447,
      -0.012470027431845665,
      -0.006104419939219952,
      -0.02804751694202423,
      0.03367309272289276,
      -0.01997079700231552,
      0.035869747400283813,
      -0.003139272565022111,
      -0.03546791896224022,
      -0.01238296553492546,
      0.03284265100955963,
      -0.0063019851222634315,
      -0.004035011865198612,
      0.0005156778497621417,
      0.035012517124414444,
      0.0041890451684594154,
      -0.00181324384175241,
      -0.01761341281235218,
      -0.0258642565459013,
      -0.004356473218649626,
      0.011465460993349552,
      -0.027069738134741783,
      -0.021524526178836823,
      0.015269421972334385,
      -0.0047415574081242085,
      -0.008311119861900806,
      0.0040684971027076244,
      0.00309741566888988,
      0.033244479447603226,
      0.004892242606729269,
      0.04074524715542793,
      0.015523912385106087,
      -0.013782662339508533,
      -0.002775954082608223,
      -0.010360436514019966,
      -0.00844506174325943,
      -0.022020112723112106,
      -0.005913552362471819,
      0.0144523736089468,
      -0.007621316704899073,
      -0.022917527705430984,
      0.03372666984796524,
      0.009342474862933159,
      0.04318299517035484,
      0.028744017705321312,
      -0.005906855221837759,
      -0.0017161356518045068,
      0.015698038041591644,
      0.005287372041493654,
      0.0012749632587656379,
      0.019421633332967758,
      1.4649938748334534e-05,
      0.002931661903858185,
      0.030217381194233894,
      0.007159215863794088,
      0.0035996991209685802,
      0.0025147665292024612,
      -0.015001537278294563,
      0.023747969418764114,
      0.013889816589653492,
      -0.004945819266140461,
      0.001370397163555026,
      0.01481401827186346,
      0.09933160990476608,
      0.011539128609001637,
      -0.015028325840830803,
      -0.002409287029877305,
      0.03629836067557335,
      -0.023091651499271393,
      0.036593034863471985,
      0.0032380549237132072,
      0.030163805931806564,
      -0.02423016168177128,
      0.005263932049274445,
      0.043075840920209885,
      0.02978876605629921,
      0.012188749387860298,
      0.022221026942133904,
      0.007688287645578384,
      -0.04023626446723938,
      0.004567432217299938,
      0.003710201708599925,
      -0.007534254342317581,
      -0.02815467119216919,
      0.027592113241553307,
      -0.026279479265213013,
      0.009884941391646862,
      0.0035327281802892685,
      0.029494093731045723,
      0.032574765384197235,
      0.026761671528220177,
      0.03139607608318329,
      -0.0014641567831858993,
      0.006733948830515146,
      0.005555256735533476,
      -0.0019773230887949467,
      -0.015282816253602505,
      -0.011679768562316895,
      -0.003370322985574603,
      -0.002779302652925253,
      0.002143076853826642,
      0.009737605229020119,
      0.010594835504889488,
      0.006070934701710939,
      0.0030371416360139847,
      0.0007664010627195239,
      -0.013045979663729668,
      -0.011880681850016117,
      -0.03153001517057419,
      0.006395744625478983,
      0.004008223302662373,
      0.026748277246952057,
      0.008090115152299404,
      -0.017077643424272537,
      -0.020680690184235573,
      -0.02014492079615593,
      0.004902287852019072,
      0.0022251163609325886,
      -0.040530938655138016,
      0.022703219205141068,
      -0.026962583884596825,
      0.03313732519745827,
      -0.012182052247226238,
      -0.017908085137605667,
      0.009422840550541878,
      0.015189056284725666,
      0.01611325889825821,
      -0.0006002289010211825
    ],
    "updated_at": "2025-11-07T15:08:16.383822",
    "job_id": "ml_engineer_v1",
    "position": "算法工程师",
    "target_profile": "● “论文复现者+优化者”： 他们不仅读论文，还会动手去实现它，并思考“如果我把A论文的方法和B论文的方法结合会怎样？”\n● “实验科学家”： 对“为什么这个方法有效”充满好奇，会通过严谨的消融实验来验证自己的假设，而不是“炼丹”。\n● “结构化思考者”： 简历中的项目经验描述清晰、有逻辑。明确描述了“问题背景 -> 尝试的方案 -> 为什么选择该方案 -> 实验结果 -> 迭代分析”。\n● “业务翻译官”： 能够用清晰的语言解释复杂的技术权衡（例如，“用QLoRA可以省显存，但代价可能是……”），具备引导客户需求的潜力。",
    "keywords": {
      "positive": [
        "Data Attribution",
        "Influence Functions",
        "Fact Tracing",
        "Source Tracing",
        "Approximate Unrolling",
        "DataInf",
        "Hessian-vector product",
        "Optimizer State Correction",
        "QLoRA",
        "HydraLoRA",
        "Subspace Regularization",
        "Trans-LoRA",
        "LoRA-GA",
        "Controlled LoRA",
        "CLoRA",
        "Asymmetric LoRA",
        "Hallucination Detection",
        "Uncertainty-Based Detection",
        "RAG-HAT",
        "HaloScope",
        "Hallucination-Aware Tuning",
        "Factual Inconsistency",
        "Self-Correction",
        "Verification-based Refinement",
        "Verbal Reinforcement Learning",
        "Reflexion",
        "Reverse Curriculum RL",
        "Asynchronous RLHF",
        "Multi-turn RLHF",
        "Semi-Parametric RL Agents",
        "Outcome Supervision",
        "Process Supervision",
        "GraphRAG",
        "GRAG",
        "Knowledge Graph-Guided RAG",
        "Graph Retrieval",
        "Community Detection",
        "Deep-path Traversals",
        "Semantic Relationships",
        "Chain of Preference Optimization",
        "Visual CoT",
        "Boosting of Thoughts",
        "Noisy Rationales",
        "Tree-of-Thought Search",
        "Multimodal Reasoning",
        "Ablation Study",
        "消融实验"
      ],
      "negative": [
        "machine learning",
        "deep learning",
        "AI",
        "CNN",
        "RNN",
        "LSTM",
        "SFT",
        "RAG",
        "Pandas",
        "Matplotlib",
        "Seaborn",
        "MNIST",
        "CIFAR-10",
        "SVM",
        "XGBoost",
        "API调用",
        "爬虫",
        "数据分析",
        "可视化",
        "调参"
      ]
    },
    "candidate_filters": {
      "活跃度": "本周活跃",
      "院校": [
        "985",
        "留学",
        "国内外名校"
      ],
      "只看第一学历": true,
      "近期没有看过": "近14天没有",
      "跳槽频率": "5年少于3份",
      "是否与同事交换简历": "近一个月没有",
      "牛人关键词": [
        "语言模型"
      ],
      "性别": "男",
      "学历": "硕士",
      "经验": "3-5年",
      "经验要求": "应届",
      "求职意向": [
        "离职-随时到岗",
        "在职-考虑机会"
      ],
      "薪资待遇": "15-25K"
    },
    "description": "HR在筛选简历时必须严格执行以下标准：\n1. 学历背景（第一学历）：\n    ○ 应届生/1年内经验： 必须为 985 或 国际顶尖名校（如QS全球排名前50）的计算机科学、数学、统计学、人工智能或相关理工科专业本科及以上。\n    ○ 1-3年经验： 本科第一学历必须为 211 及以上，专业为计算机科学、数学等相关理工科背景。\n2. 工作年限与薪资：\n    ○ 总工作经验不超过3年。\n    ○ 或，明确的月薪资期望低于25k。\n    ○ （备注：此标准用于定位高潜力初级人才，超出此范围的候选人应转至资深或专家通道）。\n3. 核心技能（必须具备）：\n    ○ 扎实的编程基础，精通 Python 及主流深度学习框架（PyTorch优先）。\n    ○ 深刻理解Transformer架构和核心ML概念（对数学有良好直觉）。\n    ○ 最关键标准： 简历中必须有明确证据，表明候选人有**“阅读并复现/改进学术论文算法”**的经历（课程项目、毕设、实习、工作均可）。\n    ○ 有设计和执行消融实验 (Ablation Study) 的经验。\n    ○ 展现出结构化的思考能力和复杂问题拆解能力。\n\n* 如果简历中有链接，请打开链接并阅读内容，用于判断候选人能力",
    "drill_down_questions": "通用破冰问题（必问）：\n1. “我看到你简历里提到了xxx，你能用通俗的语言给我解释一下，它解决了标准LoRA的什么问题吗？”\n    ○ 考察点： 是否真的理解了，还是只是堆砌词汇。好的回答会提到“多任务冲突”、“参数共享”等。\n2. “在你做的 [某个项目] 中，你提到了 [论文X] 和。你是怎么把它们结合起来的？为什么要这么做？”\n    ○ 考察点： 考察“融会贯通”的能力，这是岗位的核心要求。\n3. “你在 [某个训练/实验] 中，提到你做了‘消融实验’。能具体说一下你‘消融’了哪个部分吗？这个实验是为了验证什么？”\n    ○ 考察点： 确认其具备“实验科学家”的结构化思维。如果答不上来，说明项目经验水分大。\n4. “如果现在有个业务需求是 [一个相关的简化版业务问题，如：我们希望模型在持续学习新知识时，不要忘记旧知识]，根据你的经验，你会从哪几个算法方向去尝试解决？”\n    ○ 考察点： 考察其连接“业务需求”和“算法库”的能力。好的回答可能会提到 CLoRA, Subspace Regularization 等。\n5.  “您好，我看到您的简历非常优秀。能否请您选择一个您最有深度的算法项目，简单说一下：1）当时要解决的核心问题是什么？2）您为什么选择了A方案而不是B方案？3）您通过什么实验（比如消融实验）来证明您的方案是有效的？”\n\n针对特定方向的追问（根据简历关键词选用）：\n1. 如果简历提及 LoRA 优化 (QLoRA, HydraLoRA, CLoRA):\n“我看到您在项目中使用了 LoRA / QLoRA。在实际训练中，您是如何设定它的秩（Rank）的？设定高或低，分别会对您的显存占用和模型效果产生什么具体影响？”\n（追问 HydraLoRA / CLoRA）：“您为什么选择 HydraLoRA / Controlled LoRA 而不是标准的LoRA？它解决了标准LoRA的什么特定问题（比如多任务干扰或灾难性遗忘）？”\n\n2. 如果简历提及 RL 对齐 (RLHF, DPO, Reflexion):\n“在您提到的 DPO / RLHF 项目中，偏好数据集（Preference Dataset）是如何构建的？这个过程中最大的难点是什么？”\n（追问 Reflexion）：“您提到了 Reflexion（反思）框架，它和传统RLHF最大的不同是什么？您在项目中是如何实现模型的‘自我反思’这一步的？”\n\n3. 如果简历提及模型幻觉 (Hallucination Detection, RAG-HAT):\n“针对‘幻觉’问题，我注意到您使用了 RAG-HAT / HaloScope 这样的技术。您能具体说明一下，您是如何定义和标注‘幻觉’数据来训练您的检测或优化模型的吗？”\n“在您的项目中，您是如何量化评估幻觉问题得到了多大程度的缓解的？”\n\n4. 如果简历提及 GraphRAG (知识图谱):\n“我看到您有 GraphRAG 的经验。和传统的向量RAG相比，您在项目中构建的‘知识图谱’提供了哪些额外的信息？（比如，是实体关系，还是多跳推理路径？）”\n“在您做图检索时（比如使用 Community Detection），您是如何平衡检索的**深度（召回率）和相关性（精确率）**的？”\n\n5. 如果简历提及数据归因 (Data Attribution, Influence Functions):\n“您在项目中提到使用了‘数据归因’ 技术。您当时是为了解决什么具体问题（比如，是追踪一个错误答案的来源，还是提升数据质量）？”\n（追问 DataInf）：“在 LoRA 调优的背景下，计算数据影响（Influence）的主要难点是什么？DataInf 是如何解决的？”\n\n6. 如果简历提及 CoT / 高级推理 (Tree-of-Thought, Noisy Rationales):\n“在您关于 CoT（思维链） 的项目中，您是如何评估模型生成的那些‘中间推理步骤’的质量的？”\n“您是否尝试过优化CoT的推理过程，比如当推理步骤包含错误信息（Noisy Rationales）时，模型该如何处理？”",
    "version": 1,
    "current": false,
    "created_at": "2025-11-06T18:10:15.918010",
    "background": "我们正在寻找一位高潜力的算法工程师，加入我们的核心AI研发团队。该职位的核心任务是将前沿的算法理论（特别是我们关注的6大研究方向）转化为实际、高效的模型和系统。\n你将负责深入阅读、拆解和融会贯通多篇最新的学术论文，结合业务需求，自主设计和执行严谨的算法实验（包括消融实验和对比实验）。你不仅要实现算法，更要负责训练优化、迭代结果、分析瓶颈，并最终交付可用的高性能模型。此外，你还需要与业务和产品团队紧密协作，深刻理解客户需求，并从技术层面引导和定义解决方案。"
  },
  {
    "responsibilities": "● 前沿算法转化： 跟踪、阅读和深入理解我们核心研究方向（数据归因、LoRA优化、幻觉、RL、GraphRAG、CoT）的最新论文，并将其核心思想工程化实现。\n● 实验设计与执行： 针对具体业务问题，设计严谨的算法实验方案、消融实验和A/B测试，评估不同方法的优劣。\n● 模型训练与优化： 负责模型的全流程训练、微调、调试和迭代；分析实验结果，定位性能瓶颈（无论是算法层面还是工程层面），并提出改进方案。\n● 算法融合创新： 结合业务需求，将来自不同论文的算法模块（例如，将一种新的LoRA变体应用到RLHF流程中）进行创新性组合与优化。\n● 业务需求对接： 与产品和业务方沟通，将模糊的客户需求转化为清晰、可执行的算法技术指标和研发路径。",
    "requirements": "- 高效微调工程化： 负责参数高效微调（PEFT）算法的工程实现、性能基准测试和迭代优化。重点是将QLoRA、HydraLoRA等前沿LoRA变体 应用于业务场景，管理和部署大规模的LoRA适配器。\n- RLHF/DPO流水线搭建： 搭建和维护可扩展、高通量的强化学习对齐（RLHF/DPO）训练流水线。重点攻坚Asynchronous RLHF（异步RLHF） 等技术，通过解耦数据生成和模型训练，最大化GPU集群效率，提升模型迭代速度。\n- 高级RAG系统实现： 主导GraphRAG（图谱RAG）系统的工程落地。负责自动化构建知识图谱、实现图检索算法（如社区发现、多跳查询），并将其与LLM推理流程高效集成。\n- 幻觉闭环优化： 开发和部署模型幻觉检测系统。构建基于RAG-HAT（幻觉感知微调）的闭环优化流程，即自动捕获RAG系统中的幻觉案例，生成偏好数据，并反馈给DPO流程以持续优化模型。\n- 数据归因与溯源系统： 实现可扩展的数据归因（Data Attribution）工具。应用DataInf、Influence Functions或Fact Tracing 等技术，构建数据清洗和优化流水线，量化分析训练数据对模型（尤其是LoRA微调后）具体行为的影响。\n- 推理与智能体工程化： 将高级推理算法（如CoT、Visual CoT）和智能体框架（如Reflexion）工程化，交付解决复杂多步骤产品任务的可靠API。\n- 负面关键词清单（出现则大概率淘汰）\n● 基础概念罗列（红灯）： 技能列表里只有 Machine Learning, Deep Learning, AI, CNN, RNN, LSTM。这表明知识体系停留在5年前。\n● 项目经验过浅（红灯）：\n    ○ 项目描述为“调用XX（如OpenAI）的API实现了XX功能”。（我们找的是造模型的人，不是调API的人）。\n    ○ 项目是简单的数据分析或可视化（如 Pandas, Matplotlib, Seaborn）。\n    ○ 唯一的“深度学习”项目是在 MNIST, CIFAR-10 或 IMDB 情感分类上跑通一个标准模型。\n● 与我们方向不符（黄灯）： 经验仅集中在 CV (如目标检测, 分割), Speech (ASR/TTS) 或传统机器学习（SVM, XGBoost）。如果他们没有展现出向LLM迁移的强烈意愿和自学能力，则不匹配。\n● 泛泛的流行词（黄灯）： 只提 SFT 或 RAG，但没有提供任何关于“如何优化”、“踩了什么坑”、“做了哪些改进”的细节。",
    "job_embedding": [
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0
    ],
    "updated_at": "2025-11-14T17:09:05.119969",
    "job_id": "ml_engineer_v2",
    "position": "算法工程师",
    "target_profile": "● “论文复现者+优化者”： 他们不仅读论文，还会动手去实现它，并思考“如果我把A论文的方法和B论文的方法结合会怎样？”\n● “实验科学家”： 对“为什么这个方法有效”充满好奇，会通过严谨的消融实验来验证自己的假设，而不是“炼丹”。\n● “结构化思考者”： 简历中的项目经验描述清晰、有逻辑。明确描述了“问题背景 -> 尝试的方案 -> 为什么选择该方案 -> 实验结果 -> 迭代分析”。\n● “业务翻译官”： 能够用清晰的语言解释复杂的技术权衡（例如，“用QLoRA可以省显存，但代价可能是……”），具备引导客户需求的潜力。",
    "keywords": {
      "positive": [
        "Data Attribution",
        "Influence Functions",
        "Fact Tracing",
        "Source Tracing",
        "Approximate Unrolling",
        "DataInf",
        "Hessian-vector product",
        "Optimizer State Correction",
        "QLoRA",
        "HydraLoRA",
        "Subspace Regularization",
        "Trans-LoRA",
        "LoRA-GA",
        "Controlled LoRA",
        "CLoRA",
        "Asymmetric LoRA",
        "Hallucination Detection",
        "Uncertainty-Based Detection",
        "RAG-HAT",
        "HaloScope",
        "Hallucination-Aware Tuning",
        "Factual Inconsistency",
        "Self-Correction",
        "Verification-based Refinement",
        "Verbal Reinforcement Learning",
        "Reflexion",
        "Reverse Curriculum RL",
        "Asynchronous RLHF",
        "Multi-turn RLHF",
        "Semi-Parametric RL Agents",
        "Outcome Supervision",
        "Process Supervision",
        "GraphRAG",
        "GRAG",
        "Knowledge Graph-Guided RAG",
        "Graph Retrieval",
        "Community Detection",
        "Deep-path Traversals",
        "Semantic Relationships",
        "Chain of Preference Optimization",
        "Visual CoT",
        "Boosting of Thoughts",
        "Noisy Rationales",
        "Tree-of-Thought Search",
        "Multimodal Reasoning",
        "Ablation Study",
        "消融实验"
      ],
      "negative": [
        "machine learning",
        "deep learning",
        "AI",
        "CNN",
        "RNN",
        "LSTM",
        "SFT",
        "RAG",
        "Pandas",
        "Matplotlib",
        "Seaborn",
        "MNIST",
        "CIFAR-10",
        "SVM",
        "XGBoost",
        "API调用",
        "爬虫",
        "数据分析",
        "可视化",
        "调参"
      ]
    },
    "candidate_filters": {
      "活跃度": "本周活跃",
      "院校": [
        "985",
        "留学",
        "国内外名校"
      ],
      "只看第一学历": true,
      "近期没有看过": "近14天没有",
      "跳槽频率": "5年少于3份",
      "是否与同事交换简历": "近一个月没有",
      "牛人关键词": [
        "语言模型"
      ],
      "性别": "男",
      "学历": "硕士",
      "经验": "3-5年",
      "经验要求": "应届",
      "求职意向": [
        "离职-随时到岗",
        "在职-考虑机会"
      ],
      "薪资待遇": "15-25K"
    },
    "description": "HR在筛选简历时必须严格执行以下标准：\n1. 学历背景（第一学历）：\n    ○ 应届生/1年内经验： 必须为 985 或 国际顶尖名校（如QS全球排名前200）的计算机科学、数学、统计学、人工智能或相关理工科专业本科及以上。\n    ○ 1-3年经验： 本科第一学历必须为 211 及以上，专业为计算机科学、数学等相关理工科背景。\n2. 工作年限与薪资：\n    ○ 总工作经验不超过3年。\n    ○ 或，明确的月薪资期望低于25k。\n    ○ （备注：此标准用于定位高潜力初级人才，超出此范围的候选人应转至资深或专家通道）。\n3. 核心技能（必须具备）：\n    ○ 简历中必须有明确证据，表明候选人有**“阅读并复现/改进学术论文算法”**的经历（课程项目、毕设、实习、工作均可）。\n    ○ 有设计和执行消融实验 (Ablation Study) 的经验。\n    ○ 展现出结构化的思考能力和复杂问题拆解能力。\n    ○ 有独立研究，能基于最新算法，探索前沿领域，做出独特的结果，并带来业务价值\n\n* 如果简历中有链接，请打开链接并阅读内容，用于判断候选人能力",
    "drill_down_questions": "通用破冰问题（必问）：\n1. “我看到你简历里提到了xxx，你能用通俗的语言给我解释一下，它解决了标准LoRA的什么问题吗？”\n    ○ 考察点： 是否真的理解了，还是只是堆砌词汇。好的回答会提到“多任务冲突”、“参数共享”等。\n2. “在你做的 [某个项目] 中，你提到了 [论文X] 和。你是怎么把它们结合起来的？为什么要这么做？”\n    ○ 考察点： 考察“融会贯通”的能力，这是岗位的核心要求。\n3. “你在 [某个训练/实验] 中，提到你做了‘消融实验’。能具体说一下你‘消融’了哪个部分吗？这个实验是为了验证什么？”\n    ○ 考察点： 确认其具备“实验科学家”的结构化思维。如果答不上来，说明项目经验水分大。\n4. “如果现在有个业务需求是 [一个相关的简化版业务问题，如：我们希望模型在持续学习新知识时，不要忘记旧知识]，根据你的经验，你会从哪几个算法方向去尝试解决？”\n    ○ 考察点： 考察其连接“业务需求”和“算法库”的能力。好的回答可能会提到 CLoRA, Subspace Regularization 等。\n5.  “您好，我看到您的简历非常优秀。能否请您选择一个您最有深度的算法项目，简单说一下：1）当时要解决的核心问题是什么？2）您为什么选择了A方案而不是B方案？3）您通过什么实验（比如消融实验）来证明您的方案是有效的？”\n\n针对特定方向的追问（根据简历关键词选用）：\n1. 如果简历提及 LoRA 优化 (QLoRA, HydraLoRA, CLoRA):\n“我看到您在项目中使用了 LoRA / QLoRA。在实际训练中，您是如何设定它的秩（Rank）的？设定高或低，分别会对您的显存占用和模型效果产生什么具体影响？”\n（追问 HydraLoRA / CLoRA）：“您为什么选择 HydraLoRA / Controlled LoRA 而不是标准的LoRA？它解决了标准LoRA的什么特定问题（比如多任务干扰或灾难性遗忘）？”\n\n2. 如果简历提及 RL 对齐 (RLHF, DPO, Reflexion):\n“在您提到的 DPO / RLHF 项目中，偏好数据集（Preference Dataset）是如何构建的？这个过程中最大的难点是什么？”\n（追问 Reflexion）：“您提到了 Reflexion（反思）框架，它和传统RLHF最大的不同是什么？您在项目中是如何实现模型的‘自我反思’这一步的？”\n\n3. 如果简历提及模型幻觉 (Hallucination Detection, RAG-HAT):\n“针对‘幻觉’问题，我注意到您使用了 RAG-HAT / HaloScope 这样的技术。您能具体说明一下，您是如何定义和标注‘幻觉’数据来训练您的检测或优化模型的吗？”\n“在您的项目中，您是如何量化评估幻觉问题得到了多大程度的缓解的？”\n\n4. 如果简历提及 GraphRAG (知识图谱):\n“我看到您有 GraphRAG 的经验。和传统的向量RAG相比，您在项目中构建的‘知识图谱’提供了哪些额外的信息？（比如，是实体关系，还是多跳推理路径？）”\n“在您做图检索时（比如使用 Community Detection），您是如何平衡检索的**深度（召回率）和相关性（精确率）**的？”\n\n5. 如果简历提及数据归因 (Data Attribution, Influence Functions):\n“您在项目中提到使用了‘数据归因’ 技术。您当时是为了解决什么具体问题（比如，是追踪一个错误答案的来源，还是提升数据质量）？”\n（追问 DataInf）：“在 LoRA 调优的背景下，计算数据影响（Influence）的主要难点是什么？DataInf 是如何解决的？”\n\n6. 如果简历提及 CoT / 高级推理 (Tree-of-Thought, Noisy Rationales):\n“在您关于 CoT（思维链） 的项目中，您是如何评估模型生成的那些‘中间推理步骤’的质量的？”\n“您是否尝试过优化CoT的推理过程，比如当推理步骤包含错误信息（Noisy Rationales）时，模型该如何处理？”",
    "version": 2,
    "current": false,
    "created_at": "2025-11-06T18:10:15.918010",
    "background": "我们正在寻找一位高潜力的算法工程师，加入我们的核心AI研发团队。该职位的核心任务是将前沿的算法理论（特别是我们关注的6大研究方向）转化为实际、高效的模型和系统。\n你将负责深入阅读、拆解和融会贯通多篇最新的学术论文，结合业务需求，自主设计和执行严谨的算法实验（包括消融实验和对比实验）。你不仅要实现算法，更要负责训练优化、迭代结果、分析瓶颈，并最终交付可用的高性能模型。此外，你还需要与业务和产品团队紧密协作，深刻理解客户需求，并从技术层面引导和定义解决方案。"
  },
  {
    "responsibilities": "- 高效微调工程化： 负责参数高效微调（PEFT）算法的工程实现、性能基准测试和迭代优化。重点是将QLoRA、HydraLoRA等前沿LoRA变体 应用于业务场景，管理和部署大规模的LoRA适配器。\n- RLHF/DPO流水线搭建： 搭建和维护可扩展、高通量的强化学习对齐（RLHF/DPO）训练流水线。重点攻坚Asynchronous RLHF（异步RLHF） 等技术，通过解耦数据生成和模型训练，最大化GPU集群效率，提升模型迭代速度。\n- 高级RAG系统实现： 主导GraphRAG（图谱RAG）系统的工程落地。负责自动化构建知识图谱、实现图检索算法（如社区发现、多跳查询），并将其与LLM推理流程高效集成。\n- 幻觉闭环优化： 开发和部署模型幻觉检测系统。构建基于RAG-HAT（幻觉感知微调）的闭环优化流程，即自动捕获RAG系统中的幻觉案例，生成偏好数据，并反馈给DPO流程以持续优化模型。\n- 数据归因与溯源系统： 实现可扩展的数据归因（Data Attribution）工具。应用DataInf、Influence Functions或Fact Tracing 等技术，构建数据清洗和优化流水线，量化分析训练数据对模型（尤其是LoRA微调后）具体行为的影响。\n- 推理与智能体工程化： 将高级推理算法（如CoT、Visual CoT）和智能体框架（如Reflexion）工程化，交付解决复杂多步骤产品任务的可靠API。",
    "requirements": "HR在筛选简历时必须严格执行以下标准：\n1. 学历背景（第一学历）：\n    ○ 应届生/1年内经验： 必须为 985 或 国际顶尖名校（如QS全球排名前200）的计算机科学、数学、统计学、人工智能或相关理工科专业本科及以上。\n    ○ 1-3年经验： 本科第一学历必须为 211 及以上，专业为计算机科学、数学等相关理工科背景。\n2. 工作年限与薪资：\n    ○ 总工作经验不超过3年。\n    ○ 或，明确的月薪资期望低于25k。\n    ○ （备注：此标准用于定位高潜力初级人才，超出此范围的候选人应转至资深或专家通道）。\n3. 核心技能（必须具备）：\n    ○ 简历中必须有明确证据，表明候选人有**“阅读并复现/改进学术论文算法”**的经历（课程项目、毕设、实习、工作均可）。\n    ○ 有设计和执行消融实验 (Ablation Study) 的经验。\n    ○ 展现出结构化的思考能力和复杂问题拆解能力。\n    ○ 有独立研究，能基于最新算法，探索前沿领域，做出独特的结果，并带来业务价值\n\n* 如果简历中有链接，请打开链接并阅读内容，用于判断候选人能力\n\n- 负面关键词清单（出现则大概率淘汰）\n● 基础概念罗列（红灯）： 技能列表里只有 Machine Learning, Deep Learning, AI, CNN, RNN, LSTM。这表明知识体系停留在5年前。\n● 项目经验过浅（红灯）：\n    ○ 项目描述为“调用XX（如OpenAI）的API实现了XX功能”。（我们找的是造模型的人，不是调API的人）。\n    ○ 项目是简单的数据分析或可视化（如 Pandas, Matplotlib, Seaborn）。\n    ○ 唯一的“深度学习”项目是在 MNIST, CIFAR-10 或 IMDB 情感分类上跑通一个标准模型。\n● 与我们方向不符（黄灯）： 经验仅集中在 CV (如目标检测, 分割), Speech (ASR/TTS) 或传统机器学习（SVM, XGBoost）。如果他们没有展现出向LLM迁移的强烈意愿和自学能力，则不匹配。\n● 泛泛的流行词（黄灯）： 只提 SFT 或 RAG，但没有提供任何关于“如何优化”、“踩了什么坑”、“做了哪些改进”的细节。",
    "job_embedding": [
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0
    ],
    "updated_at": "2025-11-15T10:04:55.512112",
    "job_id": "ml_engineer_v3",
    "position": "算法工程师",
    "target_profile": "● “论文复现者+优化者”： 他们不仅读论文，还会动手去实现它，并思考“如果我把A论文的方法和B论文的方法结合会怎样？”\n● “实验科学家”： 对“为什么这个方法有效”充满好奇，会通过严谨的消融实验来验证自己的假设，而不是“炼丹”。\n● “结构化思考者”： 简历中的项目经验描述清晰、有逻辑。明确描述了“问题背景 -> 尝试的方案 -> 为什么选择该方案 -> 实验结果 -> 迭代分析”。\n● “业务翻译官”： 能够用清晰的语言解释复杂的技术权衡（例如，“用QLoRA可以省显存，但代价可能是……”），具备引导客户需求的潜力。",
    "keywords": {
      "positive": [
        "Data Attribution",
        "Influence Functions",
        "Fact Tracing",
        "Source Tracing",
        "Approximate Unrolling",
        "DataInf",
        "Hessian-vector product",
        "Optimizer State Correction",
        "QLoRA",
        "HydraLoRA",
        "Subspace Regularization",
        "Trans-LoRA",
        "LoRA-GA",
        "Controlled LoRA",
        "CLoRA",
        "Asymmetric LoRA",
        "Hallucination Detection",
        "Uncertainty-Based Detection",
        "RAG-HAT",
        "HaloScope",
        "Hallucination-Aware Tuning",
        "Factual Inconsistency",
        "Self-Correction",
        "Verification-based Refinement",
        "Verbal Reinforcement Learning",
        "Reflexion",
        "Reverse Curriculum RL",
        "Asynchronous RLHF",
        "Multi-turn RLHF",
        "Semi-Parametric RL Agents",
        "Outcome Supervision",
        "Process Supervision",
        "GraphRAG",
        "GRAG",
        "Knowledge Graph-Guided RAG",
        "Graph Retrieval",
        "Community Detection",
        "Deep-path Traversals",
        "Semantic Relationships",
        "Chain of Preference Optimization",
        "Visual CoT",
        "Boosting of Thoughts",
        "Noisy Rationales",
        "Tree-of-Thought Search",
        "Multimodal Reasoning",
        "Ablation Study",
        "消融实验"
      ],
      "negative": [
        "machine learning",
        "deep learning",
        "AI",
        "CNN",
        "RNN",
        "LSTM",
        "SFT",
        "RAG",
        "Pandas",
        "Matplotlib",
        "Seaborn",
        "MNIST",
        "CIFAR-10",
        "SVM",
        "XGBoost",
        "API调用",
        "爬虫",
        "数据分析",
        "可视化",
        "调参"
      ]
    },
    "candidate_filters": {
      "活跃度": "本周活跃",
      "院校": [
        "985",
        "留学",
        "国内外名校"
      ],
      "只看第一学历": true,
      "近期没有看过": "近14天没有",
      "跳槽频率": "5年少于3份",
      "是否与同事交换简历": "近一个月没有",
      "牛人关键词": [
        "语言模型"
      ],
      "性别": "男",
      "学历": "硕士",
      "经验": "3-5年",
      "经验要求": "应届",
      "求职意向": [
        "离职-随时到岗",
        "在职-考虑机会"
      ],
      "薪资待遇": "15-25K"
    },
    "description": "● 前沿算法转化： 跟踪、阅读和深入理解我们核心研究方向（数据归因、LoRA优化、幻觉、RL、GraphRAG、CoT）的最新论文，并将其核心思想工程化实现。\n● 实验设计与执行： 针对具体业务问题，设计严谨的算法实验方案、消融实验和A/B测试，评估不同方法的优劣。\n● 模型训练与优化： 负责模型的全流程训练、微调、调试和迭代；分析实验结果，定位性能瓶颈（无论是算法层面还是工程层面），并提出改进方案。\n● 算法融合创新： 结合业务需求，将来自不同论文的算法模块（例如，将一种新的LoRA变体应用到RLHF流程中）进行创新性组合与优化。\n● 业务需求对接： 与产品和业务方沟通，将模糊的客户需求转化为清晰、可执行的算法技术指标和研发路径。",
    "drill_down_questions": "通用破冰问题（必问）：\n1. “我看到你简历里提到了xxx，你能用通俗的语言给我解释一下，它解决了标准LoRA的什么问题吗？”\n    ○ 考察点： 是否真的理解了，还是只是堆砌词汇。好的回答会提到“多任务冲突”、“参数共享”等。\n2. “在你做的 [某个项目] 中，你提到了 [论文X] 和。你是怎么把它们结合起来的？为什么要这么做？”\n    ○ 考察点： 考察“融会贯通”的能力，这是岗位的核心要求。\n3. “你在 [某个训练/实验] 中，提到你做了‘消融实验’。能具体说一下你‘消融’了哪个部分吗？这个实验是为了验证什么？”\n    ○ 考察点： 确认其具备“实验科学家”的结构化思维。如果答不上来，说明项目经验水分大。\n4. “如果现在有个业务需求是 [一个相关的简化版业务问题，如：我们希望模型在持续学习新知识时，不要忘记旧知识]，根据你的经验，你会从哪几个算法方向去尝试解决？”\n    ○ 考察点： 考察其连接“业务需求”和“算法库”的能力。好的回答可能会提到 CLoRA, Subspace Regularization 等。\n5.  “您好，我看到您的简历非常优秀。能否请您选择一个您最有深度的算法项目，简单说一下：1）当时要解决的核心问题是什么？2）您为什么选择了A方案而不是B方案？3）您通过什么实验（比如消融实验）来证明您的方案是有效的？”\n\n针对特定方向的追问（根据简历关键词选用）：\n1. 如果简历提及 LoRA 优化 (QLoRA, HydraLoRA, CLoRA):\n“我看到您在项目中使用了 LoRA / QLoRA。在实际训练中，您是如何设定它的秩（Rank）的？设定高或低，分别会对您的显存占用和模型效果产生什么具体影响？”\n（追问 HydraLoRA / CLoRA）：“您为什么选择 HydraLoRA / Controlled LoRA 而不是标准的LoRA？它解决了标准LoRA的什么特定问题（比如多任务干扰或灾难性遗忘）？”\n\n2. 如果简历提及 RL 对齐 (RLHF, DPO, Reflexion):\n“在您提到的 DPO / RLHF 项目中，偏好数据集（Preference Dataset）是如何构建的？这个过程中最大的难点是什么？”\n（追问 Reflexion）：“您提到了 Reflexion（反思）框架，它和传统RLHF最大的不同是什么？您在项目中是如何实现模型的‘自我反思’这一步的？”\n\n3. 如果简历提及模型幻觉 (Hallucination Detection, RAG-HAT):\n“针对‘幻觉’问题，我注意到您使用了 RAG-HAT / HaloScope 这样的技术。您能具体说明一下，您是如何定义和标注‘幻觉’数据来训练您的检测或优化模型的吗？”\n“在您的项目中，您是如何量化评估幻觉问题得到了多大程度的缓解的？”\n\n4. 如果简历提及 GraphRAG (知识图谱):\n“我看到您有 GraphRAG 的经验。和传统的向量RAG相比，您在项目中构建的‘知识图谱’提供了哪些额外的信息？（比如，是实体关系，还是多跳推理路径？）”\n“在您做图检索时（比如使用 Community Detection），您是如何平衡检索的**深度（召回率）和相关性（精确率）**的？”\n\n5. 如果简历提及数据归因 (Data Attribution, Influence Functions):\n“您在项目中提到使用了‘数据归因’ 技术。您当时是为了解决什么具体问题（比如，是追踪一个错误答案的来源，还是提升数据质量）？”\n（追问 DataInf）：“在 LoRA 调优的背景下，计算数据影响（Influence）的主要难点是什么？DataInf 是如何解决的？”\n\n6. 如果简历提及 CoT / 高级推理 (Tree-of-Thought, Noisy Rationales):\n“在您关于 CoT（思维链） 的项目中，您是如何评估模型生成的那些‘中间推理步骤’的质量的？”\n“您是否尝试过优化CoT的推理过程，比如当推理步骤包含错误信息（Noisy Rationales）时，模型该如何处理？”",
    "version": 3,
    "current": true,
    "created_at": "2025-11-06T18:10:15.918010",
    "background": "我们正在寻找一位高潜力的算法工程师，加入我们的核心AI研发团队。该职位的核心任务是将前沿的算法理论（特别是我们关注的6大研究方向）转化为实际、高效的模型和系统。\n你将负责深入阅读、拆解和融会贯通多篇最新的学术论文，结合业务需求，自主设计和执行严谨的算法实验（包括消融实验和对比实验）。你不仅要实现算法，更要负责训练优化、迭代结果、分析瓶颈，并最终交付可用的高性能模型。此外，你还需要与业务和产品团队紧密协作，深刻理解客户需求，并从技术层面引导和定义解决方案。"
  }
]