"""Enhanced message console with resume viewing, scoring, and AI drafting."""
from __future__ import annotations

from pathlib import Path
from typing import Any, Dict, List, Optional

import streamlit as st

from streamlit_shared import call_api, ensure_state, sidebar_controls

COMPANY_MD_PATH = Path("config/company.md")
DEFAULT_HISTORY_LIMIT = 10

# ---------------------------------------------------------------------------
# Data loaders and helpers
# ---------------------------------------------------------------------------



@st.cache_data(ttl=600, show_spinner="è·å–æ¶ˆæ¯åˆ—è¡¨ä¸­...")
def _get_dialogs_cached(limit: int) -> List[Dict[str, Any]]:
    """Cached message fetching - depends only on inputs, not session state."""
    ok, payload = call_api("GET", "/chat/dialogs", params={"limit": limit})
    if not ok:
        raise ValueError(f"è·å–æ¶ˆæ¯åˆ—è¡¨å¤±è´¥: {payload}")
    messages = payload.get("messages") or []
    if not isinstance(messages, list):
        raise ValueError("API è¿”å›çš„æ¶ˆæ¯æ ¼å¼ä¸ç¬¦åˆé¢„æœŸ")
    return messages

@st.cache_data(ttl=300, show_spinner="è·å–ç®€å†ä¸­...")
def _fetch_resume(chat_id: str, endpoint: str) -> Optional[Dict[str, Any]]:
    """Fetch resume data with Streamlit caching."""
    ok, payload = call_api("POST", endpoint, json={"chat_id": chat_id})
    if not ok or not isinstance(payload, dict):
        # Don't cache errors - raise exception to skip caching
        raise ValueError(f"è·å–ç®€å†å¤±è´¥: {payload}")
    return payload


@st.cache_data(ttl=300, show_spinner="è·å–èŠå¤©è®°å½•ä¸­...")
def _fetch_history(chat_id: str) -> List[str]:
    """Fetch chat history with Streamlit caching."""
    ok, payload = call_api("GET", f"/chat/{chat_id}/messages")
    messages: List[str] = []
    if ok and isinstance(payload, dict):
        raw = payload.get("messages") 
        for item in raw[-DEFAULT_HISTORY_LIMIT:]:
            messages.append(item)
    else:
        raise ValueError(f"è·å–èŠå¤©è®°å½•å¤±è´¥: {payload}")
    return messages


def _fetch_best_resume(chat_id: str) -> tuple[str, str]:
    """
    Fetch best available resume (full resume preferred, online as fallback).
    
    Cached for 10 minutes to improve performance.
    
    Returns:
        tuple[str, str]: (resume_text, source) where source is "é™„ä»¶ç®€å†" or "åœ¨çº¿ç®€å†"
    """
    # Step 1: Check if full resume is available
    ok_check, check_payload = call_api(
        "POST", "/resume/check_full",
        json={"chat_id": chat_id}
    )
    
    if ok_check and check_payload.get("available"):
        # Step 2: Get full resume if available
        full_payload = _fetch_full_resume(chat_id)
        if full_payload.get("success"):
            resume_text = full_payload.get("text", "")
            if resume_text:
                return resume_text, "é™„ä»¶ç®€å†"
    
    # Step 3: Fallback to online resume
    online_payload = _fetch_online_resume(chat_id)
    if online_payload.get("success"):
        resume_text = online_payload.get("text", "")
        if resume_text:
            return resume_text, "åœ¨çº¿ç®€å†"
    
    return "æ— ç®€å†æ•°æ®", "æ— "


@st.cache_data(show_spinner="è·å–ç®€å†ä¸­...")
def _fetch_full_resume(chat_id: str) -> Dict[str, Any]:
    """Fetch full resume with Streamlit caching."""
    ok, payload = call_api("POST", "/resume/view_full", json={"chat_id": chat_id})
    if not ok or not isinstance(payload, dict):
        raise ValueError(f"è·å–ç®€å†å¤±è´¥: {payload}")
    return payload


@st.cache_data(show_spinner="è·å–ç®€å†ä¸­...")
def _fetch_online_resume(chat_id: str) -> Dict[str, Any]:
    """Fetch online resume with Streamlit caching."""
    ok, payload = call_api("POST", "/resume/online", json={"chat_id": chat_id})
    if not ok or not isinstance(payload, dict):
        raise ValueError(f"è·å–ç®€å†å¤±è´¥: {payload}")
    return payload



# ---------------------------------------------------------------------------
# UI components
# ---------------------------------------------------------------------------

def render_resume_section(
    title: str,
    chat_id: str,
    endpoint: str,
    cache_key: str,
    request_when_missing: bool = False,
    check_endpoint: Optional[str] = None,
) -> str:
    """
    æ¸²æŸ“ç®€å†å±•ç¤ºåŒºå—ï¼Œæ”¯æŒåŠ è½½ã€åˆ·æ–°ã€å¯é€‰çš„å¯ç”¨æ€§æ£€æŸ¥å’Œç®€å†è¯·æ±‚ã€‚

    å‚æ•°:
        title (str): å±•å¼€åŒºå—æ ‡é¢˜ã€‚
        chat_id (str): èŠå¤©ä¼šè¯IDã€‚
        endpoint (str): è·å–ç®€å†çš„APIç«¯ç‚¹ã€‚
        cache_key (str): ç”¨äºç¼“å­˜çš„å”¯ä¸€é”®ã€‚
        request_when_missing (bool): è‹¥ç®€å†ä¸å¯ç”¨æ—¶æ˜¯å¦å…è®¸è¯·æ±‚ç®€å†ã€‚
        check_endpoint (Optional[str]): æ£€æŸ¥ç®€å†å¯ç”¨æ€§çš„APIç«¯ç‚¹ï¼ˆå¯é€‰ï¼‰ã€‚

    è¿”å›:
        str: ç®€å†æ–‡æœ¬å†…å®¹ï¼ˆå¦‚æœ‰ï¼‰ï¼Œå¦åˆ™ä¸ºç©ºå­—ç¬¦ä¸²ã€‚
    """
    text = ""
    load_state_key = f"loaded_{cache_key}_{chat_id}"
    with st.expander(title, expanded=False):
        cols = st.columns([1, 1, 3])
        if cols[0].button("åŠ è½½", key=f"load_{cache_key}_{chat_id}"):
            st.session_state[load_state_key] = True

        load_state = st.session_state.get(load_state_key, False)
        if not load_state:
            st.caption("ç‚¹å‡»â€œåŠ è½½â€ä»¥è·å–å†…å®¹ã€‚")
            return text

        if st.session_state[load_state_key]:
            data = _fetch_resume(chat_id, endpoint)

        success = bool(data and data.get("success", True))
        if not success:
            details = data.get("details") if data else None
            st.warning(details or "æ— æ³•è·å–ç®€å†ã€‚")
            return text

        text = data.get("text") or data.get("content") or ""
        if text:
            st.text_area("å†…å®¹", value=text, height=300)
        else:
            st.info("æš‚æ— å¯æ˜¾ç¤ºçš„ç®€å†æ–‡æœ¬ã€‚")
        return text


def _get_history_data(chat_id: str) -> List[Dict[str, Any]]:
    """Fetch history data (separated from UI rendering)"""
    try:
        return _fetch_history(chat_id)
    except ValueError as e:
        st.error(str(e))
        return []


def render_history_section(history: List[Dict[str, Any]]) -> None:
    """Render history section UI (separated from data fetching)"""
    with st.expander("æœ€è¿‘ 10 æ¡å¯¹è¯", expanded=False):
        if history:
            # Format history data for better table display
            formatted_history = []
            for item in history:
                type_emoji = "ğŸ‘¤" if item.get('type') == 'candidate' else "ğŸ¢"
                status_emoji = "âœ…" if item.get('status') == 'processed' else "â³" if item.get('status') else "â“"
                formatted_item = {
                    'ç±»å‹': f"{type_emoji} {'å€™é€‰äºº' if item.get('type') == 'candidate' else 'HR'}",
                    'æ—¶é—´': item.get('timestamp', ''),
                    'æ¶ˆæ¯å†…å®¹': item.get('message', ''),
                    'çŠ¶æ€': f"{status_emoji} {item.get('status', 'æœªå¤„ç†') if item.get('status') else 'æœªå¤„ç†'}"
                }
                formatted_history.append(formatted_item)
            import pandas as pd
            df = pd.DataFrame(formatted_history)
            st.dataframe(
                df, 
                width="stretch", 
                hide_index=True,
                column_config={
                    "ç±»å‹": st.column_config.TextColumn("ç±»å‹", width="small"),
                    "æ—¶é—´": st.column_config.TextColumn("æ—¶é—´", width="medium"),
                    "æ¶ˆæ¯å†…å®¹": st.column_config.TextColumn("æ¶ˆæ¯å†…å®¹", width="large"),
                    "çŠ¶æ€": st.column_config.TextColumn("çŠ¶æ€", width="small")
                }
            )
        else:
            st.info("æš‚æ— èŠå¤©è®°å½•")




# ---------------------------------------------------------------------------
# Main entrypoint
# ---------------------------------------------------------------------------

def main() -> None:
    st.title("æ¶ˆæ¯åˆ—è¡¨")
    ensure_state()
    sidebar_controls(include_config_path=False, include_job_selector=True)
    
    # === Data Loading Phase (cached, fast) ===
    limit = st.sidebar.slider("æ¯æ¬¡è·å–å¯¹è¯æ•°é‡", min_value=5, max_value=100, value=30, step=5)
    with st.spinner("è·å–èŠå¤©å¯¹è¯æ•°æ®ä¸­..."):
        dialogs = _get_dialogs_cached(limit)

    if not dialogs:
        st.info("æš‚æ— èŠå¤©å¯¹è¯æ•°æ®ã€‚ã€‚ã€‚")
        return

    col_select, col_refresh = st.columns([9, 1])
    chat_id = col_select.selectbox(
        'None',
        options=[row["id"] for row in dialogs],
        format_func=lambda cid: next(
            (f"{row['name']}({row['job_title']}):{row['text']}" for row in dialogs if row['id'] == cid),
            cid,
        ),
        key="chat_selector",
        index=1,
        label_visibility="collapsed",
    )
    selected_dialog = next((row for row in dialogs if row['id'] == chat_id), None)
    if col_refresh.button("ğŸ”„", key="refresh_messages_main"):
        _get_dialogs_cached.clear()
        st.rerun()
    
    # Null safety check
    if not selected_dialog:
        st.warning("æœªèƒ½æ‰¾åˆ°é€‰ä¸­çš„å€™é€‰äººï¼Œè¯·åˆ·æ–°åˆ—è¡¨é‡è¯•")
        return

    # Sync job selection
    selected_job = st.session_state["selected_job"]

    # === Data Fetching Phase (upfront, cached by Streamlit) ===
    # Fetch resume data (cached by @st.cache_data for 10 minutes)
    resume_text, resume_source = _fetch_best_resume(chat_id)
    
    # Fetch history data (cached by @st.cache_data for 5 minutes)
    history_lines = _get_history_data(chat_id)
    history_text = "\n".join([
        f"{item.get('type', 'unknown')}: {item.get('message', '')}"
        for item in history_lines
    ])
    
    # === UI Rendering Phase (display data in expanders) ===
    # Resume expanders - now filled with cached data
    with st.expander("ç®€å†ä¿¡æ¯", expanded=False):
        col1, col2 = st.columns(2)
        with col1:
            st.metric("ç®€å†æ¥æº", resume_source)
        with col2:
            if st.button("ğŸ”„ åˆ·æ–°ç®€å†", key=f"refresh_resume_{chat_id}"):
                # Clear Streamlit cache and reload
                _fetch_best_resume.clear()
                st.rerun()
        
        if resume_text and resume_text != "æ— ç®€å†æ•°æ®":
            st.text_area("ç®€å†å†…å®¹", value=resume_text, height=300, key=f"resume_display_{chat_id}")
        else:
            st.warning("æš‚æ— ç®€å†æ•°æ®")
    
    # History expander - now filled with fetched data
    render_history_section(history_lines)

    # === Scoring Section (user-triggered) ===
    st.subheader("è‡ªåŠ¨è¯„åˆ†")
    notes = st.text_input(
        "è‡ªåŠ¨è¯„åˆ†", 
        placeholder="è¡¥å……è¯´æ˜ (å¯é€‰)", 
        value="", 
        key=f"score_notes_{chat_id}", 
        label_visibility="collapsed"
    )
    if st.button("Analyze", key=f"analyze_{chat_id}"):
        # Use cached resume data
        context = {
            "job_description": selected_job.get("description", ""),
            "target_profile": selected_job.get("target_profile", ""),
            "candidate_resume": resume_text,
            "chat_history": history_text or "æ— ",
            "notes": notes,
        }
        with st.spinner("åˆ†æä¸­..."):
            ok, payload = call_api(
                "POST", "/assistant/analyze-candidate",
                json={"chat_id": chat_id, "context": context}
            )
            if ok and payload.get("success"):
                result = payload.get("analysis")
                st.session_state.setdefault("analysis_results", {})[chat_id] = result
            else:
                error = payload.get("error") if isinstance(payload, dict) else str(payload)
                st.error(f"æ— æ³•è§£æè¯„åˆ†ç»“æœ: {error}")
        st.rerun()

    # Display analysis results
    result = st.session_state.get("analysis_results", {}).get(chat_id)
    if result:
        cols = st.columns(4)
        cols[0].metric("æŠ€èƒ½åŒ¹é…", result.get("skill"))
        cols[1].metric("åˆ›ä¸šå¥‘åˆ", result.get("startup_fit"))
        cols[2].metric("åŠ å…¥æ„æ„¿", result.get("willingness"))
        cols[3].metric("ç»¼åˆè¯„åˆ†", result.get("overall"))
        st.markdown(f"**åˆ†ææ€»ç»“ï¼š** {result.get('summary', 'â€”')}")

    # === Message Section (user-triggered) ===
    st.subheader("ç”Ÿæˆæ¶ˆæ¯")
    message_state = st.session_state.setdefault("generated_messages", {})
    draft = message_state.get(chat_id, "")
    draft_message = st.empty()
    draft = draft_message.text_area("æ¶ˆæ¯å†…å®¹", value=draft, height=180, key=f"message_draft_{chat_id}")
    col_generate, col_send = st.columns(2)
    # Generate button
    if col_generate.button("ç”Ÿæˆå»ºè®®", key=f"generate_msg_{chat_id}"):
        # Use cached resume data
        context = {
            "job_description": selected_job.get("description", ""),
            "target_profile": selected_job.get("target_profile", ""),
            "candidate_resume": resume_text,
            "chat_history": history_text or "æ— ",
            "notes": draft,
        }
        with st.spinner("ç”Ÿæˆä¸­..."):
            ok, payload = call_api(
                "POST", "/assistant/generate-followup",
                json={
                    "chat_id": chat_id,
                    "prompt": draft or "",
                    "context": context
                }
            )
            message = payload.get("message") if ok else None
        if message:
            message_state[chat_id] = message
            st.success("ç”Ÿæˆå®Œæˆï¼")
            draft_message.text_area("æ¶ˆæ¯å†…å®¹", value=message, height=180)
            st.rerun()
        else:
            st.error(f"ç”Ÿæˆå¤±è´¥: {payload}")
    # Send button
    if col_send.button("å‘é€æ¶ˆæ¯", key=f"send_msg_{chat_id}"):
        content = draft.strip()
        if not content:
            st.warning("æ¶ˆæ¯å†…å®¹ä¸èƒ½ä¸ºç©º")
        else:
            with st.spinner("å‘é€ä¸­..."):
                ok, payload = call_api(
                    "POST",
                    f"/chat/{chat_id}/send",
                    json={"message": content},
                )
                success = ok
            if success:
                st.success("æ¶ˆæ¯å·²å‘é€")
                message_state[chat_id] = content
            else:
                st.error("å‘é€å¤±è´¥")

if __name__ == "__main__":
    main()
