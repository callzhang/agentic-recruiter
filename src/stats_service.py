"""Data aggregation and daily reporting for hiring metrics.

This module keeps all statistics logic in one place so both the web UI and
scheduled DingTalk reports can share the same calculations.

All functions are written to be sideâ€‘effect free except for
``send_daily_dingtalk_report`` which formats and dispatches the message.
"""

from __future__ import annotations

from collections import Counter, defaultdict
from dataclasses import dataclass
from datetime import datetime, timedelta, time
from typing import Any, Dict, Iterable, List, Optional

from .candidate_store import search_candidates_advanced
from .jobs_store import get_all_jobs
from .assistant_actions import send_dingtalk_notification
from .global_logger import logger


# Stage order used for conversion calculations
# Import from unified stage definition
from .candidate_stages import STAGE_FLOW, STAGE_SEEK, STAGE_PASS, STAGE_CHAT, STAGE_CONTACT, normalize_stage
HIGH_SCORE_THRESHOLD = 7


def _parse_dt(value: str) -> Optional[datetime]:
    """Parse ISO timestamp stored in Milvus records.

    Milvus stores timestamps as ISO strings without timezone; we treat them as
    local time to keep day-grouping intuitive for operators.
    """

    if not value:
        return None
    try:
        # Handle both "2024-01-01T12:00:00" and with offset
        if value.endswith("Z"):
            value = value.replace("Z", "+00:00")
        return datetime.fromisoformat(value)
    except Exception:  # noqa: BLE001 - defensive
        logger.debug("Failed to parse datetime: %s", value)
        return None


@dataclass
class ScoreAnalysis:
    count: int
    average: float
    high_share: float
    distribution: Dict[int, int]
    quality_score: float
    comment: str


def _score_quality(scores: List[int]) -> ScoreAnalysis:
    """è®¡ç®—è‚–åƒå¾—åˆ†ï¼Œç”¨äºè¯„ä¼°å²—ä½ç”»åƒè´¨é‡ã€‚
    
    è‚–åƒå¾—åˆ†ç”±ä¸‰ä¸ªç»´åº¦ç»„æˆï¼š
    1. åˆ†å¸ƒå‡åŒ€åº¦ï¼ˆ40%ï¼‰ï¼šè¯„ä¼°3-8åˆ†æ®µçš„åˆ†å¸ƒæ˜¯å¦å‡åŒ€
    2. é«˜åˆ†å æ¯”ï¼ˆ30%ï¼‰ï¼šè¯„ä¼°é«˜åˆ†ï¼ˆâ‰¥7åˆ†ï¼‰å æ¯”æ˜¯å¦åˆç†ï¼ˆä¸è¶…è¿‡25%ï¼‰
    3. ä¸­å¿ƒåˆ†æ•°ï¼ˆ30%ï¼‰ï¼šè¯„ä¼°å¹³å‡åˆ†æ˜¯å¦æ¥è¿‘ç†æƒ³å€¼6åˆ†
    
    Args:
        scores: å€™é€‰äººè¯„åˆ†åˆ—è¡¨ï¼ˆ1-10åˆ†ï¼‰
        
    Returns:
        ScoreAnalysis: åŒ…å«å„é¡¹ç»Ÿè®¡æŒ‡æ ‡å’Œè‚–åƒå¾—åˆ†çš„åˆ†æç»“æœ
    """
    if not scores:
        return ScoreAnalysis(0, 0.0, 0.0, {}, 0.0, "æš‚æ— è¯„åˆ†æ•°æ®")
    
    # ä½¿ç”¨ numpy å‘é‡åŒ–ä»¥åŠ é€Ÿå¤§æ ·æœ¬è®¡ç®—
    import numpy as np  # type: ignore

    arr = np.clip(np.array(scores, dtype=int), 1, 10)
    avg = float(arr.mean())
    dist_dict = {int(k): int(v) for k, v in zip(*np.unique(arr, return_counts=True))}

    focus = arr[(arr >= 3) & (arr <= 8)]
    if focus.size:
        counts = np.bincount(focus, minlength=11)[3:9]
        max_dev = (counts.max() - counts.min()) / max(1, counts.sum())
        uniform_score = max(0.0, 1 - max_dev * 1.5)
    else:
        uniform_score = 0.1

    high_share = float((arr >= HIGH_SCORE_THRESHOLD).mean())

    # é«˜åˆ†å æ¯”è¶…è¿‡25%å¼€å§‹æƒ©ç½š
    high_penalty = max(0.0, (high_share - 0.25) / 0.75)
    center_score = max(0.0, 1 - abs(avg - 6) / 6)

    # ç»¼åˆè®¡ç®—è‚–åƒå¾—åˆ†ï¼šä¸‰ä¸ªç»´åº¦çš„åŠ æƒå¹³å‡
    # åˆ†å¸ƒå‡åŒ€åº¦40% + (1-é«˜åˆ†æƒ©ç½š)30% + ä¸­å¿ƒåˆ†æ•°30%
    quality = (uniform_score * 0.4) + ((1 - high_penalty) * 0.3) + (center_score * 0.3)
    # å°†å¾—åˆ†æ˜ å°„åˆ°1-10åˆ†èŒƒå›´ï¼Œä¿ç•™1ä½å°æ•°
    quality_score = round(max(1.0, min(10.0, quality * 10)), 1)

    # æ ¹æ®è®¡ç®—ç»“æœç”Ÿæˆè¯„è¯­
    comment = (
        "åˆ†å¸ƒé›†ä¸­åœ¨é«˜åˆ†æ®µï¼Œéœ€ä¼˜åŒ–ç”»åƒ" if high_penalty > 0.1  # é«˜åˆ†å æ¯”è¿‡é«˜
        else "åˆ†å¸ƒå‡è¡¡ï¼Œç”»åƒè´¨é‡è‰¯å¥½" if uniform_score > 0.6  # åˆ†å¸ƒå‡åŒ€
        else "åˆ†å¸ƒç•¥åï¼Œå¯å†ç»†åŒ–ç”»åƒ"  # å…¶ä»–æƒ…å†µ
    )

    return ScoreAnalysis(
        count=sum(dist_dict.values()),
        average=round(avg, 2),
        high_share=round(high_share, 3),
        distribution=dist_dict,
        quality_score=quality_score,
        comment=comment,
    )


def dist_count(items: Iterable[int], predicate) -> int:
    return sum(1 for i in items if predicate(i))


def build_daily_series(candidates: List[Dict[str, Any]], days: int = 7) -> List[Dict[str, Any]]:
    today = datetime.now().date()
    start = today - timedelta(days=days - 1)

    bucket = defaultdict(lambda: {"new": 0, "seek": 0, "processed": 0})
    for cand in candidates:
        dt = _parse_dt(cand.get("updated_at"))
        if not dt:
            continue
        day = dt.date()
        if day < start:
            continue
        bucket[day]["new"] += 1

        stage_norm = normalize_stage(cand.get("stage"))
        if stage_norm == STAGE_SEEK:
            bucket[day]["seek"] += 1
        # Check if processed: strictly contacted metadata
        contacted = cand.get("metadata", {}).get("contacted")
        if contacted:
            bucket[day]["processed"] += 1

    series = []
    for i in range(days):
        d = start + timedelta(days=i)
        data = bucket.get(d, {"new": 0, "seek": 0, "processed": 0})
        series.append({"date": d.isoformat(), **data})
    return series


def conversion_table(candidates: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """Calculate stage conversion rates.
    
    Stage flow: PASS â†’ CHAT â†’ SEEK â†’ CONTACT
    Conversion rate formula: (current_stage + all_following_stages) / previous_stage_count
    
    - PASS: First stage, rejected candidates (score < chat_threshold)
    - CHAT: From total screened (score >= chat_threshold)
    - SEEK: From CHAT (score >= borderline_threshold)
    - CONTACT: From SEEK (score >= seek_threshold)
    """
    # Normalize stage names using unified stage utilities
    stage_counts = Counter(normalize_stage(cand.get("stage")) or "" for cand in candidates)
    rows: List[Dict[str, Any]] = []
    
    # Calculate stage counts
    pass_count = stage_counts.get(STAGE_PASS, 0)
    chat_count = stage_counts.get(STAGE_CHAT, 0)
    seek_count = stage_counts.get(STAGE_SEEK, 0)
    contact_count = stage_counts.get(STAGE_CONTACT, 0)
    
    # Calculate total screened (all candidates except those without stage)
    total_screened = (pass_count + chat_count + seek_count + contact_count) or 1
    
    # PASS: First stage
    # è½¬åŒ–ç‡ = (PASS + CHAT + SEEK + CONTACT) / æ€»ç­›é€‰äººæ•° = 100%
    rows.append({
        "stage": STAGE_PASS,
        "count": pass_count,
        "previous": total_screened,  # Total screened is the "previous" for PASS
        "rate": round(pass_count / total_screened, 3),
    })
    
    # CHAT: Second stage
    # è½¬åŒ–ç‡ = CHAT / PASSäººæ•° (ä»PASSé˜¶æ®µè½¬åŒ–åˆ°CHATé˜¶æ®µçš„æ¯”ä¾‹)
    rows.append({
        "stage": STAGE_CHAT,
        "count": chat_count,
        "previous": pass_count,  # From PASS
        "rate": round(chat_count / (pass_count or 1), 3),
    })
    
    # SEEK: Third stage
    # è½¬åŒ–ç‡ = SEEK / CHATäººæ•° (ä»CHATé˜¶æ®µè½¬åŒ–åˆ°SEEKé˜¶æ®µçš„æ¯”ä¾‹)
    rows.append({
        "stage": STAGE_SEEK,
        "count": seek_count,
        "previous": chat_count,  # From CHAT
        "rate": round(seek_count / (chat_count or 1), 3),
    })
    
    # CONTACT: Fourth stage
    # è½¬åŒ–ç‡ = CONTACT / SEEKäººæ•° (ä»SEEKé˜¶æ®µè½¬åŒ–åˆ°CONTACTé˜¶æ®µçš„æ¯”ä¾‹)
    rows.append({
        "stage": STAGE_CONTACT,
        "count": contact_count,
        "previous": seek_count,  # From SEEK
        "rate": round(contact_count / (seek_count or 1), 3),
    })
    
    return rows


def build_daily_candidate_counts(candidates: List[Dict[str, Any]], total_count: int, days: int = 30) -> List[Dict[str, Any]]:
    """Build daily cumulative candidate counts for historical chart.
    
    Note: Candidate collection only has updated_at field, not created_at.
    We use updated_at as the date for counting.
    
    Args:
        candidates: List of candidate records (limited by Milvus query limit)
        total_count: Total number of candidates in the collection
        days: Number of days to show in the chart
    """
    today = datetime.now().date()
    start = today - timedelta(days=days - 1)
    
    # Count candidates by updated_at date (candidate collection doesn't have created_at)
    daily_counts = defaultdict(int)
    candidates_without_date = 0
    candidates_in_period = 0
    
    for cand in candidates:
        # Candidate collection only has updated_at, not created_at
        dt = _parse_dt(cand.get("updated_at"))
        if not dt:
            candidates_without_date += 1
            continue
        day = dt.date()
        if day >= start:
            daily_counts[day] += 1
            candidates_in_period += 1
    
    # Calculate candidates before the period
    # Since we can only fetch 16384 candidates, we estimate:
    # total_count - candidates_in_period - candidates_without_date = candidates_before_start
    candidates_before_start = max(0, total_count - candidates_in_period - candidates_without_date)
    
    logger.debug(f"build_daily_candidate_counts: total_fetched={len(candidates)}, total_in_db={total_count}, in_period={candidates_in_period}, without_date={candidates_without_date}, before_start={candidates_before_start}")
    
    # Build cumulative series starting from candidates before the period
    series = []
    cumulative = candidates_before_start
    for i in range(days):
        d = start + timedelta(days=i)
        count = daily_counts.get(d, 0)
        cumulative += count
        series.append({
            "date": d.isoformat(),
            "count": cumulative,
            "new": count
        })
    return series


def fetch_job_candidates(job_name: str, days: int | None = None) -> List[Dict[str, Any]]:
    """Fetch candidates for a job with optional time range and limit.
    
    Args:
        job_name: Job position name
        limit: Maximum number of candidates to return. If None, uses a large default (10000)
        days: Number of days to look back. If None, fetches all candidates
    
    Returns:
        List of candidate dictionaries
    """
    updated_from = None
    if days:
        start_dt = datetime.now() - timedelta(days=days)
        updated_from = start_dt.isoformat()
    return search_candidates_advanced(
        job_applied=job_name,
        limit=None,
        fields=["candidate_id", "job_applied", "stage", "analysis", "updated_at", "metadata"],
        updated_from=updated_from,
        sort_by="updated_at",
        sort_direction="desc",
    )


def compile_job_stats(job_name: str) -> Dict[str, Any]:
    # è·å–æœ€è¿‘ä¸€å‘¨çš„å€™é€‰äººæ•°æ®ç”¨äºç»Ÿè®¡
    # ä¸è®¾ç½® limitï¼Œä½¿ç”¨é»˜è®¤çš„ 10000 ä»¥è·å–æ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„å€™é€‰äºº
    candidates = fetch_job_candidates(job_name, days=7)
    # Score analysis uses latest 100
    recent_scores = [
        (cand.get("analysis") or {}).get("overall")
        for cand in candidates
        if (cand.get("analysis") or {}).get("overall") is not None
    ][:100]
    score_summary = _score_quality(recent_scores)

    daily = build_daily_series(candidates, days=7)
    conversions = conversion_table(candidates)

    # è¿‘7å¤©ç»Ÿè®¡æ•°æ®ï¼ˆç”¨äºè¿›å±•åˆ†è®¡ç®—å’Œ"best record"è¯„é€‰ï¼‰
    # candidates å·²ç»æ˜¯æœ€è¿‘7å¤©çš„æ•°æ®ï¼ˆé€šè¿‡ days=7 è·å–ï¼‰
    recent_7days_candidates = candidates  # æœ€è¿‘7å¤©çš„æ‰€æœ‰å€™é€‰äºº
    recent_7days_high = dist_count(
        [
            (c.get("analysis") or {}).get("overall")
            for c in recent_7days_candidates
            if (c.get("analysis") or {}).get("overall") is not None
        ],
        lambda s: s >= HIGH_SCORE_THRESHOLD,
    )
    # è¿›å±•åˆ†ï¼šè¿‘7å¤©è¿›å±•åˆ°SEEKå’ŒCONTACTé˜¶æ®µçš„å€™é€‰äººæ•°
    recent_7days_seek = sum(1 for c in recent_7days_candidates if normalize_stage(c.get("stage")) == STAGE_SEEK)
    recent_7days_contacted = sum(1 for c in recent_7days_candidates if normalize_stage(c.get("stage")) == STAGE_CONTACT)
    
    # è¿›å±•åˆ† = (è¿‘7æ—¥å€™é€‰äººæ•°é‡ + SEEKäººæ•° + CONTACTäººæ•° x 10) Ã— è‚–åƒå¾—åˆ† / 10
    recent_7days_metric = (len(recent_7days_candidates) + recent_7days_seek + recent_7days_contacted * 10) * score_summary.quality_score / 10
    
    # ä»Šæ—¥æ•°æ®ï¼ˆç”¨äºæ˜¾ç¤ºä»Šæ—¥æ–°å¢ï¼‰
    today = datetime.now().date()
    today_candidates = [c for c in candidates if _parse_dt(c.get("updated_at")) and _parse_dt(c.get("updated_at")).date() == today]

    return {
        "job": job_name,
        "daily": daily,
        "conversions": conversions,
        "score_summary": score_summary,
        "today": {
            "count": len(recent_7days_candidates),  # è¿‘7å¤©å€™é€‰äººæ•°é‡ï¼ˆç”¨äºè¿›å±•åˆ†è®¡ç®—ï¼‰
            "high": recent_7days_high,  # è¿‘7å¤©é«˜åˆ†äººæ•°
            "seek": recent_7days_seek,  # è¿‘7å¤©SEEKäººæ•°
            "contacted": recent_7days_contacted,  # è¿‘7å¤©å·²è”ç³»äººæ•°
            "metric": round(recent_7days_metric, 2),  # è¿›å±•åˆ†ï¼ˆåŸºäºè¿‘7å¤©ï¼‰
        },
        "total": len(candidates),
    }


def compile_all_jobs() -> Dict[str, Any]:
    jobs = get_all_jobs() or []
    stats: List[Dict[str, Any]] = []
    for job in jobs:
        position = job.get("position") or job.get("job_id")
        if not position:
            continue
        try:
            stats.append(compile_job_stats(position))
        except Exception as exc:  # noqa: BLE001
            logger.warning("ç»Ÿè®¡å²—ä½ %s å¤±è´¥: %s", position, exc)
    best = max(stats, key=lambda s: s["today"]["metric"], default=None)
    return {"jobs": stats, "best": best}


def send_daily_dingtalk_report() -> bool:
    summary = compile_all_jobs()
    jobs = summary.get("jobs", [])
    if not jobs:
        logger.info("No jobs found for daily report, skipping DingTalk push")
        return False

    best = summary.get("best")
    title = f"æ¯æ—¥æ‹›è˜æˆ˜æŠ¥ - {datetime.now().date().isoformat()}"

    lines = []
    if best:
        ss = best["score_summary"]
        lines.append(
            f"ğŸ† ä»Šæ—¥æœ€ä¼˜å²—ä½ï¼š{best['job']} | æˆç»© {best['today']['metric']:.1f}"
        )
        lines.append(
            f"  ä»Šæ—¥æ–°å¢ {best['today']['count']} äººï¼Œå…¶ä¸­é«˜åˆ†(â‰¥{HIGH_SCORE_THRESHOLD}) {best['today']['high']} äººï¼ŒSEEK {best['today']['seek']}ï¼Œå·²è”ç³» {best['today']['contacted']}ï¼Œè¿›å±•åˆ† {best['today']['metric']:.1f}"
        )
    lines.append("")
    lines.append("å„å²—ä½æ‘˜è¦ï¼š")
    for job in jobs:
        ss = job["score_summary"]
        lines.append(
            f"- {job['job']}: æ€»æ•° {job['total']} | 7æ—¥æ–°å¢ {sum(d['new'] for d in job['daily'])} | ç”»åƒè´¨ {ss.quality_score}/10 | é«˜åˆ†å æ¯” {ss.high_share*100:.1f}%"
        )

    message = "\n".join(lines)
    return send_dingtalk_notification(title=title, message=message, job_id=None)


__all__ = [
    "compile_all_jobs",
    "compile_job_stats",
    "build_daily_series",
    "conversion_table",
    "send_daily_dingtalk_report",
]
