"""Data aggregation and daily reporting for hiring metrics.

This module keeps all statistics logic in one place so both the web UI and
scheduled DingTalk reports can share the same calculations.

All functions are written to be sideâ€‘effect free except for
``send_daily_dingtalk_report`` which formats and dispatches the message.
"""

from __future__ import annotations

from collections import Counter, defaultdict
from dataclasses import dataclass
from datetime import datetime, timedelta, time
from typing import Any, Dict, Iterable, List, Optional

from .candidate_store import search_candidates_advanced
from .jobs_store import get_all_jobs
from .assistant_actions import send_dingtalk_notification
from .global_logger import logger


# Stage order used for conversion calculations
# Import from unified stage definition
from .candidate_stages import STAGE_FLOW, STAGE_SEEK, STAGE_PASS, normalize_stage
HIGH_SCORE_THRESHOLD = 7


def _parse_dt(value: str) -> Optional[datetime]:
    """Parse ISO timestamp stored in Milvus records.

    Milvus stores timestamps as ISO strings without timezone; we treat them as
    local time to keep day-grouping intuitive for operators.
    """

    if not value:
        return None
    try:
        # Handle both "2024-01-01T12:00:00" and with offset
        if value.endswith("Z"):
            value = value.replace("Z", "+00:00")
        return datetime.fromisoformat(value)
    except Exception:  # noqa: BLE001 - defensive
        logger.debug("Failed to parse datetime: %s", value)
        return None


@dataclass
class ScoreAnalysis:
    count: int
    average: float
    high_share: float
    distribution: Dict[int, int]
    quality_score: float
    comment: str


def _score_quality(scores: List[int]) -> ScoreAnalysis:
    """è®¡ç®—è‚–åƒå¾—åˆ†ï¼Œç”¨äºè¯„ä¼°å²—ä½ç”»åƒè´¨é‡ã€‚
    
    è‚–åƒå¾—åˆ†ç”±ä¸‰ä¸ªç»´åº¦ç»„æˆï¼š
    1. åˆ†å¸ƒå‡åŒ€åº¦ï¼ˆ40%ï¼‰ï¼šè¯„ä¼°3-8åˆ†æ®µçš„åˆ†å¸ƒæ˜¯å¦å‡åŒ€
    2. é«˜åˆ†å æ¯”ï¼ˆ30%ï¼‰ï¼šè¯„ä¼°é«˜åˆ†ï¼ˆâ‰¥7åˆ†ï¼‰å æ¯”æ˜¯å¦åˆç†ï¼ˆä¸è¶…è¿‡25%ï¼‰
    3. ä¸­å¿ƒåˆ†æ•°ï¼ˆ30%ï¼‰ï¼šè¯„ä¼°å¹³å‡åˆ†æ˜¯å¦æ¥è¿‘ç†æƒ³å€¼6åˆ†
    
    Args:
        scores: å€™é€‰äººè¯„åˆ†åˆ—è¡¨ï¼ˆ1-10åˆ†ï¼‰
        
    Returns:
        ScoreAnalysis: åŒ…å«å„é¡¹ç»Ÿè®¡æŒ‡æ ‡å’Œè‚–åƒå¾—åˆ†çš„åˆ†æç»“æœ
    """
    if not scores:
        return ScoreAnalysis(0, 0.0, 0.0, {}, 0.0, "æš‚æ— è¯„åˆ†æ•°æ®")
    # ä¼˜å…ˆä½¿ç”¨ numpy å‘é‡åŒ–ä»¥åŠ é€Ÿå¤§æ ·æœ¬ï¼Œè‹¥ä¸å¯ç”¨åˆ™å›é€€åˆ°çº¯ Python
    try:
        import numpy as np  # type: ignore

        arr = np.clip(np.array(scores, dtype=int), 1, 10)
        avg = float(arr.mean())
        dist_dict = {int(k): int(v) for k, v in zip(*np.unique(arr, return_counts=True))}

        focus = arr[(arr >= 3) & (arr <= 8)]
        if focus.size:
            counts = np.bincount(focus, minlength=11)[3:9]
            max_dev = (counts.max() - counts.min()) / max(1, counts.sum())
            uniform_score = max(0.0, 1 - max_dev * 1.5)
        else:
            uniform_score = 0.4

        high_share = float((arr >= HIGH_SCORE_THRESHOLD).mean())
    except Exception:  # pragma: no cover - fallback path
        clipped = [min(10, max(1, int(s))) for s in scores]
        dist = Counter(clipped)
        dist_dict = dict(dist)
        avg = sum(clipped) / len(clipped)
        focus_scores = [dist.get(i, 0) for i in range(3, 9)]
        focus_total = sum(focus_scores)
        if focus_total:
            max_dev = (max(focus_scores) - min(focus_scores)) / max(1, focus_total)
            uniform_score = max(0.0, 1 - max_dev * 1.5)
        else:
            uniform_score = 0.4
        high_share = dist_count(clipped, lambda s: s >= HIGH_SCORE_THRESHOLD) / len(clipped)

    # é«˜åˆ†å æ¯”è¶…è¿‡25%å¼€å§‹æƒ©ç½š
    high_penalty = max(0.0, (high_share - 0.25) / 0.75)
    center_score = max(0.0, 1 - abs(avg - 6) / 6)

    # ç»¼åˆè®¡ç®—è‚–åƒå¾—åˆ†ï¼šä¸‰ä¸ªç»´åº¦çš„åŠ æƒå¹³å‡
    # åˆ†å¸ƒå‡åŒ€åº¦40% + (1-é«˜åˆ†æƒ©ç½š)30% + ä¸­å¿ƒåˆ†æ•°30%
    quality = (uniform_score * 0.4) + ((1 - high_penalty) * 0.3) + (center_score * 0.3)
    # å°†å¾—åˆ†æ˜ å°„åˆ°1-10åˆ†èŒƒå›´ï¼Œä¿ç•™1ä½å°æ•°
    quality_score = round(max(1.0, min(10.0, quality * 10)), 1)

    # æ ¹æ®è®¡ç®—ç»“æœç”Ÿæˆè¯„è¯­
    comment = (
        "åˆ†å¸ƒé›†ä¸­åœ¨é«˜åˆ†æ®µï¼Œéœ€ä¼˜åŒ–ç”»åƒ" if high_penalty > 0.05  # é«˜åˆ†å æ¯”è¿‡é«˜
        else "åˆ†å¸ƒå‡è¡¡ï¼Œç”»åƒè´¨é‡è‰¯å¥½" if uniform_score > 0.6  # åˆ†å¸ƒå‡åŒ€
        else "åˆ†å¸ƒç•¥åï¼Œå¯å†ç»†åŒ–ç”»åƒ"  # å…¶ä»–æƒ…å†µ
    )

    return ScoreAnalysis(
        count=sum(dist_dict.values()),
        average=round(avg, 2),
        high_share=round(high_share, 3),
        distribution=dist_dict,
        quality_score=quality_score,
        comment=comment,
    )


def dist_count(items: Iterable[int], predicate) -> int:
    return sum(1 for i in items if predicate(i))


def build_daily_series(candidates: List[Dict[str, Any]], days: int = 7) -> List[Dict[str, Any]]:
    today = datetime.now().date()
    start = today - timedelta(days=days - 1)

    bucket = defaultdict(lambda: {"new": 0, "seek": 0})
    for cand in candidates:
        dt = _parse_dt(cand.get("updated_at"))
        if not dt:
            continue
        day = dt.date()
        if day < start:
            continue
        bucket[day]["new"] += 1
        if normalize_stage(cand.get("stage")) == STAGE_SEEK:
            bucket[day]["seek"] += 1

    series = []
    for i in range(days):
        d = start + timedelta(days=i)
        data = bucket.get(d, {"new": 0, "seek": 0})
        series.append({"date": d.isoformat(), **data})
    return series


def conversion_table(candidates: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    # Normalize stage names using unified stage utilities
    stage_counts = Counter(normalize_stage(cand.get("stage")) or "" for cand in candidates)
    rows: List[Dict[str, Any]] = []
    for idx, stage in enumerate(STAGE_FLOW):
        count = stage_counts.get(stage, 0)
        prev = stage_counts.get(STAGE_FLOW[idx - 1], 0) if idx > 0 else 0
        denominator = count + prev if idx > 0 else max(count, 1)
        rate = count / denominator if denominator else 0.0
        rows.append({
            "stage": stage,
            "count": count,
            "previous": prev,
            "rate": round(rate, 3),
        })
    # Track PASS separately to show rejection ratio
    pass_count = stage_counts.get(STAGE_PASS, 0)
    if pass_count:
        total_screened = pass_count + sum(stage_counts[s] for s in STAGE_FLOW)
        rows.append({
            "stage": STAGE_PASS,
            "count": pass_count,
            "previous": total_screened - pass_count,
            "rate": round(pass_count / max(total_screened, 1), 3),
        })
    return rows


def fetch_job_candidates(job_name: str, days: int | None = None) -> List[Dict[str, Any]]:
    """Fetch candidates for a job with optional time range and limit.
    
    Args:
        job_name: Job position name
        limit: Maximum number of candidates to return. If None, uses a large default (10000)
        days: Number of days to look back. If None, fetches all candidates
    
    Returns:
        List of candidate dictionaries
    """
    updated_from = None
    if days:
        start_dt = datetime.now() - timedelta(days=days)
        updated_from = start_dt.isoformat()
    return search_candidates_advanced(
        job_applied=job_name,
        fields=["candidate_id", "job_applied", "stage", "analysis", "updated_at"],
        updated_from=updated_from,
        sort_by="updated_at",
        sort_direction="desc",
    )


def compile_job_stats(job_name: str) -> Dict[str, Any]:
    # è·å–æœ€è¿‘ä¸€å‘¨çš„å€™é€‰äººæ•°æ®ç”¨äºç»Ÿè®¡
    # ä¸è®¾ç½® limitï¼Œä½¿ç”¨é»˜è®¤çš„ 10000 ä»¥è·å–æ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„å€™é€‰äºº
    candidates = fetch_job_candidates(job_name, days=7)
    # Score analysis uses latest 100
    recent_scores = [
        (cand.get("analysis") or {}).get("overall")
        for cand in candidates
        if (cand.get("analysis") or {}).get("overall") is not None
    ][:100]
    score_summary = _score_quality(recent_scores)

    daily = build_daily_series(candidates, days=7)
    conversions = conversion_table(candidates)

    # è¿‘7å¤©ç»Ÿè®¡æ•°æ®ï¼ˆç”¨äºè¿›å±•åˆ†è®¡ç®—å’Œ"best record"è¯„é€‰ï¼‰
    # candidates å·²ç»æ˜¯æœ€è¿‘7å¤©çš„æ•°æ®ï¼ˆé€šè¿‡ days=7 è·å–ï¼‰
    recent_7days_candidates = candidates  # æœ€è¿‘7å¤©çš„æ‰€æœ‰å€™é€‰äºº
    recent_7days_high = dist_count(
        [
            (c.get("analysis") or {}).get("overall")
            for c in recent_7days_candidates
            if (c.get("analysis") or {}).get("overall") is not None
        ],
        lambda s: s >= HIGH_SCORE_THRESHOLD,
    )
    # è¿›å±•åˆ†ï¼šè¿‘7å¤©è¿›å±•åˆ°SEEKé˜¶æ®µçš„å€™é€‰äººæ•°
    recent_7days_seek = sum(1 for c in recent_7days_candidates if normalize_stage(c.get("stage")) == STAGE_SEEK)
    # è¿›å±•åˆ† = (è¿‘7å¤©å€™é€‰äººæ•°é‡ + SEEKé˜¶æ®µäººæ•°) Ã— è‚–åƒè´¨é‡åˆ† / 10 (å½’ä¸€åŒ–)
    # è‚–åƒè´¨é‡åˆ†èŒƒå›´æ˜¯1-10ï¼Œé™¤ä»¥10å½’ä¸€åŒ–åˆ°0-1èŒƒå›´
    recent_7days_metric = (len(recent_7days_candidates) + recent_7days_seek) * score_summary.quality_score / 10
    
    # ä»Šæ—¥æ•°æ®ï¼ˆç”¨äºæ˜¾ç¤ºä»Šæ—¥æ–°å¢ï¼‰
    today = datetime.now().date()
    today_candidates = [c for c in candidates if _parse_dt(c.get("updated_at")) and _parse_dt(c.get("updated_at")).date() == today]

    return {
        "job": job_name,
        "daily": daily,
        "conversions": conversions,
        "score_summary": score_summary,
        "today": {
            "count": len(recent_7days_candidates),  # è¿‘7å¤©å€™é€‰äººæ•°é‡ï¼ˆç”¨äºè¿›å±•åˆ†è®¡ç®—ï¼‰
            "high": recent_7days_high,  # è¿‘7å¤©é«˜åˆ†äººæ•°
            "seek": recent_7days_seek,  # è¿‘7å¤©SEEKäººæ•°
            "metric": round(recent_7days_metric, 2),  # è¿›å±•åˆ†ï¼ˆåŸºäºè¿‘7å¤©ï¼‰
        },
        "total": len(candidates),
    }


def compile_all_jobs() -> Dict[str, Any]:
    jobs = get_all_jobs() or []
    stats: List[Dict[str, Any]] = []
    for job in jobs:
        position = job.get("position") or job.get("job_id")
        if not position:
            continue
        try:
            stats.append(compile_job_stats(position))
        except Exception as exc:  # noqa: BLE001
            logger.warning("ç»Ÿè®¡å²—ä½ %s å¤±è´¥: %s", position, exc)
    best = max(stats, key=lambda s: s["today"]["metric"], default=None)
    return {"jobs": stats, "best": best}


def send_daily_dingtalk_report() -> bool:
    summary = compile_all_jobs()
    jobs = summary.get("jobs", [])
    if not jobs:
        logger.info("No jobs found for daily report, skipping DingTalk push")
        return False

    best = summary.get("best")
    title = f"æ¯æ—¥æ‹›è˜æˆ˜æŠ¥ - {datetime.now().date().isoformat()}"

    lines = []
    if best:
        ss = best["score_summary"]
        lines.append(
            f"ğŸ† ä»Šæ—¥æœ€ä¼˜å²—ä½ï¼š{best['job']} | æˆç»© {best['today']['metric']:.1f}"
        )
        lines.append(
            f"  ä»Šæ—¥æ–°å¢ {best['today']['count']} äººï¼Œå…¶ä¸­é«˜åˆ†(â‰¥{HIGH_SCORE_THRESHOLD}) {best['today']['high']} äººï¼Œè¿›å±•åˆ† {best['today']['seek']} äºº"
        )
    lines.append("")
    lines.append("å„å²—ä½æ‘˜è¦ï¼š")
    for job in jobs:
        ss = job["score_summary"]
        lines.append(
            f"- {job['job']}: æ€»æ•° {job['total']} | 7æ—¥æ–°å¢ {sum(d['new'] for d in job['daily'])} | ç”»åƒè´¨ {ss.quality_score}/10 | é«˜åˆ†å æ¯” {ss.high_share*100:.1f}%"
        )

    message = "\n".join(lines)
    return send_dingtalk_notification(title=title, message=message, job_id=None)


__all__ = [
    "compile_all_jobs",
    "compile_job_stats",
    "build_daily_series",
    "conversion_table",
    "send_daily_dingtalk_report",
]
